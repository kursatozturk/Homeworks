{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network for Regression (Estimate blood pressure from PPG signal)\n",
    "\n",
    "\n",
    "Having gained some experience with neural networks, let us train a network that estimates the blood pressure from a PPG signal window.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Photoplethysmograph (PPG) signal\n",
    "\n",
    "A PPG (photoplethysmograph) signal is a signal obtained with a pulse oximeter, which illuminates the skin and measures changes in light absorption. A PPG signal carries rich information about the status of the cardiovascular health of a person, such as breadth rate, heart rate and blood pressure. An example is shown below, where you also see the blood pressure signal that we will estimate (the data also has the ECG signal, which you should ignore).\n",
    "\n",
    "<img width=\"80%\" src=\"PPG_ABG_ECG_example.png\">\n",
    "\n",
    "\n",
    "# Preparing the Dataset \n",
    "\n",
    "In this task, you are expected to perform the full pipeline for creating a learning system from scratch. Here is how you should construct the dataset:\n",
    "* Download the dataset from the following website, and only take \"Part 1\" (since the whole dataset is too big): https://archive.ics.uci.edu/ml/datasets/Cuff-Less+Blood+Pressure+Estimation\n",
    "* Take a window of size $W$ from the PPG channel between time $t$ and $t+W$. Let us call this $\\textbf{x}_t$.\n",
    "* Take the corresponding window of size $W$ from the ABP (arterial blood pressure) channel between time $t$ and $t+W$. Find the maxima and minima of this signal within the window (you can use \"findpeaks\" from Matlab or \"find_peaks_cwt\" from scipy). Here is an example window from the ABP signal, and its peaks:\n",
    " <img width=\"60%\" src=\"ABP_peaks.png\">\n",
    "    \n",
    "* Calculate the average of the maxima, call it $y^1_t$, and the average of the minima, call it $y^2_t$.\n",
    "* Slide the window (by an amount that is on the order of a few samples) over the PPG signals and collect many training instances of the form $\\{\\textbf{x}_t, <y^1_t, y^2_t>\\}$ instances. \n",
    "* This will be your input-output for training the network. In other words, your network outputs two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from ceng783.utils import load_BP_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "from ceng783.neural_net_for_regression import TwoLayerNet\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "  np.random.seed(0)\n",
    "  return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "  np.random.seed(1)\n",
    "  X = 10 * np.random.randn(num_inputs, input_size)\n",
    "  y = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4], [2, 1, 4], [2, 1, 4]])\n",
    "  return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `ceng783/neural_net_for_regression.py` and look at the method `TwoLayerNet.loss`. This function is very similar to the loss functions you have written for the previous exercises: It takes the data and weights and computes the *regression* scores, the *squared error loss*, and the gradients on the parameters. \n",
    "\n",
    "To be more specific, you will implement the following loss function:\n",
    "\n",
    "$$\\frac{1}{N}\\frac{1}{2}\\sum_i\\sum_{j} (o_{ij} - y_{ij})^2 + \\lambda\\frac{1}{2}\\sum_j w_j^2,$$\n",
    "\n",
    "where $i$ runs through the $N$ samples in the batch; $o_{ij}$ is the prediction of the network for the $i^{th}$ sample for output $j$, and $y_{ij}$ is the correct value; $\\lambda$ is the weight of the regularization term.\n",
    "\n",
    "The first layer uses ReLU as the activation function. The output layer does not use any activation functions.\n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.6802720496109664e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print 'Your scores:'\n",
    "print scores\n",
    "print\n",
    "print 'correct scores:'\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print correct_scores\n",
    "print\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print 'Difference between your scores and correct scores:'\n",
    "print np.sum(np.abs(scores - correct_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, implement the second part that computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss:  13.298479809544318\n",
      "Difference between your loss and correct loss:\n",
      "4.431832678619685e-11\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "print \"Your loss: \", loss\n",
    "correct_loss = 13.2984798095\n",
    "\n",
    "# should be very small, we get < 1e-10\n",
    "print 'Difference between your loss and correct loss:'\n",
    "print np.sum(np.abs(loss - correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Implement the rest of the function. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2 max relative error: 1.443397e-06\n",
      "b1 max relative error: 2.190492e-07\n",
      "W1 max relative error: 5.459961e-04\n",
      "W2 max relative error: 3.756381e-04\n"
     ]
    }
   ],
   "source": [
    "from ceng783.utils import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "  f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "  param_grad_num = eval_numerical_gradient(f, net.params[param_name])\n",
    "  print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PPG dataset for training your regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the training set:  89737\n",
      "Number of instances in the validation set:  11217\n",
      "Number of instances in the testing set:  11218\n"
     ]
    }
   ],
   "source": [
    "# Load the PPG dataset\n",
    "# If your memory turns out to be sufficient, try loading a subset\n",
    "#\n",
    "# TODO: Open up ceng783/utils.py and fill in the `load_BP_dataset()' function.\n",
    "#\n",
    "def get_data(datafile, training_ratio=0.9, test_ratio=0.06, val_ratio=0.01):\n",
    "  # Load the PPG training data \n",
    "  X, y = load_BP_dataset(datafile)\n",
    "\n",
    "  ################################################################\n",
    "  # TODO: Split the data into training, validation and test sets #\n",
    "  ################################################################\n",
    "  length = len(y)\n",
    "  y /= 100\n",
    "  split_idx = int(length * 0.8)\n",
    "  test_idx = int(length * 0.90)\n",
    "    \n",
    "  X_train = X[:split_idx]\n",
    "  X_val = X[split_idx:test_idx]\n",
    "  X_test = X[test_idx:]\n",
    "  \n",
    "  y_train = y[:split_idx]\n",
    "  y_val = y[split_idx:test_idx]\n",
    "  y_test = y[test_idx:]\n",
    "\n",
    "  #########    END OF YOUR CODE    ###############################\n",
    "  ################################################################\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "datafile = 'ceng783/data/Part_1.mat' #TODO: PATH to your data file\n",
    "input_size = 182 * 4# TODO: Size of the input of the network\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data(datafile)\n",
    "print \"Number of instances in the training set: \", len(X_train)\n",
    "print \"Number of instances in the validation set: \", len(X_val)\n",
    "print \"Number of instances in the testing set: \", len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now train our network on the PPG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 0.980879\n",
      "iteration 100 / 5000: loss 0.001785\n",
      "iteration 200 / 5000: loss 0.000775\n",
      "iteration 300 / 5000: loss 0.009763\n",
      "iteration 400 / 5000: loss 0.003624\n",
      "iteration 500 / 5000: loss 0.060081\n",
      "iteration 600 / 5000: loss 0.005458\n",
      "iteration 700 / 5000: loss 0.009271\n",
      "iteration 800 / 5000: loss 0.010488\n",
      "iteration 900 / 5000: loss 0.005587\n",
      "iteration 1000 / 5000: loss 0.001785\n",
      "iteration 1100 / 5000: loss 0.030723\n",
      "iteration 1200 / 5000: loss 0.016017\n",
      "iteration 1300 / 5000: loss 0.021237\n",
      "iteration 1400 / 5000: loss 0.002034\n",
      "iteration 1500 / 5000: loss 0.000602\n",
      "iteration 1600 / 5000: loss 0.010787\n",
      "iteration 1700 / 5000: loss 0.039325\n",
      "iteration 1800 / 5000: loss 0.006094\n",
      "iteration 1900 / 5000: loss 0.002968\n",
      "iteration 2000 / 5000: loss 0.003158\n",
      "iteration 2100 / 5000: loss 0.018012\n",
      "iteration 2200 / 5000: loss 0.001576\n",
      "iteration 2300 / 5000: loss 0.008225\n",
      "iteration 2400 / 5000: loss 0.003085\n",
      "iteration 2500 / 5000: loss 0.009099\n",
      "iteration 2600 / 5000: loss 0.005042\n",
      "iteration 2700 / 5000: loss 0.025951\n",
      "iteration 2800 / 5000: loss 0.002707\n",
      "iteration 2900 / 5000: loss 0.003828\n",
      "iteration 3000 / 5000: loss 0.000319\n",
      "iteration 3100 / 5000: loss 0.001929\n",
      "iteration 3200 / 5000: loss 0.000790\n",
      "iteration 3300 / 5000: loss 0.000922\n",
      "iteration 3400 / 5000: loss 0.000981\n",
      "iteration 3500 / 5000: loss 0.009412\n",
      "iteration 3600 / 5000: loss 0.019920\n",
      "iteration 3700 / 5000: loss 0.083919\n",
      "iteration 3800 / 5000: loss 0.226718\n",
      "iteration 3900 / 5000: loss 0.016992\n",
      "iteration 4000 / 5000: loss 0.007042\n",
      "iteration 4100 / 5000: loss 0.000691\n",
      "iteration 4200 / 5000: loss 0.003552\n",
      "iteration 4300 / 5000: loss 0.003092\n",
      "iteration 4400 / 5000: loss 0.002350\n",
      "iteration 4500 / 5000: loss 0.000513\n",
      "iteration 4600 / 5000: loss 0.021955\n",
      "iteration 4700 / 5000: loss 0.003494\n",
      "iteration 4800 / 5000: loss 0.005862\n",
      "iteration 4900 / 5000: loss 0.000682\n",
      "Validation error:  0.08530423272173666\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train a neural network\n",
    "input_size = input_size\n",
    "hidden_size = 500 # TODO: Choose a suitable hidden layer size\n",
    "num_classes = 2 # We have two outputs\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=5000, batch_size=16,\n",
    "            learning_rate=1, learning_rate_decay=0.90,\n",
    "            reg=0.005, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "#val_err = ... # TODO: Perform prediction on the validation set\n",
    "val_err = np.sum(np.square(net.predict(X_val) - y_val), axis=1).mean()\n",
    "print 'Validation error: ', val_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the training and improve learning\n",
    "You should be able to get a validation error of ~16.\n",
    "\n",
    "So far so good. But, is it really good? Let us plot the validation and training errors to see how good the network did. Did it memorize or generalize? Discuss your observations and conclusions. If its performance is not looking good, propose and test measures. This is the part that will show me how well you have digested everything covered in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation errors\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "train = plt.plot(stats['train_err_history'], label='train')\n",
    "val = plt.plot(stats['val_err_history'], label='val')\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.title('Classification error history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning and Improving Your Network (Bonus)\n",
    "There are many aspects and hyper-parameters you can play with. Do play with them and find the best setting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 0.971397\n",
      "iteration 100 / 2000: loss 0.000333\n",
      "iteration 200 / 2000: loss 0.000043\n",
      "iteration 300 / 2000: loss 0.002787\n",
      "iteration 400 / 2000: loss 0.000873\n",
      "iteration 500 / 2000: loss 0.001628\n",
      "iteration 600 / 2000: loss 0.004049\n",
      "iteration 700 / 2000: loss 0.000057\n",
      "iteration 800 / 2000: loss 0.000045\n",
      "iteration 900 / 2000: loss 0.005698\n",
      "iteration 1000 / 2000: loss 0.003008\n",
      "iteration 1100 / 2000: loss 0.000779\n",
      "iteration 1200 / 2000: loss 0.002803\n",
      "iteration 1300 / 2000: loss 0.002433\n",
      "iteration 1400 / 2000: loss 0.000906\n",
      "iteration 1500 / 2000: loss 0.000126\n",
      "iteration 1600 / 2000: loss 0.000063\n",
      "iteration 1700 / 2000: loss 0.000376\n",
      "iteration 1800 / 2000: loss 0.001233\n",
      "iteration 1900 / 2000: loss 0.001306\n",
      "iteration 0 / 2000: loss 0.995155\n",
      "iteration 100 / 2000: loss 0.035500\n",
      "iteration 200 / 2000: loss 0.006171\n",
      "iteration 300 / 2000: loss 0.007698\n",
      "iteration 400 / 2000: loss 0.000281\n",
      "iteration 500 / 2000: loss 0.001555\n",
      "iteration 600 / 2000: loss 0.002086\n",
      "iteration 700 / 2000: loss 0.000463\n",
      "iteration 800 / 2000: loss 0.000388\n",
      "iteration 900 / 2000: loss 0.002261\n",
      "iteration 1000 / 2000: loss 0.000850\n",
      "iteration 1100 / 2000: loss 0.008995\n",
      "iteration 1200 / 2000: loss 0.000730\n",
      "iteration 1300 / 2000: loss 0.004139\n",
      "iteration 1400 / 2000: loss 0.003511\n",
      "iteration 1500 / 2000: loss 0.010312\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.007109\n",
      "iteration 1800 / 2000: loss 0.000088\n",
      "iteration 1900 / 2000: loss 0.000284\n",
      "iteration 0 / 2000: loss 0.984135\n",
      "iteration 100 / 2000: loss 0.004520\n",
      "iteration 200 / 2000: loss 0.000755\n",
      "iteration 300 / 2000: loss 0.001813\n",
      "iteration 400 / 2000: loss 0.000311\n",
      "iteration 500 / 2000: loss 0.001398\n",
      "iteration 600 / 2000: loss 0.001025\n",
      "iteration 700 / 2000: loss 0.003576\n",
      "iteration 800 / 2000: loss 0.000155\n",
      "iteration 900 / 2000: loss 0.028571\n",
      "iteration 1000 / 2000: loss 0.000526\n",
      "iteration 1100 / 2000: loss 0.001699\n",
      "iteration 1200 / 2000: loss 0.004196\n",
      "iteration 1300 / 2000: loss 0.013402\n",
      "iteration 1400 / 2000: loss 0.000267\n",
      "iteration 1500 / 2000: loss 0.003841\n",
      "iteration 1600 / 2000: loss 0.001486\n",
      "iteration 1700 / 2000: loss 0.000356\n",
      "iteration 1800 / 2000: loss 0.001884\n",
      "iteration 1900 / 2000: loss 0.001003\n",
      "iteration 0 / 2000: loss 0.925769\n",
      "iteration 100 / 2000: loss 0.016088\n",
      "iteration 200 / 2000: loss 0.002314\n",
      "iteration 300 / 2000: loss 0.000908\n",
      "iteration 400 / 2000: loss 0.000228\n",
      "iteration 500 / 2000: loss 0.000361\n",
      "iteration 600 / 2000: loss 0.004178\n",
      "iteration 700 / 2000: loss 0.000277\n",
      "iteration 800 / 2000: loss 0.002005\n",
      "iteration 900 / 2000: loss 0.002509\n",
      "iteration 1000 / 2000: loss 0.088594\n",
      "iteration 1100 / 2000: loss 0.031875\n",
      "iteration 1200 / 2000: loss 0.003317\n",
      "iteration 1300 / 2000: loss 0.003267\n",
      "iteration 1400 / 2000: loss 0.004006\n",
      "iteration 1500 / 2000: loss 0.007555\n",
      "iteration 1600 / 2000: loss 0.012375\n",
      "iteration 1700 / 2000: loss 0.002367\n",
      "iteration 1800 / 2000: loss 0.005162\n",
      "iteration 1900 / 2000: loss 0.008338\n",
      "iteration 0 / 2000: loss 0.980856\n",
      "iteration 100 / 2000: loss 0.001002\n",
      "iteration 200 / 2000: loss 0.000267\n",
      "iteration 300 / 2000: loss 0.009750\n",
      "iteration 400 / 2000: loss 0.003313\n",
      "iteration 500 / 2000: loss 0.057436\n",
      "iteration 600 / 2000: loss 0.005026\n",
      "iteration 700 / 2000: loss 0.008930\n",
      "iteration 800 / 2000: loss 0.010494\n",
      "iteration 900 / 2000: loss 0.005569\n",
      "iteration 1000 / 2000: loss 0.001793\n",
      "iteration 1100 / 2000: loss 0.032285\n",
      "iteration 1200 / 2000: loss 0.015956\n",
      "iteration 1300 / 2000: loss 0.021292\n",
      "iteration 1400 / 2000: loss 0.001854\n",
      "iteration 1500 / 2000: loss 0.000642\n",
      "iteration 1600 / 2000: loss 0.009215\n",
      "iteration 1700 / 2000: loss 0.039204\n",
      "iteration 1800 / 2000: loss 0.006022\n",
      "iteration 1900 / 2000: loss 0.002808\n",
      "iteration 0 / 2000: loss 0.971399\n",
      "iteration 100 / 2000: loss 0.000350\n",
      "iteration 200 / 2000: loss 0.000071\n",
      "iteration 300 / 2000: loss 0.002812\n",
      "iteration 400 / 2000: loss 0.000894\n",
      "iteration 500 / 2000: loss 0.001645\n",
      "iteration 600 / 2000: loss 0.004063\n",
      "iteration 700 / 2000: loss 0.000069\n",
      "iteration 800 / 2000: loss 0.331080\n",
      "iteration 900 / 2000: loss 0.279426\n",
      "iteration 1000 / 2000: loss 0.229349\n",
      "iteration 1100 / 2000: loss 0.187937\n",
      "iteration 1200 / 2000: loss 0.157561\n",
      "iteration 1300 / 2000: loss 0.130400\n",
      "iteration 1400 / 2000: loss 0.106720\n",
      "iteration 1500 / 2000: loss 0.087622\n",
      "iteration 1600 / 2000: loss 0.072412\n",
      "iteration 1700 / 2000: loss 0.060200\n",
      "iteration 1800 / 2000: loss 0.050701\n",
      "iteration 1900 / 2000: loss 0.042210\n",
      "iteration 0 / 2000: loss 0.995142\n",
      "iteration 100 / 2000: loss 0.648888\n",
      "iteration 200 / 2000: loss 0.536945\n",
      "iteration 300 / 2000: loss 0.450080\n",
      "iteration 400 / 2000: loss 0.366597\n",
      "iteration 500 / 2000: loss 0.304533\n",
      "iteration 600 / 2000: loss 0.252626\n",
      "iteration 700 / 2000: loss 0.207632\n",
      "iteration 800 / 2000: loss 0.171693\n",
      "iteration 900 / 2000: loss 0.143911\n",
      "iteration 1000 / 2000: loss 0.117977\n",
      "iteration 1100 / 2000: loss 0.105846\n",
      "iteration 1200 / 2000: loss 0.080815\n",
      "iteration 1300 / 2000: loss 0.070360\n",
      "iteration 1400 / 2000: loss 0.058268\n",
      "iteration 1500 / 2000: loss 0.055590\n",
      "iteration 1600 / 2000: loss 0.037711\n",
      "iteration 1700 / 2000: loss 0.038067\n",
      "iteration 1800 / 2000: loss 0.025686\n",
      "iteration 1900 / 2000: loss 0.021451\n",
      "iteration 0 / 2000: loss 0.984129\n",
      "iteration 100 / 2000: loss 0.000551\n",
      "iteration 200 / 2000: loss 0.000198\n",
      "iteration 300 / 2000: loss 0.001757\n",
      "iteration 400 / 2000: loss 0.000324\n",
      "iteration 500 / 2000: loss 0.001418\n",
      "iteration 600 / 2000: loss 0.001043\n",
      "iteration 700 / 2000: loss 0.003591\n",
      "iteration 800 / 2000: loss 0.000167\n",
      "iteration 900 / 2000: loss 0.028582\n",
      "iteration 1000 / 2000: loss 0.000534\n",
      "iteration 1100 / 2000: loss 0.001706\n",
      "iteration 1200 / 2000: loss 0.004202\n",
      "iteration 1300 / 2000: loss 0.013407\n",
      "iteration 1400 / 2000: loss 0.000271\n",
      "iteration 1500 / 2000: loss 0.003844\n",
      "iteration 1600 / 2000: loss 0.001489\n",
      "iteration 1700 / 2000: loss 0.000359\n",
      "iteration 1800 / 2000: loss 0.001886\n",
      "iteration 1900 / 2000: loss 0.001004\n",
      "iteration 0 / 2000: loss 0.925774\n",
      "iteration 100 / 2000: loss 0.010481\n",
      "iteration 200 / 2000: loss 0.006601\n",
      "iteration 300 / 2000: loss 0.005776\n",
      "iteration 400 / 2000: loss 0.004384\n",
      "iteration 500 / 2000: loss 0.003871\n",
      "iteration 600 / 2000: loss 0.007091\n",
      "iteration 700 / 2000: loss 0.002688\n",
      "iteration 800 / 2000: loss 0.003998\n",
      "iteration 900 / 2000: loss 0.004157\n",
      "iteration 1000 / 2000: loss 0.089957\n",
      "iteration 1100 / 2000: loss 0.033002\n",
      "iteration 1200 / 2000: loss 0.004249\n",
      "iteration 1300 / 2000: loss 0.004037\n",
      "iteration 1400 / 2000: loss 0.004643\n",
      "iteration 1500 / 2000: loss 0.008082\n",
      "iteration 1600 / 2000: loss 0.012811\n",
      "iteration 1700 / 2000: loss 0.002727\n",
      "iteration 1800 / 2000: loss 0.005460\n",
      "iteration 1900 / 2000: loss 0.008584\n",
      "iteration 0 / 2000: loss 0.980859\n",
      "iteration 100 / 2000: loss 0.000966\n",
      "iteration 200 / 2000: loss 0.000285\n",
      "iteration 300 / 2000: loss 0.009772\n",
      "iteration 400 / 2000: loss 0.003332\n",
      "iteration 500 / 2000: loss 0.057453\n",
      "iteration 600 / 2000: loss 0.005039\n",
      "iteration 700 / 2000: loss 0.008941\n",
      "iteration 800 / 2000: loss 0.010503\n",
      "iteration 900 / 2000: loss 0.005576\n",
      "iteration 1000 / 2000: loss 0.001800\n",
      "iteration 1100 / 2000: loss 0.032290\n",
      "iteration 1200 / 2000: loss 0.015961\n",
      "iteration 1300 / 2000: loss 0.021296\n",
      "iteration 1400 / 2000: loss 0.001856\n",
      "iteration 1500 / 2000: loss 0.000644\n",
      "iteration 1600 / 2000: loss 0.009217\n",
      "iteration 1700 / 2000: loss 0.039206\n",
      "iteration 1800 / 2000: loss 0.006023\n",
      "iteration 1900 / 2000: loss 0.002809\n",
      "iteration 0 / 2000: loss 0.971396\n",
      "iteration 100 / 2000: loss 0.000365\n",
      "iteration 200 / 2000: loss 0.000060\n",
      "iteration 300 / 2000: loss 0.002795\n",
      "iteration 400 / 2000: loss 0.000876\n",
      "iteration 500 / 2000: loss 0.001629\n",
      "iteration 600 / 2000: loss 0.004049\n",
      "iteration 700 / 2000: loss 0.000057\n",
      "iteration 800 / 2000: loss 0.000046\n",
      "iteration 900 / 2000: loss 0.005698\n",
      "iteration 1000 / 2000: loss 0.003008\n",
      "iteration 1100 / 2000: loss 0.000779\n",
      "iteration 1200 / 2000: loss 0.002803\n",
      "iteration 1300 / 2000: loss 0.002433\n",
      "iteration 1400 / 2000: loss 0.000906\n",
      "iteration 1500 / 2000: loss 0.000126\n",
      "iteration 1600 / 2000: loss 0.000063\n",
      "iteration 1700 / 2000: loss 0.000376\n",
      "iteration 1800 / 2000: loss 0.001233\n",
      "iteration 1900 / 2000: loss 0.001306\n",
      "iteration 0 / 2000: loss 0.995145\n",
      "iteration 100 / 2000: loss 0.090095\n",
      "iteration 200 / 2000: loss 0.035301\n",
      "iteration 300 / 2000: loss 0.020162\n",
      "iteration 400 / 2000: loss 0.005271\n",
      "iteration 500 / 2000: loss 0.003508\n",
      "iteration 600 / 2000: loss 0.002844\n",
      "iteration 700 / 2000: loss 0.000756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 2000: loss 0.000501\n",
      "iteration 900 / 2000: loss 0.002305\n",
      "iteration 1000 / 2000: loss 0.000866\n",
      "iteration 1100 / 2000: loss 0.009002\n",
      "iteration 1200 / 2000: loss 0.000732\n",
      "iteration 1300 / 2000: loss 0.004140\n",
      "iteration 1400 / 2000: loss 0.003511\n",
      "iteration 1500 / 2000: loss 0.010312\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.007109\n",
      "iteration 1800 / 2000: loss 0.000088\n",
      "iteration 1900 / 2000: loss 0.000284\n",
      "iteration 0 / 2000: loss 0.984133\n",
      "iteration 100 / 2000: loss 0.136371\n",
      "iteration 200 / 2000: loss 0.052586\n",
      "iteration 300 / 2000: loss 0.021955\n",
      "iteration 400 / 2000: loss 0.008104\n",
      "iteration 500 / 2000: loss 0.004408\n",
      "iteration 600 / 2000: loss 0.002187\n",
      "iteration 700 / 2000: loss 0.004025\n",
      "iteration 800 / 2000: loss 0.000328\n",
      "iteration 900 / 2000: loss 0.028638\n",
      "iteration 1000 / 2000: loss 0.000552\n",
      "iteration 1100 / 2000: loss 0.001709\n",
      "iteration 1200 / 2000: loss 0.004200\n",
      "iteration 1300 / 2000: loss 0.013404\n",
      "iteration 1400 / 2000: loss 0.000267\n",
      "iteration 1500 / 2000: loss 0.003841\n",
      "iteration 1600 / 2000: loss 0.001486\n",
      "iteration 1700 / 2000: loss 0.000356\n",
      "iteration 1800 / 2000: loss 0.001884\n",
      "iteration 1900 / 2000: loss 0.001003\n",
      "iteration 0 / 2000: loss 0.925771\n",
      "iteration 100 / 2000: loss 0.024412\n",
      "iteration 200 / 2000: loss 0.008650\n",
      "iteration 300 / 2000: loss 0.003817\n",
      "iteration 400 / 2000: loss 0.001354\n",
      "iteration 500 / 2000: loss 0.000821\n",
      "iteration 600 / 2000: loss 0.004359\n",
      "iteration 700 / 2000: loss 0.000348\n",
      "iteration 800 / 2000: loss 0.002032\n",
      "iteration 900 / 2000: loss 0.002519\n",
      "iteration 1000 / 2000: loss 0.088598\n",
      "iteration 1100 / 2000: loss 0.031876\n",
      "iteration 1200 / 2000: loss 0.003318\n",
      "iteration 1300 / 2000: loss 0.003267\n",
      "iteration 1400 / 2000: loss 0.004006\n",
      "iteration 1500 / 2000: loss 0.007555\n",
      "iteration 1600 / 2000: loss 0.012375\n",
      "iteration 1700 / 2000: loss 0.002367\n",
      "iteration 1800 / 2000: loss 0.005162\n",
      "iteration 1900 / 2000: loss 0.008338\n",
      "iteration 0 / 2000: loss 0.980854\n",
      "iteration 100 / 2000: loss 0.000976\n",
      "iteration 200 / 2000: loss 0.000274\n",
      "iteration 300 / 2000: loss 0.009755\n",
      "iteration 400 / 2000: loss 0.003315\n",
      "iteration 500 / 2000: loss 0.057457\n",
      "iteration 600 / 2000: loss 0.005034\n",
      "iteration 700 / 2000: loss 0.008933\n",
      "iteration 800 / 2000: loss 0.010495\n",
      "iteration 900 / 2000: loss 0.005569\n",
      "iteration 1000 / 2000: loss 0.001794\n",
      "iteration 1100 / 2000: loss 0.032285\n",
      "iteration 1200 / 2000: loss 0.015956\n",
      "iteration 1300 / 2000: loss 0.021292\n",
      "iteration 1400 / 2000: loss 0.001854\n",
      "iteration 1500 / 2000: loss 0.000642\n",
      "iteration 1600 / 2000: loss 0.009215\n",
      "iteration 1700 / 2000: loss 0.039204\n",
      "iteration 1800 / 2000: loss 0.006022\n",
      "iteration 1900 / 2000: loss 0.002808\n",
      "iteration 0 / 2000: loss 0.971405\n",
      "iteration 100 / 2000: loss 0.047293\n",
      "iteration 200 / 2000: loss 0.039072\n",
      "iteration 300 / 2000: loss 0.033099\n",
      "iteration 400 / 2000: loss 0.027632\n",
      "iteration 500 / 2000: loss 0.021917\n",
      "iteration 600 / 2000: loss 0.025571\n",
      "iteration 700 / 2000: loss 0.017227\n",
      "iteration 800 / 2000: loss 0.012937\n",
      "iteration 900 / 2000: loss 0.018726\n",
      "iteration 1000 / 2000: loss 0.009008\n",
      "iteration 1100 / 2000: loss 0.008284\n",
      "iteration 1200 / 2000: loss 0.006279\n",
      "iteration 1300 / 2000: loss 0.005046\n",
      "iteration 1400 / 2000: loss 0.007390\n",
      "iteration 1500 / 2000: loss 0.003717\n",
      "iteration 1600 / 2000: loss 0.003173\n",
      "iteration 1700 / 2000: loss 0.007949\n",
      "iteration 1800 / 2000: loss 0.004091\n",
      "iteration 1900 / 2000: loss 0.002772\n",
      "iteration 0 / 2000: loss 0.995147\n",
      "iteration 100 / 2000: loss 0.029518\n",
      "iteration 200 / 2000: loss 0.017533\n",
      "iteration 300 / 2000: loss 0.028153\n",
      "iteration 400 / 2000: loss 0.012493\n",
      "iteration 500 / 2000: loss 0.010358\n",
      "iteration 600 / 2000: loss 0.008290\n",
      "iteration 700 / 2000: loss 0.009680\n",
      "iteration 800 / 2000: loss 0.005624\n",
      "iteration 900 / 2000: loss 0.006857\n",
      "iteration 1000 / 2000: loss 0.005897\n",
      "iteration 1100 / 2000: loss 0.008140\n",
      "iteration 1200 / 2000: loss 0.003179\n",
      "iteration 1300 / 2000: loss 0.005214\n",
      "iteration 1400 / 2000: loss 0.002531\n",
      "iteration 1500 / 2000: loss 0.005506\n",
      "iteration 1600 / 2000: loss 0.009285\n",
      "iteration 1700 / 2000: loss 0.009995\n",
      "iteration 1800 / 2000: loss 0.014156\n",
      "iteration 1900 / 2000: loss 0.000917\n",
      "iteration 0 / 2000: loss 0.984128\n",
      "iteration 100 / 2000: loss 0.003629\n",
      "iteration 200 / 2000: loss 0.004986\n",
      "iteration 300 / 2000: loss 0.002590\n",
      "iteration 400 / 2000: loss 0.001436\n",
      "iteration 500 / 2000: loss 0.008475\n",
      "iteration 600 / 2000: loss 0.002931\n",
      "iteration 700 / 2000: loss 0.005777\n",
      "iteration 800 / 2000: loss 0.016853\n",
      "iteration 900 / 2000: loss 0.021182\n",
      "iteration 1000 / 2000: loss 0.001549\n",
      "iteration 1100 / 2000: loss 0.005328\n",
      "iteration 1200 / 2000: loss 0.008462\n",
      "iteration 1300 / 2000: loss 0.027715\n",
      "iteration 1400 / 2000: loss 0.003383\n",
      "iteration 1500 / 2000: loss 0.002594\n",
      "iteration 1600 / 2000: loss 0.009288\n",
      "iteration 1700 / 2000: loss 0.000591\n",
      "iteration 1800 / 2000: loss 0.002528\n",
      "iteration 1900 / 2000: loss 0.001438\n",
      "iteration 0 / 2000: loss 0.925771\n",
      "iteration 100 / 2000: loss 0.010859\n",
      "iteration 200 / 2000: loss 0.002621\n",
      "iteration 300 / 2000: loss 0.011341\n",
      "iteration 400 / 2000: loss 0.017688\n",
      "iteration 500 / 2000: loss 0.046639\n",
      "iteration 600 / 2000: loss 0.013144\n",
      "iteration 700 / 2000: loss 0.016859\n",
      "iteration 800 / 2000: loss 0.010956\n",
      "iteration 900 / 2000: loss 0.005435\n",
      "iteration 1000 / 2000: loss 0.106218\n",
      "iteration 1100 / 2000: loss 0.004709\n",
      "iteration 1200 / 2000: loss 0.018294\n",
      "iteration 1300 / 2000: loss 0.004845\n",
      "iteration 1400 / 2000: loss 0.004201\n",
      "iteration 1500 / 2000: loss 0.002825\n",
      "iteration 1600 / 2000: loss 0.011084\n",
      "iteration 1700 / 2000: loss 0.003003\n",
      "iteration 1800 / 2000: loss 0.005697\n",
      "iteration 1900 / 2000: loss 0.015502\n",
      "iteration 0 / 2000: loss 0.980863\n",
      "iteration 100 / 2000: loss 1.242002\n",
      "iteration 200 / 2000: loss 1.040422\n",
      "iteration 300 / 2000: loss 0.880290\n",
      "iteration 400 / 2000: loss 0.713852\n",
      "iteration 500 / 2000: loss 0.658592\n",
      "iteration 600 / 2000: loss 0.502817\n",
      "iteration 700 / 2000: loss 0.402043\n",
      "iteration 800 / 2000: loss 0.339519\n",
      "iteration 900 / 2000: loss 0.278792\n",
      "iteration 1000 / 2000: loss 0.226180\n",
      "iteration 1100 / 2000: loss 0.231524\n",
      "iteration 1200 / 2000: loss 0.165914\n",
      "iteration 1300 / 2000: loss 0.168109\n",
      "iteration 1400 / 2000: loss 0.166213\n",
      "iteration 1500 / 2000: loss 0.103192\n",
      "iteration 1600 / 2000: loss 0.124953\n",
      "iteration 1700 / 2000: loss 0.145405\n",
      "iteration 1800 / 2000: loss 0.061325\n",
      "iteration 1900 / 2000: loss 0.044634\n",
      "iteration 0 / 2000: loss 0.971399\n",
      "iteration 100 / 2000: loss 0.007998\n",
      "iteration 200 / 2000: loss 0.006900\n",
      "iteration 300 / 2000: loss 0.007736\n",
      "iteration 400 / 2000: loss 0.007153\n",
      "iteration 500 / 2000: loss 0.006316\n",
      "iteration 600 / 2000: loss 0.013642\n",
      "iteration 700 / 2000: loss 0.008321\n",
      "iteration 800 / 2000: loss 0.006504\n",
      "iteration 900 / 2000: loss 0.014324\n",
      "iteration 1000 / 2000: loss 0.006269\n",
      "iteration 1100 / 2000: loss 0.006902\n",
      "iteration 1200 / 2000: loss 0.006003\n",
      "iteration 1300 / 2000: loss 0.005668\n",
      "iteration 1400 / 2000: loss 0.008738\n",
      "iteration 1500 / 2000: loss 0.005651\n",
      "iteration 1600 / 2000: loss 0.005576\n",
      "iteration 1700 / 2000: loss 0.010723\n",
      "iteration 1800 / 2000: loss 0.007158\n",
      "iteration 1900 / 2000: loss 0.006067\n",
      "iteration 0 / 2000: loss 0.995147\n",
      "iteration 100 / 2000: loss 0.011399\n",
      "iteration 200 / 2000: loss 0.003595\n",
      "iteration 300 / 2000: loss 0.016677\n",
      "iteration 400 / 2000: loss 0.003317\n",
      "iteration 500 / 2000: loss 0.003077\n",
      "iteration 600 / 2000: loss 0.002571\n",
      "iteration 700 / 2000: loss 0.005247\n",
      "iteration 800 / 2000: loss 0.002249\n",
      "iteration 900 / 2000: loss 0.004351\n",
      "iteration 1000 / 2000: loss 0.004104\n",
      "iteration 1100 / 2000: loss 0.006931\n",
      "iteration 1200 / 2000: loss 0.002449\n",
      "iteration 1300 / 2000: loss 0.004833\n",
      "iteration 1400 / 2000: loss 0.002505\n",
      "iteration 1500 / 2000: loss 0.005739\n",
      "iteration 1600 / 2000: loss 0.009665\n",
      "iteration 1700 / 2000: loss 0.010605\n",
      "iteration 1800 / 2000: loss 0.014900\n",
      "iteration 1900 / 2000: loss 0.001768\n",
      "iteration 0 / 2000: loss 0.984127\n",
      "iteration 100 / 2000: loss 0.002830\n",
      "iteration 200 / 2000: loss 0.002965\n",
      "iteration 300 / 2000: loss 0.001535\n",
      "iteration 400 / 2000: loss 0.000611\n",
      "iteration 500 / 2000: loss 0.007840\n",
      "iteration 600 / 2000: loss 0.002452\n",
      "iteration 700 / 2000: loss 0.003395\n",
      "iteration 800 / 2000: loss 0.017797\n",
      "iteration 900 / 2000: loss 0.022171\n",
      "iteration 1000 / 2000: loss 0.006225\n",
      "iteration 1100 / 2000: loss 0.006367\n",
      "iteration 1200 / 2000: loss 0.009557\n",
      "iteration 1300 / 2000: loss 0.028827\n",
      "iteration 1400 / 2000: loss 0.004507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1500 / 2000: loss 0.003723\n",
      "iteration 1600 / 2000: loss 0.010418\n",
      "iteration 1700 / 2000: loss 0.001718\n",
      "iteration 1800 / 2000: loss 0.003642\n",
      "iteration 1900 / 2000: loss 0.002550\n",
      "iteration 0 / 2000: loss 0.925775\n",
      "iteration 100 / 2000: loss 0.012497\n",
      "iteration 200 / 2000: loss 0.004456\n",
      "iteration 300 / 2000: loss 0.013484\n",
      "iteration 400 / 2000: loss 0.020100\n",
      "iteration 500 / 2000: loss 0.038432\n",
      "iteration 600 / 2000: loss 0.026972\n",
      "iteration 700 / 2000: loss 0.019578\n",
      "iteration 800 / 2000: loss 0.013775\n",
      "iteration 900 / 2000: loss 0.008326\n",
      "iteration 1000 / 2000: loss 0.109120\n",
      "iteration 1100 / 2000: loss 0.007688\n",
      "iteration 1200 / 2000: loss 0.021287\n",
      "iteration 1300 / 2000: loss 0.007840\n",
      "iteration 1400 / 2000: loss 0.007215\n",
      "iteration 1500 / 2000: loss 0.005794\n",
      "iteration 1600 / 2000: loss 0.014030\n",
      "iteration 1700 / 2000: loss 0.016637\n",
      "iteration 1800 / 2000: loss 0.010407\n",
      "iteration 1900 / 2000: loss 0.018337\n",
      "iteration 0 / 2000: loss 0.980863\n",
      "iteration 100 / 2000: loss 0.004960\n",
      "iteration 200 / 2000: loss 0.014059\n",
      "iteration 300 / 2000: loss 0.035766\n",
      "iteration 400 / 2000: loss 0.016174\n",
      "iteration 500 / 2000: loss 0.082341\n",
      "iteration 600 / 2000: loss 0.026959\n",
      "iteration 700 / 2000: loss 0.009186\n",
      "iteration 800 / 2000: loss 0.015653\n",
      "iteration 900 / 2000: loss 0.011295\n",
      "iteration 1000 / 2000: loss 0.005632\n",
      "iteration 1100 / 2000: loss 0.050798\n",
      "iteration 1200 / 2000: loss 0.016082\n",
      "iteration 1300 / 2000: loss 0.044803\n",
      "iteration 1400 / 2000: loss 0.064775\n",
      "iteration 1500 / 2000: loss 0.019851\n",
      "iteration 1600 / 2000: loss 0.056522\n",
      "iteration 1700 / 2000: loss 0.089374\n",
      "iteration 1800 / 2000: loss 0.015501\n",
      "iteration 1900 / 2000: loss 0.007240\n",
      "iteration 0 / 2000: loss 0.971389\n",
      "iteration 100 / 2000: loss 0.011742\n",
      "iteration 200 / 2000: loss 0.011433\n",
      "iteration 300 / 2000: loss 0.010684\n",
      "iteration 400 / 2000: loss 0.009379\n",
      "iteration 500 / 2000: loss 0.007886\n",
      "iteration 600 / 2000: loss 0.014618\n",
      "iteration 700 / 2000: loss 0.008730\n",
      "iteration 800 / 2000: loss 0.006469\n",
      "iteration 900 / 2000: loss 0.013868\n",
      "iteration 1000 / 2000: loss 0.005435\n",
      "iteration 1100 / 2000: loss 0.005733\n",
      "iteration 1200 / 2000: loss 0.004536\n",
      "iteration 1300 / 2000: loss 0.003938\n",
      "iteration 1400 / 2000: loss 0.006777\n",
      "iteration 1500 / 2000: loss 0.003486\n",
      "iteration 1600 / 2000: loss 0.003233\n",
      "iteration 1700 / 2000: loss 0.008226\n",
      "iteration 1800 / 2000: loss 0.004527\n",
      "iteration 1900 / 2000: loss 0.003321\n",
      "iteration 0 / 2000: loss 0.995148\n",
      "iteration 100 / 2000: loss 0.005390\n",
      "iteration 200 / 2000: loss 0.001665\n",
      "iteration 300 / 2000: loss 0.014426\n",
      "iteration 400 / 2000: loss 0.001472\n",
      "iteration 500 / 2000: loss 0.001257\n",
      "iteration 600 / 2000: loss 0.000775\n",
      "iteration 700 / 2000: loss 0.003476\n",
      "iteration 800 / 2000: loss 0.000504\n",
      "iteration 900 / 2000: loss 0.002632\n",
      "iteration 1000 / 2000: loss 0.002410\n",
      "iteration 1100 / 2000: loss 0.005264\n",
      "iteration 1200 / 2000: loss 0.000807\n",
      "iteration 1300 / 2000: loss 0.003218\n",
      "iteration 1400 / 2000: loss 0.000921\n",
      "iteration 1500 / 2000: loss 0.004180\n",
      "iteration 1600 / 2000: loss 0.008194\n",
      "iteration 1700 / 2000: loss 0.009096\n",
      "iteration 1800 / 2000: loss 0.013417\n",
      "iteration 1900 / 2000: loss 0.000309\n",
      "iteration 0 / 2000: loss 0.984135\n",
      "iteration 100 / 2000: loss 0.110278\n",
      "iteration 200 / 2000: loss 0.100431\n",
      "iteration 300 / 2000: loss 0.091609\n",
      "iteration 400 / 2000: loss 0.082813\n",
      "iteration 500 / 2000: loss 0.076855\n",
      "iteration 600 / 2000: loss 0.070346\n",
      "iteration 700 / 2000: loss 0.078751\n",
      "iteration 800 / 2000: loss 0.083426\n",
      "iteration 900 / 2000: loss 0.081764\n",
      "iteration 1000 / 2000: loss 0.060314\n",
      "iteration 1100 / 2000: loss 0.055375\n",
      "iteration 1200 / 2000: loss 0.052505\n",
      "iteration 1300 / 2000: loss 0.069266\n",
      "iteration 1400 / 2000: loss 0.041185\n",
      "iteration 1500 / 2000: loss 0.037030\n",
      "iteration 1600 / 2000: loss 0.040582\n",
      "iteration 1700 / 2000: loss 0.029053\n",
      "iteration 1800 / 2000: loss 0.028411\n",
      "iteration 1900 / 2000: loss 0.024987\n",
      "iteration 0 / 2000: loss 0.925760\n",
      "iteration 100 / 2000: loss 0.008534\n",
      "iteration 200 / 2000: loss 0.003424\n",
      "iteration 300 / 2000: loss 0.008298\n",
      "iteration 400 / 2000: loss 0.018735\n",
      "iteration 500 / 2000: loss 0.024243\n",
      "iteration 600 / 2000: loss 0.015922\n",
      "iteration 700 / 2000: loss 0.017845\n",
      "iteration 800 / 2000: loss 0.009150\n",
      "iteration 900 / 2000: loss 0.006393\n",
      "iteration 1000 / 2000: loss 0.107144\n",
      "iteration 1100 / 2000: loss 0.005597\n",
      "iteration 1200 / 2000: loss 0.019139\n",
      "iteration 1300 / 2000: loss 0.005644\n",
      "iteration 1400 / 2000: loss 0.004981\n",
      "iteration 1500 / 2000: loss 0.004472\n",
      "iteration 1600 / 2000: loss 0.011794\n",
      "iteration 1700 / 2000: loss 0.014380\n",
      "iteration 1800 / 2000: loss 0.008135\n",
      "iteration 1900 / 2000: loss 0.016055\n",
      "iteration 0 / 2000: loss 0.980858\n",
      "iteration 100 / 2000: loss 0.032640\n",
      "iteration 200 / 2000: loss 0.039163\n",
      "iteration 300 / 2000: loss 0.058530\n",
      "iteration 400 / 2000: loss 0.036512\n",
      "iteration 500 / 2000: loss 0.100534\n",
      "iteration 600 / 2000: loss 0.043208\n",
      "iteration 700 / 2000: loss 0.023671\n",
      "iteration 800 / 2000: loss 0.028170\n",
      "iteration 900 / 2000: loss 0.022727\n",
      "iteration 1000 / 2000: loss 0.015701\n",
      "iteration 1100 / 2000: loss 0.059730\n",
      "iteration 1200 / 2000: loss 0.023940\n",
      "iteration 1300 / 2000: loss 0.051689\n",
      "iteration 1400 / 2000: loss 0.070782\n",
      "iteration 1500 / 2000: loss 0.025064\n",
      "iteration 1600 / 2000: loss 0.061061\n",
      "iteration 1700 / 2000: loss 0.093220\n",
      "iteration 1800 / 2000: loss 0.018762\n",
      "iteration 1900 / 2000: loss 0.009974\n",
      "iteration 0 / 2000: loss 0.971401\n",
      "iteration 100 / 2000: loss 7.674501\n",
      "iteration 200 / 2000: loss 2.961345\n",
      "iteration 300 / 2000: loss 1.144407\n",
      "iteration 400 / 2000: loss 0.441571\n",
      "iteration 500 / 2000: loss 0.171042\n",
      "iteration 600 / 2000: loss 0.070030\n",
      "iteration 700 / 2000: loss 0.025648\n",
      "iteration 800 / 2000: loss 0.009970\n",
      "iteration 900 / 2000: loss 0.005992\n",
      "iteration 1000 / 2000: loss 0.003390\n",
      "iteration 1100 / 2000: loss 0.001482\n",
      "iteration 1200 / 2000: loss 0.001822\n",
      "iteration 1300 / 2000: loss 0.001370\n",
      "iteration 1400 / 2000: loss 0.001071\n",
      "iteration 1500 / 2000: loss 0.000136\n",
      "iteration 1600 / 2000: loss 0.000238\n",
      "iteration 1700 / 2000: loss 0.000258\n",
      "iteration 1800 / 2000: loss 0.001916\n",
      "iteration 1900 / 2000: loss 0.000572\n",
      "iteration 0 / 2000: loss 0.995155\n",
      "iteration 100 / 2000: loss 12.325586\n",
      "iteration 200 / 2000: loss 4.756793\n",
      "iteration 300 / 2000: loss 1.842074\n",
      "iteration 400 / 2000: loss 0.708208\n",
      "iteration 500 / 2000: loss 0.274221\n",
      "iteration 600 / 2000: loss 0.106457\n",
      "iteration 700 / 2000: loss 0.042004\n",
      "iteration 800 / 2000: loss 0.016099\n",
      "iteration 900 / 2000: loss 0.008019\n",
      "iteration 1000 / 2000: loss 0.003209\n",
      "iteration 1100 / 2000: loss 0.012188\n",
      "iteration 1200 / 2000: loss 0.001083\n",
      "iteration 1300 / 2000: loss 0.004350\n",
      "iteration 1400 / 2000: loss 0.005170\n",
      "iteration 1500 / 2000: loss 0.003399\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.006753\n",
      "iteration 1800 / 2000: loss 0.000205\n",
      "iteration 1900 / 2000: loss 0.000147\n",
      "iteration 0 / 2000: loss 0.984125\n",
      "iteration 100 / 2000: loss 1.894299\n",
      "iteration 200 / 2000: loss 0.730885\n",
      "iteration 300 / 2000: loss 0.282856\n",
      "iteration 400 / 2000: loss 0.109092\n",
      "iteration 500 / 2000: loss 0.043318\n",
      "iteration 600 / 2000: loss 0.016917\n",
      "iteration 700 / 2000: loss 0.008201\n",
      "iteration 800 / 2000: loss 0.002628\n",
      "iteration 900 / 2000: loss 0.020672\n",
      "iteration 1000 / 2000: loss 0.000830\n",
      "iteration 1100 / 2000: loss 0.000840\n",
      "iteration 1200 / 2000: loss 0.003996\n",
      "iteration 1300 / 2000: loss 0.022328\n",
      "iteration 1400 / 2000: loss 0.000334\n",
      "iteration 1500 / 2000: loss 0.002860\n",
      "iteration 1600 / 2000: loss 0.001098\n",
      "iteration 1700 / 2000: loss 0.000294\n",
      "iteration 1800 / 2000: loss 0.001773\n",
      "iteration 1900 / 2000: loss 0.000695\n",
      "iteration 0 / 2000: loss 0.925770\n",
      "iteration 100 / 2000: loss 4.600165\n",
      "iteration 200 / 2000: loss 1.774141\n",
      "iteration 300 / 2000: loss 0.685059\n",
      "iteration 400 / 2000: loss 0.266845\n",
      "iteration 500 / 2000: loss 0.255723\n",
      "iteration 600 / 2000: loss 0.102673\n",
      "iteration 700 / 2000: loss 0.038288\n",
      "iteration 800 / 2000: loss 0.018307\n",
      "iteration 900 / 2000: loss 0.012788\n",
      "iteration 1000 / 2000: loss 0.094873\n",
      "iteration 1100 / 2000: loss 0.021912\n",
      "iteration 1200 / 2000: loss 0.007707\n",
      "iteration 1300 / 2000: loss 0.003496\n",
      "iteration 1400 / 2000: loss 0.006345\n",
      "iteration 1500 / 2000: loss 0.004155\n",
      "iteration 1600 / 2000: loss 0.011823\n",
      "iteration 1700 / 2000: loss 0.003265\n",
      "iteration 1800 / 2000: loss 0.005369\n",
      "iteration 1900 / 2000: loss 0.007347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 0.980863\n",
      "iteration 100 / 2000: loss 1.043743\n",
      "iteration 200 / 2000: loss 0.413478\n",
      "iteration 300 / 2000: loss 0.172213\n",
      "iteration 400 / 2000: loss 0.068791\n",
      "iteration 500 / 2000: loss 0.110012\n",
      "iteration 600 / 2000: loss 0.020732\n",
      "iteration 700 / 2000: loss 0.013932\n",
      "iteration 800 / 2000: loss 0.012030\n",
      "iteration 900 / 2000: loss 0.006525\n",
      "iteration 1000 / 2000: loss 0.001949\n",
      "iteration 1100 / 2000: loss 0.029103\n",
      "iteration 1200 / 2000: loss 0.014411\n",
      "iteration 1300 / 2000: loss 0.021279\n",
      "iteration 1400 / 2000: loss 0.008637\n",
      "iteration 1500 / 2000: loss 0.001761\n",
      "iteration 1600 / 2000: loss 0.031800\n",
      "iteration 1700 / 2000: loss 0.049100\n",
      "iteration 1800 / 2000: loss 0.006184\n",
      "iteration 1900 / 2000: loss 0.005461\n",
      "iteration 0 / 2000: loss 0.971403\n",
      "iteration 100 / 2000: loss 0.286123\n",
      "iteration 200 / 2000: loss 0.260107\n",
      "iteration 300 / 2000: loss 0.238199\n",
      "iteration 400 / 2000: loss 0.215671\n",
      "iteration 500 / 2000: loss 0.196430\n",
      "iteration 600 / 2000: loss 0.182179\n",
      "iteration 700 / 2000: loss 0.161998\n",
      "iteration 800 / 2000: loss 0.147220\n",
      "iteration 900 / 2000: loss 0.135918\n",
      "iteration 1000 / 2000: loss 0.123513\n",
      "iteration 1100 / 2000: loss 0.111478\n",
      "iteration 1200 / 2000: loss 0.102141\n",
      "iteration 1300 / 2000: loss 0.092709\n",
      "iteration 1400 / 2000: loss 0.084174\n",
      "iteration 1500 / 2000: loss 0.075722\n",
      "iteration 1600 / 2000: loss 0.068979\n",
      "iteration 1700 / 2000: loss 0.062771\n",
      "iteration 1800 / 2000: loss 0.058763\n",
      "iteration 1900 / 2000: loss 0.052267\n",
      "iteration 0 / 2000: loss 0.995153\n",
      "iteration 100 / 2000: loss 0.378060\n",
      "iteration 200 / 2000: loss 0.344549\n",
      "iteration 300 / 2000: loss 0.319380\n",
      "iteration 400 / 2000: loss 0.284193\n",
      "iteration 500 / 2000: loss 0.259326\n",
      "iteration 600 / 2000: loss 0.235946\n",
      "iteration 700 / 2000: loss 0.214947\n",
      "iteration 800 / 2000: loss 0.194662\n",
      "iteration 900 / 2000: loss 0.178614\n",
      "iteration 1000 / 2000: loss 0.161510\n",
      "iteration 1100 / 2000: loss 0.157364\n",
      "iteration 1200 / 2000: loss 0.133571\n",
      "iteration 1300 / 2000: loss 0.125010\n",
      "iteration 1400 / 2000: loss 0.114964\n",
      "iteration 1500 / 2000: loss 0.103267\n",
      "iteration 1600 / 2000: loss 0.091097\n",
      "iteration 1700 / 2000: loss 0.089349\n",
      "iteration 1800 / 2000: loss 0.075316\n",
      "iteration 1900 / 2000: loss 0.068449\n",
      "iteration 0 / 2000: loss 0.984129\n",
      "iteration 100 / 2000: loss 0.850063\n",
      "iteration 200 / 2000: loss 0.772682\n",
      "iteration 300 / 2000: loss 0.703402\n",
      "iteration 400 / 2000: loss 0.639126\n",
      "iteration 500 / 2000: loss 0.582263\n",
      "iteration 600 / 2000: loss 0.528986\n",
      "iteration 700 / 2000: loss 0.482333\n",
      "iteration 800 / 2000: loss 0.437053\n",
      "iteration 900 / 2000: loss 0.416980\n",
      "iteration 1000 / 2000: loss 0.361701\n",
      "iteration 1100 / 2000: loss 0.329187\n",
      "iteration 1200 / 2000: loss 0.302652\n",
      "iteration 1300 / 2000: loss 0.293939\n",
      "iteration 1400 / 2000: loss 0.247335\n",
      "iteration 1500 / 2000: loss 0.227475\n",
      "iteration 1600 / 2000: loss 0.205354\n",
      "iteration 1700 / 2000: loss 0.186035\n",
      "iteration 1800 / 2000: loss 0.170678\n",
      "iteration 1900 / 2000: loss 0.154289\n",
      "iteration 0 / 2000: loss 0.925777\n",
      "iteration 100 / 2000: loss 0.466500\n",
      "iteration 200 / 2000: loss 0.421510\n",
      "iteration 300 / 2000: loss 0.383562\n",
      "iteration 400 / 2000: loss 0.350963\n",
      "iteration 500 / 2000: loss 0.316985\n",
      "iteration 600 / 2000: loss 0.292074\n",
      "iteration 700 / 2000: loss 0.262107\n",
      "iteration 800 / 2000: loss 0.241743\n",
      "iteration 900 / 2000: loss 0.219250\n",
      "iteration 1000 / 2000: loss 0.287887\n",
      "iteration 1100 / 2000: loss 0.199463\n",
      "iteration 1200 / 2000: loss 0.169947\n",
      "iteration 1300 / 2000: loss 0.151333\n",
      "iteration 1400 / 2000: loss 0.140898\n",
      "iteration 1500 / 2000: loss 0.126555\n",
      "iteration 1600 / 2000: loss 0.123145\n",
      "iteration 1700 / 2000: loss 0.104503\n",
      "iteration 1800 / 2000: loss 0.097433\n",
      "iteration 1900 / 2000: loss 0.091067\n",
      "iteration 0 / 2000: loss 0.980860\n",
      "iteration 100 / 2000: loss 0.318585\n",
      "iteration 200 / 2000: loss 0.300072\n",
      "iteration 300 / 2000: loss 0.276640\n",
      "iteration 400 / 2000: loss 0.246693\n",
      "iteration 500 / 2000: loss 0.303791\n",
      "iteration 600 / 2000: loss 0.209295\n",
      "iteration 700 / 2000: loss 0.190163\n",
      "iteration 800 / 2000: loss 0.174129\n",
      "iteration 900 / 2000: loss 0.154640\n",
      "iteration 1000 / 2000: loss 0.136911\n",
      "iteration 1100 / 2000: loss 0.151937\n",
      "iteration 1200 / 2000: loss 0.126150\n",
      "iteration 1300 / 2000: loss 0.122906\n",
      "iteration 1400 / 2000: loss 0.101057\n",
      "iteration 1500 / 2000: loss 0.085807\n",
      "iteration 1600 / 2000: loss 0.108227\n",
      "iteration 1700 / 2000: loss 0.118600\n",
      "iteration 1800 / 2000: loss 0.069384\n",
      "iteration 1900 / 2000: loss 0.062932\n",
      "iteration 0 / 2000: loss 0.971408\n",
      "iteration 100 / 2000: loss 0.260543\n",
      "iteration 200 / 2000: loss 0.161918\n",
      "iteration 300 / 2000: loss 0.102330\n",
      "iteration 400 / 2000: loss 0.063171\n",
      "iteration 500 / 2000: loss 0.039761\n",
      "iteration 600 / 2000: loss 0.028530\n",
      "iteration 700 / 2000: loss 0.015326\n",
      "iteration 800 / 2000: loss 0.009524\n",
      "iteration 900 / 2000: loss 0.008018\n",
      "iteration 1000 / 2000: loss 0.005539\n",
      "iteration 1100 / 2000: loss 0.003161\n",
      "iteration 1200 / 2000: loss 0.002998\n",
      "iteration 1300 / 2000: loss 0.002152\n",
      "iteration 1400 / 2000: loss 0.001577\n",
      "iteration 1500 / 2000: loss 0.000458\n",
      "iteration 1600 / 2000: loss 0.000441\n",
      "iteration 1700 / 2000: loss 0.000385\n",
      "iteration 1800 / 2000: loss 0.001996\n",
      "iteration 1900 / 2000: loss 0.000622\n",
      "iteration 0 / 2000: loss 0.995148\n",
      "iteration 100 / 2000: loss 3.238339\n",
      "iteration 200 / 2000: loss 2.013353\n",
      "iteration 300 / 2000: loss 1.257718\n",
      "iteration 400 / 2000: loss 0.777485\n",
      "iteration 500 / 2000: loss 0.484156\n",
      "iteration 600 / 2000: loss 0.301331\n",
      "iteration 700 / 2000: loss 0.187972\n",
      "iteration 800 / 2000: loss 0.116410\n",
      "iteration 900 / 2000: loss 0.074066\n",
      "iteration 1000 / 2000: loss 0.045686\n",
      "iteration 1100 / 2000: loss 0.039141\n",
      "iteration 1200 / 2000: loss 0.018047\n",
      "iteration 1300 / 2000: loss 0.014976\n",
      "iteration 1400 / 2000: loss 0.011806\n",
      "iteration 1500 / 2000: loss 0.007536\n",
      "iteration 1600 / 2000: loss 0.002847\n",
      "iteration 1700 / 2000: loss 0.008355\n",
      "iteration 1800 / 2000: loss 0.001202\n",
      "iteration 1900 / 2000: loss 0.000767\n",
      "iteration 0 / 2000: loss 0.984137\n",
      "iteration 100 / 2000: loss 0.342903\n",
      "iteration 200 / 2000: loss 0.212946\n",
      "iteration 300 / 2000: loss 0.133153\n",
      "iteration 400 / 2000: loss 0.082499\n",
      "iteration 500 / 2000: loss 0.052431\n",
      "iteration 600 / 2000: loss 0.032475\n",
      "iteration 700 / 2000: loss 0.021689\n",
      "iteration 800 / 2000: loss 0.012484\n",
      "iteration 900 / 2000: loss 0.027366\n",
      "iteration 1000 / 2000: loss 0.005210\n",
      "iteration 1100 / 2000: loss 0.003647\n",
      "iteration 1200 / 2000: loss 0.005774\n",
      "iteration 1300 / 2000: loss 0.023445\n",
      "iteration 1400 / 2000: loss 0.001033\n",
      "iteration 1500 / 2000: loss 0.003297\n",
      "iteration 1600 / 2000: loss 0.001370\n",
      "iteration 1700 / 2000: loss 0.000463\n",
      "iteration 1800 / 2000: loss 0.001878\n",
      "iteration 1900 / 2000: loss 0.000761\n",
      "iteration 0 / 2000: loss 0.925773\n",
      "iteration 100 / 2000: loss 2.888856\n",
      "iteration 200 / 2000: loss 1.793805\n",
      "iteration 300 / 2000: loss 1.115290\n",
      "iteration 400 / 2000: loss 0.695544\n",
      "iteration 500 / 2000: loss 0.430931\n",
      "iteration 600 / 2000: loss 0.271758\n",
      "iteration 700 / 2000: loss 0.166603\n",
      "iteration 800 / 2000: loss 0.107020\n",
      "iteration 900 / 2000: loss 0.066983\n",
      "iteration 1000 / 2000: loss 0.130928\n",
      "iteration 1100 / 2000: loss 0.045237\n",
      "iteration 1200 / 2000: loss 0.022557\n",
      "iteration 1300 / 2000: loss 0.012863\n",
      "iteration 1400 / 2000: loss 0.012219\n",
      "iteration 1500 / 2000: loss 0.007826\n",
      "iteration 1600 / 2000: loss 0.014112\n",
      "iteration 1700 / 2000: loss 0.004691\n",
      "iteration 1800 / 2000: loss 0.006257\n",
      "iteration 1900 / 2000: loss 0.007899\n",
      "iteration 0 / 2000: loss 0.980862\n",
      "iteration 100 / 2000: loss 0.314272\n",
      "iteration 200 / 2000: loss 0.205940\n",
      "iteration 300 / 2000: loss 0.134934\n",
      "iteration 400 / 2000: loss 0.082960\n",
      "iteration 500 / 2000: loss 0.133233\n",
      "iteration 600 / 2000: loss 0.040724\n",
      "iteration 700 / 2000: loss 0.028503\n",
      "iteration 800 / 2000: loss 0.021914\n",
      "iteration 900 / 2000: loss 0.012988\n",
      "iteration 1000 / 2000: loss 0.006089\n",
      "iteration 1100 / 2000: loss 0.031724\n",
      "iteration 1200 / 2000: loss 0.016058\n",
      "iteration 1300 / 2000: loss 0.022310\n",
      "iteration 1400 / 2000: loss 0.009280\n",
      "iteration 1500 / 2000: loss 0.002162\n",
      "iteration 1600 / 2000: loss 0.032049\n",
      "iteration 1700 / 2000: loss 0.049255\n",
      "iteration 1800 / 2000: loss 0.006281\n",
      "iteration 1900 / 2000: loss 0.005521\n",
      "iteration 0 / 2000: loss 0.971399\n",
      "iteration 100 / 2000: loss 0.053800\n",
      "iteration 200 / 2000: loss 0.027196\n",
      "iteration 300 / 2000: loss 0.005728\n",
      "iteration 400 / 2000: loss 0.018865\n",
      "iteration 500 / 2000: loss 0.008907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 2000: loss 0.016755\n",
      "iteration 700 / 2000: loss 0.003702\n",
      "iteration 800 / 2000: loss 0.003993\n",
      "iteration 900 / 2000: loss 0.005636\n",
      "iteration 1000 / 2000: loss 0.002192\n",
      "iteration 1100 / 2000: loss 0.002609\n",
      "iteration 1200 / 2000: loss 0.001838\n",
      "iteration 1300 / 2000: loss 0.003915\n",
      "iteration 1400 / 2000: loss 0.002174\n",
      "iteration 1500 / 2000: loss 0.001022\n",
      "iteration 1600 / 2000: loss 0.001065\n",
      "iteration 1700 / 2000: loss 0.004004\n",
      "iteration 1800 / 2000: loss 0.002471\n",
      "iteration 1900 / 2000: loss 0.001099\n",
      "iteration 0 / 2000: loss 0.995147\n",
      "iteration 100 / 2000: loss 0.073507\n",
      "iteration 200 / 2000: loss 0.004600\n",
      "iteration 300 / 2000: loss 0.007412\n",
      "iteration 400 / 2000: loss 0.002340\n",
      "iteration 500 / 2000: loss 0.001342\n",
      "iteration 600 / 2000: loss 0.001559\n",
      "iteration 700 / 2000: loss 0.002934\n",
      "iteration 800 / 2000: loss 0.001191\n",
      "iteration 900 / 2000: loss 0.002600\n",
      "iteration 1000 / 2000: loss 0.001915\n",
      "iteration 1100 / 2000: loss 0.009160\n",
      "iteration 1200 / 2000: loss 0.001837\n",
      "iteration 1300 / 2000: loss 0.007394\n",
      "iteration 1400 / 2000: loss 0.002041\n",
      "iteration 1500 / 2000: loss 0.015284\n",
      "iteration 1600 / 2000: loss 0.000711\n",
      "iteration 1700 / 2000: loss 0.009769\n",
      "iteration 1800 / 2000: loss 0.001104\n",
      "iteration 1900 / 2000: loss 0.000565\n",
      "iteration 0 / 2000: loss 0.984124\n",
      "iteration 100 / 2000: loss 0.001991\n",
      "iteration 200 / 2000: loss 0.006734\n",
      "iteration 300 / 2000: loss 0.001665\n",
      "iteration 400 / 2000: loss 0.001184\n",
      "iteration 500 / 2000: loss 0.001985\n",
      "iteration 600 / 2000: loss 0.001913\n",
      "iteration 700 / 2000: loss 0.006413\n",
      "iteration 800 / 2000: loss 0.001145\n",
      "iteration 900 / 2000: loss 0.026068\n",
      "iteration 1000 / 2000: loss 0.001061\n",
      "iteration 1100 / 2000: loss 0.002219\n",
      "iteration 1200 / 2000: loss 0.009603\n",
      "iteration 1300 / 2000: loss 0.305165\n",
      "iteration 1400 / 2000: loss 0.000912\n",
      "iteration 1500 / 2000: loss 0.076572\n",
      "iteration 1600 / 2000: loss 0.001912\n",
      "iteration 1700 / 2000: loss 0.000984\n",
      "iteration 1800 / 2000: loss 0.002472\n",
      "iteration 1900 / 2000: loss 0.001080\n",
      "iteration 0 / 2000: loss 0.925781\n",
      "iteration 100 / 2000: loss 0.012441\n",
      "iteration 200 / 2000: loss 0.001376\n",
      "iteration 300 / 2000: loss 0.002141\n",
      "iteration 400 / 2000: loss 0.000676\n",
      "iteration 500 / 2000: loss 0.001078\n",
      "iteration 600 / 2000: loss 0.008428\n",
      "iteration 700 / 2000: loss 0.000478\n",
      "iteration 800 / 2000: loss 0.001489\n",
      "iteration 900 / 2000: loss 0.003493\n",
      "iteration 1000 / 2000: loss 0.075877\n",
      "iteration 1100 / 2000: loss 0.029332\n",
      "iteration 1200 / 2000: loss 0.008777\n",
      "iteration 1300 / 2000: loss 0.003585\n",
      "iteration 1400 / 2000: loss 0.005415\n",
      "iteration 1500 / 2000: loss 0.006951\n",
      "iteration 1600 / 2000: loss 0.012715\n",
      "iteration 1700 / 2000: loss 0.003002\n",
      "iteration 1800 / 2000: loss 0.003896\n",
      "iteration 1900 / 2000: loss 0.015424\n",
      "iteration 0 / 2000: loss 0.980864\n",
      "iteration 100 / 2000: loss 0.001507\n",
      "iteration 200 / 2000: loss 0.010214\n",
      "iteration 300 / 2000: loss 0.017554\n",
      "iteration 400 / 2000: loss 0.008236\n",
      "iteration 500 / 2000: loss 0.043792\n",
      "iteration 600 / 2000: loss 0.006142\n",
      "iteration 700 / 2000: loss 0.012037\n",
      "iteration 800 / 2000: loss 0.011287\n",
      "iteration 900 / 2000: loss 0.005061\n",
      "iteration 1000 / 2000: loss 0.002082\n",
      "iteration 1100 / 2000: loss 0.036233\n",
      "iteration 1200 / 2000: loss 0.016618\n",
      "iteration 1300 / 2000: loss 0.022463\n",
      "iteration 1400 / 2000: loss 0.005090\n",
      "iteration 1500 / 2000: loss 0.003914\n",
      "iteration 1600 / 2000: loss 0.033817\n",
      "iteration 1700 / 2000: loss 0.045178\n",
      "iteration 1800 / 2000: loss 0.004943\n",
      "iteration 1900 / 2000: loss 0.007315\n",
      "iteration 0 / 2000: loss 0.971396\n",
      "iteration 100 / 2000: loss 0.054739\n",
      "iteration 200 / 2000: loss 0.027070\n",
      "iteration 300 / 2000: loss 0.005525\n",
      "iteration 400 / 2000: loss 0.018345\n",
      "iteration 500 / 2000: loss 0.008280\n",
      "iteration 600 / 2000: loss 0.016546\n",
      "iteration 700 / 2000: loss 0.003454\n",
      "iteration 800 / 2000: loss 0.003403\n",
      "iteration 900 / 2000: loss 0.004683\n",
      "iteration 1000 / 2000: loss 0.001372\n",
      "iteration 1100 / 2000: loss 0.001869\n",
      "iteration 1200 / 2000: loss 0.001134\n",
      "iteration 1300 / 2000: loss 0.003128\n",
      "iteration 1400 / 2000: loss 0.001430\n",
      "iteration 1500 / 2000: loss 0.000340\n",
      "iteration 1600 / 2000: loss 0.000364\n",
      "iteration 1700 / 2000: loss 0.003327\n",
      "iteration 1800 / 2000: loss 0.001922\n",
      "iteration 1900 / 2000: loss 0.000517\n",
      "iteration 0 / 2000: loss 0.995144\n",
      "iteration 100 / 2000: loss 0.072808\n",
      "iteration 200 / 2000: loss 0.004313\n",
      "iteration 300 / 2000: loss 0.007406\n",
      "iteration 400 / 2000: loss 0.002149\n",
      "iteration 500 / 2000: loss 0.000431\n",
      "iteration 600 / 2000: loss 0.000780\n",
      "iteration 700 / 2000: loss 0.002097\n",
      "iteration 800 / 2000: loss 0.000407\n",
      "iteration 900 / 2000: loss 0.001966\n",
      "iteration 1000 / 2000: loss 0.000902\n",
      "iteration 1100 / 2000: loss 0.008373\n",
      "iteration 1200 / 2000: loss 0.000919\n",
      "iteration 1300 / 2000: loss 0.008123\n",
      "iteration 1400 / 2000: loss 0.001400\n",
      "iteration 1500 / 2000: loss 0.016407\n",
      "iteration 1600 / 2000: loss 0.000183\n",
      "iteration 1700 / 2000: loss 0.009770\n",
      "iteration 1800 / 2000: loss 0.000571\n",
      "iteration 1900 / 2000: loss 0.000087\n",
      "iteration 0 / 2000: loss 0.984124\n",
      "iteration 100 / 2000: loss 0.001744\n",
      "iteration 200 / 2000: loss 0.006652\n",
      "iteration 300 / 2000: loss 0.000845\n",
      "iteration 400 / 2000: loss 0.000373\n",
      "iteration 500 / 2000: loss 0.243913\n",
      "iteration 600 / 2000: loss 0.002745\n",
      "iteration 700 / 2000: loss 0.003428\n",
      "iteration 800 / 2000: loss 0.000398\n",
      "iteration 900 / 2000: loss 0.023546\n",
      "iteration 1000 / 2000: loss 0.000453\n",
      "iteration 1100 / 2000: loss 0.001783\n",
      "iteration 1200 / 2000: loss 0.004510\n",
      "iteration 1300 / 2000: loss 0.096627\n",
      "iteration 1400 / 2000: loss 0.008162\n",
      "iteration 1500 / 2000: loss 0.001929\n",
      "iteration 1600 / 2000: loss 0.007258\n",
      "iteration 1700 / 2000: loss 0.000419\n",
      "iteration 1800 / 2000: loss 0.002101\n",
      "iteration 1900 / 2000: loss 0.000577\n",
      "iteration 0 / 2000: loss 0.925767\n",
      "iteration 100 / 2000: loss 0.012488\n",
      "iteration 200 / 2000: loss 0.000567\n",
      "iteration 300 / 2000: loss 0.001450\n",
      "iteration 400 / 2000: loss 0.000480\n",
      "iteration 500 / 2000: loss 0.000846\n",
      "iteration 600 / 2000: loss 0.008145\n",
      "iteration 700 / 2000: loss 0.000352\n",
      "iteration 800 / 2000: loss 0.001269\n",
      "iteration 900 / 2000: loss 0.003345\n",
      "iteration 1000 / 2000: loss 0.076073\n",
      "iteration 1100 / 2000: loss 0.029295\n",
      "iteration 1200 / 2000: loss 0.008584\n",
      "iteration 1300 / 2000: loss 0.003420\n",
      "iteration 1400 / 2000: loss 0.005231\n",
      "iteration 1500 / 2000: loss 0.006699\n",
      "iteration 1600 / 2000: loss 0.012510\n",
      "iteration 1700 / 2000: loss 0.002811\n",
      "iteration 1800 / 2000: loss 0.003709\n",
      "iteration 1900 / 2000: loss 0.015312\n",
      "iteration 0 / 2000: loss 0.980871\n",
      "iteration 100 / 2000: loss 0.000700\n",
      "iteration 200 / 2000: loss 0.010129\n",
      "iteration 300 / 2000: loss 0.017111\n",
      "iteration 400 / 2000: loss 0.007731\n",
      "iteration 500 / 2000: loss 0.041566\n",
      "iteration 600 / 2000: loss 0.005635\n",
      "iteration 700 / 2000: loss 0.011998\n",
      "iteration 800 / 2000: loss 0.011147\n",
      "iteration 900 / 2000: loss 0.004807\n",
      "iteration 1000 / 2000: loss 0.001977\n",
      "iteration 1100 / 2000: loss 0.038536\n",
      "iteration 1200 / 2000: loss 0.016595\n",
      "iteration 1300 / 2000: loss 0.022325\n",
      "iteration 1400 / 2000: loss 0.004562\n",
      "iteration 1500 / 2000: loss 0.003591\n",
      "iteration 1600 / 2000: loss 0.031894\n",
      "iteration 1700 / 2000: loss 0.044118\n",
      "iteration 1800 / 2000: loss 0.004848\n",
      "iteration 1900 / 2000: loss 0.007093\n",
      "iteration 0 / 2000: loss 0.971399\n",
      "iteration 100 / 2000: loss 0.054569\n",
      "iteration 200 / 2000: loss 0.027156\n",
      "iteration 300 / 2000: loss 0.005615\n",
      "iteration 400 / 2000: loss 0.018604\n",
      "iteration 500 / 2000: loss 0.008576\n",
      "iteration 600 / 2000: loss 0.016750\n",
      "iteration 700 / 2000: loss 0.003562\n",
      "iteration 800 / 2000: loss 0.003635\n",
      "iteration 900 / 2000: loss 0.005112\n",
      "iteration 1000 / 2000: loss 0.001742\n",
      "iteration 1100 / 2000: loss 0.002201\n",
      "iteration 1200 / 2000: loss 0.001449\n",
      "iteration 1300 / 2000: loss 0.003485\n",
      "iteration 1400 / 2000: loss 0.001763\n",
      "iteration 1500 / 2000: loss 0.000647\n",
      "iteration 1600 / 2000: loss 0.000680\n",
      "iteration 1700 / 2000: loss 0.003629\n",
      "iteration 1800 / 2000: loss 0.002167\n",
      "iteration 1900 / 2000: loss 0.000785\n",
      "iteration 0 / 2000: loss 0.995139\n",
      "iteration 100 / 2000: loss 0.071738\n",
      "iteration 200 / 2000: loss 0.004422\n",
      "iteration 300 / 2000: loss 0.007622\n",
      "iteration 400 / 2000: loss 0.002017\n",
      "iteration 500 / 2000: loss 0.000834\n",
      "iteration 600 / 2000: loss 0.001133\n",
      "iteration 700 / 2000: loss 0.002468\n",
      "iteration 800 / 2000: loss 0.000759\n",
      "iteration 900 / 2000: loss 0.002242\n",
      "iteration 1000 / 2000: loss 0.001354\n",
      "iteration 1100 / 2000: loss 0.008662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 2000: loss 0.001341\n",
      "iteration 1300 / 2000: loss 0.007345\n",
      "iteration 1400 / 2000: loss 0.001706\n",
      "iteration 1500 / 2000: loss 0.015460\n",
      "iteration 1600 / 2000: loss 0.000448\n",
      "iteration 1700 / 2000: loss 0.009687\n",
      "iteration 1800 / 2000: loss 0.000843\n",
      "iteration 1900 / 2000: loss 0.000334\n",
      "iteration 0 / 2000: loss 0.984134\n",
      "iteration 100 / 2000: loss 0.001837\n",
      "iteration 200 / 2000: loss 0.006446\n",
      "iteration 300 / 2000: loss 0.001207\n",
      "iteration 400 / 2000: loss 0.000735\n",
      "iteration 500 / 2000: loss 0.039146\n",
      "iteration 600 / 2000: loss 0.001327\n",
      "iteration 700 / 2000: loss 0.005650\n",
      "iteration 800 / 2000: loss 0.000901\n",
      "iteration 900 / 2000: loss 0.028059\n",
      "iteration 1000 / 2000: loss 0.000685\n",
      "iteration 1100 / 2000: loss 0.002097\n",
      "iteration 1200 / 2000: loss 0.006994\n",
      "iteration 1300 / 2000: loss 0.245974\n",
      "iteration 1400 / 2000: loss 0.000675\n",
      "iteration 1500 / 2000: loss 0.003010\n",
      "iteration 1600 / 2000: loss 0.001661\n",
      "iteration 1700 / 2000: loss 0.000736\n",
      "iteration 1800 / 2000: loss 0.002169\n",
      "iteration 1900 / 2000: loss 0.000826\n",
      "iteration 0 / 2000: loss 0.925768\n",
      "iteration 100 / 2000: loss 0.012733\n",
      "iteration 200 / 2000: loss 0.000929\n",
      "iteration 300 / 2000: loss 0.001756\n",
      "iteration 400 / 2000: loss 0.000570\n",
      "iteration 500 / 2000: loss 0.000956\n",
      "iteration 600 / 2000: loss 0.008220\n",
      "iteration 700 / 2000: loss 0.000407\n",
      "iteration 800 / 2000: loss 0.001359\n",
      "iteration 900 / 2000: loss 0.003417\n",
      "iteration 1000 / 2000: loss 0.076161\n",
      "iteration 1100 / 2000: loss 0.029364\n",
      "iteration 1200 / 2000: loss 0.008676\n",
      "iteration 1300 / 2000: loss 0.003500\n",
      "iteration 1400 / 2000: loss 0.005307\n",
      "iteration 1500 / 2000: loss 0.006811\n",
      "iteration 1600 / 2000: loss 0.012604\n",
      "iteration 1700 / 2000: loss 0.002902\n",
      "iteration 1800 / 2000: loss 0.003784\n",
      "iteration 1900 / 2000: loss 0.015418\n",
      "iteration 0 / 2000: loss 0.980852\n",
      "iteration 100 / 2000: loss 0.001063\n",
      "iteration 200 / 2000: loss 0.009874\n",
      "iteration 300 / 2000: loss 0.017672\n",
      "iteration 400 / 2000: loss 0.007434\n",
      "iteration 500 / 2000: loss 0.039610\n",
      "iteration 600 / 2000: loss 0.005426\n",
      "iteration 700 / 2000: loss 0.011999\n",
      "iteration 800 / 2000: loss 0.011308\n",
      "iteration 900 / 2000: loss 0.004829\n",
      "iteration 1000 / 2000: loss 0.002081\n",
      "iteration 1100 / 2000: loss 0.041335\n",
      "iteration 1200 / 2000: loss 0.016878\n",
      "iteration 1300 / 2000: loss 0.022402\n",
      "iteration 1400 / 2000: loss 0.004195\n",
      "iteration 1500 / 2000: loss 0.003348\n",
      "iteration 1600 / 2000: loss 0.030115\n",
      "iteration 1700 / 2000: loss 0.043298\n",
      "iteration 1800 / 2000: loss 0.005063\n",
      "iteration 1900 / 2000: loss 0.007031\n",
      "iteration 0 / 2000: loss 0.971407\n",
      "iteration 100 / 2000: loss 0.050404\n",
      "iteration 200 / 2000: loss 0.004211\n",
      "iteration 300 / 2000: loss 0.001711\n",
      "iteration 400 / 2000: loss 0.001440\n",
      "iteration 500 / 2000: loss 0.000723\n",
      "iteration 600 / 2000: loss 0.002210\n",
      "iteration 700 / 2000: loss 0.000199\n",
      "iteration 800 / 2000: loss 0.000274\n",
      "iteration 900 / 2000: loss 0.011803\n",
      "iteration 1000 / 2000: loss 0.001169\n",
      "iteration 1100 / 2000: loss 0.001625\n",
      "iteration 1200 / 2000: loss 0.000597\n",
      "iteration 1300 / 2000: loss 0.000556\n",
      "iteration 1400 / 2000: loss 0.003625\n",
      "iteration 1500 / 2000: loss 0.000997\n",
      "iteration 1600 / 2000: loss 0.000441\n",
      "iteration 1700 / 2000: loss 0.007851\n",
      "iteration 1800 / 2000: loss 0.001973\n",
      "iteration 1900 / 2000: loss 0.008118\n",
      "iteration 0 / 2000: loss 0.995144\n",
      "iteration 100 / 2000: loss 0.062304\n",
      "iteration 200 / 2000: loss 0.003736\n",
      "iteration 300 / 2000: loss 0.006475\n",
      "iteration 400 / 2000: loss 0.001268\n",
      "iteration 500 / 2000: loss 0.001922\n",
      "iteration 600 / 2000: loss 0.001635\n",
      "iteration 700 / 2000: loss 0.004329\n",
      "iteration 800 / 2000: loss 0.001231\n",
      "iteration 900 / 2000: loss 0.003115\n",
      "iteration 1000 / 2000: loss 0.005174\n",
      "iteration 1100 / 2000: loss 0.006666\n",
      "iteration 1200 / 2000: loss 0.001425\n",
      "iteration 1300 / 2000: loss 0.035085\n",
      "iteration 1400 / 2000: loss 0.004069\n",
      "iteration 1500 / 2000: loss 0.003785\n",
      "iteration 1600 / 2000: loss 0.018846\n",
      "iteration 1700 / 2000: loss 0.018272\n",
      "iteration 1800 / 2000: loss 0.001058\n",
      "iteration 1900 / 2000: loss 0.000665\n",
      "iteration 0 / 2000: loss 0.984128\n",
      "iteration 100 / 2000: loss 0.001633\n",
      "iteration 200 / 2000: loss 0.001002\n",
      "iteration 300 / 2000: loss 0.001419\n",
      "iteration 400 / 2000: loss 0.000843\n",
      "iteration 500 / 2000: loss 0.012985\n",
      "iteration 600 / 2000: loss 0.001473\n",
      "iteration 700 / 2000: loss 0.014282\n",
      "iteration 800 / 2000: loss 0.012711\n",
      "iteration 900 / 2000: loss 0.021890\n",
      "iteration 1000 / 2000: loss 0.000959\n",
      "iteration 1100 / 2000: loss 0.006360\n",
      "iteration 1200 / 2000: loss 0.025941\n",
      "iteration 1300 / 2000: loss 0.027823\n",
      "iteration 1400 / 2000: loss 0.028541\n",
      "iteration 1500 / 2000: loss 0.001262\n",
      "iteration 1600 / 2000: loss 0.010288\n",
      "iteration 1700 / 2000: loss 0.001318\n",
      "iteration 1800 / 2000: loss 0.002986\n",
      "iteration 1900 / 2000: loss 0.005593\n",
      "iteration 0 / 2000: loss 0.925771\n",
      "iteration 100 / 2000: loss 0.003750\n",
      "iteration 200 / 2000: loss 0.000549\n",
      "iteration 300 / 2000: loss 0.003965\n",
      "iteration 400 / 2000: loss 0.002285\n",
      "iteration 500 / 2000: loss 0.000518\n",
      "iteration 600 / 2000: loss 0.037057\n",
      "iteration 700 / 2000: loss 0.056465\n",
      "iteration 800 / 2000: loss 0.013136\n",
      "iteration 900 / 2000: loss 0.003769\n",
      "iteration 1000 / 2000: loss 0.091963\n",
      "iteration 1100 / 2000: loss 0.003425\n",
      "iteration 1200 / 2000: loss 0.029047\n",
      "iteration 1300 / 2000: loss 0.004536\n",
      "iteration 1400 / 2000: loss 0.010218\n",
      "iteration 1500 / 2000: loss 0.006717\n",
      "iteration 1600 / 2000: loss 0.011809\n",
      "iteration 1700 / 2000: loss 0.035170\n",
      "iteration 1800 / 2000: loss 0.008117\n",
      "iteration 1900 / 2000: loss 0.039632\n",
      "iteration 0 / 2000: loss 0.980861\n",
      "iteration 100 / 2000: loss 0.001415\n",
      "iteration 200 / 2000: loss 0.001422\n",
      "iteration 300 / 2000: loss 0.045290\n",
      "iteration 400 / 2000: loss 0.013551\n",
      "iteration 500 / 2000: loss 0.100494\n",
      "iteration 600 / 2000: loss 0.043011\n",
      "iteration 700 / 2000: loss 0.007680\n",
      "iteration 800 / 2000: loss 0.016445\n",
      "iteration 900 / 2000: loss 0.007722\n",
      "iteration 1000 / 2000: loss 0.002142\n",
      "iteration 1100 / 2000: loss 0.032996\n",
      "iteration 1200 / 2000: loss 0.015127\n",
      "iteration 1300 / 2000: loss 0.053980\n",
      "iteration 1400 / 2000: loss 0.074653\n",
      "iteration 1500 / 2000: loss 0.014498\n",
      "iteration 1600 / 2000: loss 0.050557\n",
      "iteration 1700 / 2000: loss 0.108310\n",
      "iteration 1800 / 2000: loss 0.029668\n",
      "iteration 1900 / 2000: loss 0.005573\n",
      "iteration 0 / 2000: loss 0.971394\n",
      "iteration 100 / 2000: loss 0.012033\n",
      "iteration 200 / 2000: loss 0.000905\n",
      "iteration 300 / 2000: loss 0.002126\n",
      "iteration 400 / 2000: loss 0.002018\n",
      "iteration 500 / 2000: loss 0.000385\n",
      "iteration 600 / 2000: loss 0.003170\n",
      "iteration 700 / 2000: loss 0.000178\n",
      "iteration 800 / 2000: loss 0.000245\n",
      "iteration 900 / 2000: loss 0.012655\n",
      "iteration 1000 / 2000: loss 0.001074\n",
      "iteration 1100 / 2000: loss 0.001648\n",
      "iteration 1200 / 2000: loss 0.000609\n",
      "iteration 1300 / 2000: loss 0.000596\n",
      "iteration 1400 / 2000: loss 0.003664\n",
      "iteration 1500 / 2000: loss 0.000998\n",
      "iteration 1600 / 2000: loss 0.000500\n",
      "iteration 1700 / 2000: loss 0.007761\n",
      "iteration 1800 / 2000: loss 0.002060\n",
      "iteration 1900 / 2000: loss 0.008166\n",
      "iteration 0 / 2000: loss 0.995151\n",
      "iteration 100 / 2000: loss 0.016147\n",
      "iteration 200 / 2000: loss 0.002558\n",
      "iteration 300 / 2000: loss 0.006084\n",
      "iteration 400 / 2000: loss 0.000239\n",
      "iteration 500 / 2000: loss 0.000703\n",
      "iteration 600 / 2000: loss 0.000635\n",
      "iteration 700 / 2000: loss 0.003270\n",
      "iteration 800 / 2000: loss 0.000365\n",
      "iteration 900 / 2000: loss 0.002293\n",
      "iteration 1000 / 2000: loss 0.003320\n",
      "iteration 1100 / 2000: loss 0.006188\n",
      "iteration 1200 / 2000: loss 0.000769\n",
      "iteration 1300 / 2000: loss 0.018092\n",
      "iteration 1400 / 2000: loss 0.002711\n",
      "iteration 1500 / 2000: loss 0.003150\n",
      "iteration 1600 / 2000: loss 0.013694\n",
      "iteration 1700 / 2000: loss 0.014490\n",
      "iteration 1800 / 2000: loss 0.001447\n",
      "iteration 1900 / 2000: loss 0.000543\n",
      "iteration 0 / 2000: loss 0.984130\n",
      "iteration 100 / 2000: loss 0.022593\n",
      "iteration 200 / 2000: loss 0.002121\n",
      "iteration 300 / 2000: loss 0.002793\n",
      "iteration 400 / 2000: loss 0.000376\n",
      "iteration 500 / 2000: loss 0.014055\n",
      "iteration 600 / 2000: loss 0.008700\n",
      "iteration 700 / 2000: loss 0.001553\n",
      "iteration 800 / 2000: loss 0.026509\n",
      "iteration 900 / 2000: loss 0.017985\n",
      "iteration 1000 / 2000: loss 0.013295\n",
      "iteration 1100 / 2000: loss 0.000412\n",
      "iteration 1200 / 2000: loss 0.003990\n",
      "iteration 1300 / 2000: loss 0.027047\n",
      "iteration 1400 / 2000: loss 0.028331\n",
      "iteration 1500 / 2000: loss 0.001053\n",
      "iteration 1600 / 2000: loss 0.010838\n",
      "iteration 1700 / 2000: loss 0.001406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800 / 2000: loss 0.002826\n",
      "iteration 1900 / 2000: loss 0.005440\n",
      "iteration 0 / 2000: loss 0.925782\n",
      "iteration 100 / 2000: loss 0.002861\n",
      "iteration 200 / 2000: loss 0.000402\n",
      "iteration 300 / 2000: loss 0.001735\n",
      "iteration 400 / 2000: loss 0.005744\n",
      "iteration 500 / 2000: loss 0.043310\n",
      "iteration 600 / 2000: loss 0.032568\n",
      "iteration 700 / 2000: loss 0.055678\n",
      "iteration 800 / 2000: loss 0.013009\n",
      "iteration 900 / 2000: loss 0.003661\n",
      "iteration 1000 / 2000: loss 0.091921\n",
      "iteration 1100 / 2000: loss 0.003391\n",
      "iteration 1200 / 2000: loss 0.029359\n",
      "iteration 1300 / 2000: loss 0.004424\n",
      "iteration 1400 / 2000: loss 0.010565\n",
      "iteration 1500 / 2000: loss 0.006444\n",
      "iteration 1600 / 2000: loss 0.011692\n",
      "iteration 1700 / 2000: loss 0.035052\n",
      "iteration 1800 / 2000: loss 0.008075\n",
      "iteration 1900 / 2000: loss 0.011727\n",
      "iteration 0 / 2000: loss 0.980859\n",
      "iteration 100 / 2000: loss 0.000731\n",
      "iteration 200 / 2000: loss 0.000568\n",
      "iteration 300 / 2000: loss 0.074445\n",
      "iteration 400 / 2000: loss 0.015082\n",
      "iteration 500 / 2000: loss 0.081742\n",
      "iteration 600 / 2000: loss 0.042201\n",
      "iteration 700 / 2000: loss 0.011089\n",
      "iteration 800 / 2000: loss 0.010496\n",
      "iteration 900 / 2000: loss 0.007732\n",
      "iteration 1000 / 2000: loss 0.002500\n",
      "iteration 1100 / 2000: loss 0.032902\n",
      "iteration 1200 / 2000: loss 0.015006\n",
      "iteration 1300 / 2000: loss 0.053884\n",
      "iteration 1400 / 2000: loss 0.074428\n",
      "iteration 1500 / 2000: loss 0.026000\n",
      "iteration 1600 / 2000: loss 0.052231\n",
      "iteration 1700 / 2000: loss 0.109497\n",
      "iteration 1800 / 2000: loss 0.029627\n",
      "iteration 1900 / 2000: loss 0.007502\n",
      "iteration 0 / 2000: loss 0.971395\n",
      "iteration 100 / 2000: loss 0.032752\n",
      "iteration 200 / 2000: loss 0.001262\n",
      "iteration 300 / 2000: loss 0.002674\n",
      "iteration 400 / 2000: loss 0.001974\n",
      "iteration 500 / 2000: loss 0.000872\n",
      "iteration 600 / 2000: loss 0.003017\n",
      "iteration 700 / 2000: loss 0.000465\n",
      "iteration 800 / 2000: loss 0.000562\n",
      "iteration 900 / 2000: loss 0.011162\n",
      "iteration 1000 / 2000: loss 0.001453\n",
      "iteration 1100 / 2000: loss 0.001921\n",
      "iteration 1200 / 2000: loss 0.000853\n",
      "iteration 1300 / 2000: loss 0.000805\n",
      "iteration 1400 / 2000: loss 0.003937\n",
      "iteration 1500 / 2000: loss 0.001185\n",
      "iteration 1600 / 2000: loss 0.000685\n",
      "iteration 1700 / 2000: loss 0.007811\n",
      "iteration 1800 / 2000: loss 0.002229\n",
      "iteration 1900 / 2000: loss 0.006032\n",
      "iteration 0 / 2000: loss 0.995148\n",
      "iteration 100 / 2000: loss 0.003158\n",
      "iteration 200 / 2000: loss 0.002063\n",
      "iteration 300 / 2000: loss 0.006594\n",
      "iteration 400 / 2000: loss 0.000375\n",
      "iteration 500 / 2000: loss 0.001175\n",
      "iteration 600 / 2000: loss 0.000850\n",
      "iteration 700 / 2000: loss 0.002977\n",
      "iteration 800 / 2000: loss 0.000614\n",
      "iteration 900 / 2000: loss 0.002632\n",
      "iteration 1000 / 2000: loss 0.011727\n",
      "iteration 1100 / 2000: loss 0.004735\n",
      "iteration 1200 / 2000: loss 0.001601\n",
      "iteration 1300 / 2000: loss 0.008458\n",
      "iteration 1400 / 2000: loss 0.003724\n",
      "iteration 1500 / 2000: loss 0.003384\n",
      "iteration 1600 / 2000: loss 0.018527\n",
      "iteration 1700 / 2000: loss 0.018013\n",
      "iteration 1800 / 2000: loss 0.029069\n",
      "iteration 1900 / 2000: loss 0.000439\n",
      "iteration 0 / 2000: loss 0.984133\n",
      "iteration 100 / 2000: loss 0.001402\n",
      "iteration 200 / 2000: loss 0.000745\n",
      "iteration 300 / 2000: loss 0.001127\n",
      "iteration 400 / 2000: loss 0.000577\n",
      "iteration 500 / 2000: loss 0.013614\n",
      "iteration 600 / 2000: loss 0.001361\n",
      "iteration 700 / 2000: loss 0.003334\n",
      "iteration 800 / 2000: loss 0.004388\n",
      "iteration 900 / 2000: loss 0.033861\n",
      "iteration 1000 / 2000: loss 0.000695\n",
      "iteration 1100 / 2000: loss 0.009007\n",
      "iteration 1200 / 2000: loss 0.013159\n",
      "iteration 1300 / 2000: loss 0.028057\n",
      "iteration 1400 / 2000: loss 0.028033\n",
      "iteration 1500 / 2000: loss 0.001237\n",
      "iteration 1600 / 2000: loss 0.011011\n",
      "iteration 1700 / 2000: loss 0.001570\n",
      "iteration 1800 / 2000: loss 0.002984\n",
      "iteration 1900 / 2000: loss 0.005591\n",
      "iteration 0 / 2000: loss 0.925779\n",
      "iteration 100 / 2000: loss 0.004175\n",
      "iteration 200 / 2000: loss 0.001851\n",
      "iteration 300 / 2000: loss 0.002978\n",
      "iteration 400 / 2000: loss 0.006084\n",
      "iteration 500 / 2000: loss 0.053273\n",
      "iteration 600 / 2000: loss 0.021936\n",
      "iteration 700 / 2000: loss 0.057105\n",
      "iteration 800 / 2000: loss 0.014082\n",
      "iteration 900 / 2000: loss 0.004765\n",
      "iteration 1000 / 2000: loss 0.092924\n",
      "iteration 1100 / 2000: loss 0.004352\n",
      "iteration 1200 / 2000: loss 0.030284\n",
      "iteration 1300 / 2000: loss 0.005313\n",
      "iteration 1400 / 2000: loss 0.011459\n",
      "iteration 1500 / 2000: loss 0.004121\n",
      "iteration 1600 / 2000: loss 0.017609\n",
      "iteration 1700 / 2000: loss 0.035161\n",
      "iteration 1800 / 2000: loss 0.008813\n",
      "iteration 1900 / 2000: loss 0.040300\n",
      "iteration 0 / 2000: loss 0.980868\n",
      "iteration 100 / 2000: loss 0.000740\n",
      "iteration 200 / 2000: loss 0.002776\n",
      "iteration 300 / 2000: loss 0.063125\n",
      "iteration 400 / 2000: loss 0.013658\n",
      "iteration 500 / 2000: loss 0.099686\n",
      "iteration 600 / 2000: loss 0.009881\n",
      "iteration 700 / 2000: loss 0.004989\n",
      "iteration 800 / 2000: loss 0.020249\n",
      "iteration 900 / 2000: loss 0.006825\n",
      "iteration 1000 / 2000: loss 0.001805\n",
      "iteration 1100 / 2000: loss 0.022586\n",
      "iteration 1200 / 2000: loss 0.014156\n",
      "iteration 1300 / 2000: loss 0.023633\n",
      "iteration 1400 / 2000: loss 0.072710\n",
      "iteration 1500 / 2000: loss 0.014284\n",
      "iteration 1600 / 2000: loss 0.052022\n",
      "iteration 1700 / 2000: loss 0.108519\n",
      "iteration 1800 / 2000: loss 0.015714\n",
      "iteration 1900 / 2000: loss 0.003812\n",
      "iteration 0 / 2000: loss 0.971423\n",
      "iteration 100 / 2000: loss 0.028620\n",
      "iteration 200 / 2000: loss 0.004236\n",
      "iteration 300 / 2000: loss 0.003409\n",
      "iteration 400 / 2000: loss 0.000965\n",
      "iteration 500 / 2000: loss 0.001641\n",
      "iteration 600 / 2000: loss 0.004051\n",
      "iteration 700 / 2000: loss 0.000058\n",
      "iteration 800 / 2000: loss 0.000045\n",
      "iteration 900 / 2000: loss 0.005698\n",
      "iteration 1000 / 2000: loss 0.003008\n",
      "iteration 1100 / 2000: loss 0.000779\n",
      "iteration 1200 / 2000: loss 0.002803\n",
      "iteration 1300 / 2000: loss 0.002433\n",
      "iteration 1400 / 2000: loss 0.000906\n",
      "iteration 1500 / 2000: loss 0.000126\n",
      "iteration 1600 / 2000: loss 0.000063\n",
      "iteration 1700 / 2000: loss 0.000376\n",
      "iteration 1800 / 2000: loss 0.001233\n",
      "iteration 1900 / 2000: loss 0.001306\n",
      "iteration 0 / 2000: loss 0.995150\n",
      "iteration 100 / 2000: loss 0.003515\n",
      "iteration 200 / 2000: loss 0.001431\n",
      "iteration 300 / 2000: loss 0.006996\n",
      "iteration 400 / 2000: loss 0.000177\n",
      "iteration 500 / 2000: loss 0.001540\n",
      "iteration 600 / 2000: loss 0.002084\n",
      "iteration 700 / 2000: loss 0.000463\n",
      "iteration 800 / 2000: loss 0.000388\n",
      "iteration 900 / 2000: loss 0.002261\n",
      "iteration 1000 / 2000: loss 0.000850\n",
      "iteration 1100 / 2000: loss 0.008995\n",
      "iteration 1200 / 2000: loss 0.000730\n",
      "iteration 1300 / 2000: loss 0.004139\n",
      "iteration 1400 / 2000: loss 0.003511\n",
      "iteration 1500 / 2000: loss 0.010312\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.007109\n",
      "iteration 1800 / 2000: loss 0.000088\n",
      "iteration 1900 / 2000: loss 0.000284\n",
      "iteration 0 / 2000: loss 0.984129\n",
      "iteration 100 / 2000: loss 0.007751\n",
      "iteration 200 / 2000: loss 0.001917\n",
      "iteration 300 / 2000: loss 0.001986\n",
      "iteration 400 / 2000: loss 0.000336\n",
      "iteration 500 / 2000: loss 0.001401\n",
      "iteration 600 / 2000: loss 0.001026\n",
      "iteration 700 / 2000: loss 0.003576\n",
      "iteration 800 / 2000: loss 0.000155\n",
      "iteration 900 / 2000: loss 0.028571\n",
      "iteration 1000 / 2000: loss 0.000526\n",
      "iteration 1100 / 2000: loss 0.001699\n",
      "iteration 1200 / 2000: loss 0.004196\n",
      "iteration 1300 / 2000: loss 0.013402\n",
      "iteration 1400 / 2000: loss 0.000267\n",
      "iteration 1500 / 2000: loss 0.003841\n",
      "iteration 1600 / 2000: loss 0.001486\n",
      "iteration 1700 / 2000: loss 0.000356\n",
      "iteration 1800 / 2000: loss 0.001884\n",
      "iteration 1900 / 2000: loss 0.001003\n",
      "iteration 0 / 2000: loss 0.925790\n",
      "iteration 100 / 2000: loss 0.003934\n",
      "iteration 200 / 2000: loss 0.000512\n",
      "iteration 300 / 2000: loss 0.000641\n",
      "iteration 400 / 2000: loss 0.000124\n",
      "iteration 500 / 2000: loss 0.000345\n",
      "iteration 600 / 2000: loss 0.004176\n",
      "iteration 700 / 2000: loss 0.000277\n",
      "iteration 800 / 2000: loss 0.002005\n",
      "iteration 900 / 2000: loss 0.002509\n",
      "iteration 1000 / 2000: loss 0.088594\n",
      "iteration 1100 / 2000: loss 0.031875\n",
      "iteration 1200 / 2000: loss 0.003317\n",
      "iteration 1300 / 2000: loss 0.003267\n",
      "iteration 1400 / 2000: loss 0.004006\n",
      "iteration 1500 / 2000: loss 0.007555\n",
      "iteration 1600 / 2000: loss 0.012375\n",
      "iteration 1700 / 2000: loss 0.002367\n",
      "iteration 1800 / 2000: loss 0.005162\n",
      "iteration 1900 / 2000: loss 0.008338\n",
      "iteration 0 / 2000: loss 0.980884\n",
      "iteration 100 / 2000: loss 0.002187\n",
      "iteration 200 / 2000: loss 0.000443\n",
      "iteration 300 / 2000: loss 0.009781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 2000: loss 0.003317\n",
      "iteration 500 / 2000: loss 0.057437\n",
      "iteration 600 / 2000: loss 0.005026\n",
      "iteration 700 / 2000: loss 0.008930\n",
      "iteration 800 / 2000: loss 0.010494\n",
      "iteration 900 / 2000: loss 0.005569\n",
      "iteration 1000 / 2000: loss 0.001793\n",
      "iteration 1100 / 2000: loss 0.032285\n",
      "iteration 1200 / 2000: loss 0.015956\n",
      "iteration 1300 / 2000: loss 0.021292\n",
      "iteration 1400 / 2000: loss 0.001854\n",
      "iteration 1500 / 2000: loss 0.000642\n",
      "iteration 1600 / 2000: loss 0.009215\n",
      "iteration 1700 / 2000: loss 0.039204\n",
      "iteration 1800 / 2000: loss 0.006022\n",
      "iteration 1900 / 2000: loss 0.002808\n",
      "iteration 0 / 2000: loss 0.971406\n",
      "iteration 100 / 2000: loss 0.005436\n",
      "iteration 200 / 2000: loss 0.004277\n",
      "iteration 300 / 2000: loss 0.006290\n",
      "iteration 400 / 2000: loss 0.003769\n",
      "iteration 500 / 2000: loss 0.004023\n",
      "iteration 600 / 2000: loss 0.006029\n",
      "iteration 700 / 2000: loss 0.001695\n",
      "iteration 800 / 2000: loss 0.001400\n",
      "iteration 900 / 2000: loss 0.006818\n",
      "iteration 1000 / 2000: loss 0.003934\n",
      "iteration 1100 / 2000: loss 0.001544\n",
      "iteration 1200 / 2000: loss 0.003436\n",
      "iteration 1300 / 2000: loss 0.002956\n",
      "iteration 1400 / 2000: loss 0.001339\n",
      "iteration 1500 / 2000: loss 0.000484\n",
      "iteration 1600 / 2000: loss 0.000359\n",
      "iteration 1700 / 2000: loss 0.000621\n",
      "iteration 1800 / 2000: loss 0.001435\n",
      "iteration 1900 / 2000: loss 0.001473\n",
      "iteration 0 / 2000: loss 0.995138\n",
      "iteration 100 / 2000: loss 0.010080\n",
      "iteration 200 / 2000: loss 0.008735\n",
      "iteration 300 / 2000: loss 0.013312\n",
      "iteration 400 / 2000: loss 0.005441\n",
      "iteration 500 / 2000: loss 0.005898\n",
      "iteration 600 / 2000: loss 0.005689\n",
      "iteration 700 / 2000: loss 0.003444\n",
      "iteration 800 / 2000: loss 0.002853\n",
      "iteration 900 / 2000: loss 0.004300\n",
      "iteration 1000 / 2000: loss 0.002535\n",
      "iteration 1100 / 2000: loss 0.010389\n",
      "iteration 1200 / 2000: loss 0.001882\n",
      "iteration 1300 / 2000: loss 0.005092\n",
      "iteration 1400 / 2000: loss 0.004393\n",
      "iteration 1500 / 2000: loss 0.011042\n",
      "iteration 1600 / 2000: loss 0.000875\n",
      "iteration 1700 / 2000: loss 0.007608\n",
      "iteration 1800 / 2000: loss 0.000500\n",
      "iteration 1900 / 2000: loss 0.000625\n",
      "iteration 0 / 2000: loss 0.984129\n",
      "iteration 100 / 2000: loss 0.003089\n",
      "iteration 200 / 2000: loss 0.002298\n",
      "iteration 300 / 2000: loss 0.003493\n",
      "iteration 400 / 2000: loss 0.001759\n",
      "iteration 500 / 2000: loss 0.002605\n",
      "iteration 600 / 2000: loss 0.002025\n",
      "iteration 700 / 2000: loss 0.004403\n",
      "iteration 800 / 2000: loss 0.000838\n",
      "iteration 900 / 2000: loss 0.029137\n",
      "iteration 1000 / 2000: loss 0.000993\n",
      "iteration 1100 / 2000: loss 0.002086\n",
      "iteration 1200 / 2000: loss 0.004515\n",
      "iteration 1300 / 2000: loss 0.013666\n",
      "iteration 1400 / 2000: loss 0.000485\n",
      "iteration 1500 / 2000: loss 0.004021\n",
      "iteration 1600 / 2000: loss 0.001636\n",
      "iteration 1700 / 2000: loss 0.000480\n",
      "iteration 1800 / 2000: loss 0.001986\n",
      "iteration 1900 / 2000: loss 0.001087\n",
      "iteration 0 / 2000: loss 0.925775\n",
      "iteration 100 / 2000: loss 0.004087\n",
      "iteration 200 / 2000: loss 0.001314\n",
      "iteration 300 / 2000: loss 0.001404\n",
      "iteration 400 / 2000: loss 0.000769\n",
      "iteration 500 / 2000: loss 0.000881\n",
      "iteration 600 / 2000: loss 0.004619\n",
      "iteration 700 / 2000: loss 0.000644\n",
      "iteration 800 / 2000: loss 0.002308\n",
      "iteration 900 / 2000: loss 0.002760\n",
      "iteration 1000 / 2000: loss 0.088801\n",
      "iteration 1100 / 2000: loss 0.032046\n",
      "iteration 1200 / 2000: loss 0.003459\n",
      "iteration 1300 / 2000: loss 0.003384\n",
      "iteration 1400 / 2000: loss 0.004103\n",
      "iteration 1500 / 2000: loss 0.007635\n",
      "iteration 1600 / 2000: loss 0.012442\n",
      "iteration 1700 / 2000: loss 0.002422\n",
      "iteration 1800 / 2000: loss 0.005208\n",
      "iteration 1900 / 2000: loss 0.008375\n",
      "iteration 0 / 2000: loss 0.980871\n",
      "iteration 100 / 2000: loss 0.001531\n",
      "iteration 200 / 2000: loss 0.000752\n",
      "iteration 300 / 2000: loss 0.010160\n",
      "iteration 400 / 2000: loss 0.003653\n",
      "iteration 500 / 2000: loss 0.057718\n",
      "iteration 600 / 2000: loss 0.005259\n",
      "iteration 700 / 2000: loss 0.009123\n",
      "iteration 800 / 2000: loss 0.010653\n",
      "iteration 900 / 2000: loss 0.005700\n",
      "iteration 1000 / 2000: loss 0.001902\n",
      "iteration 1100 / 2000: loss 0.032375\n",
      "iteration 1200 / 2000: loss 0.016031\n",
      "iteration 1300 / 2000: loss 0.021354\n",
      "iteration 1400 / 2000: loss 0.001904\n",
      "iteration 1500 / 2000: loss 0.000684\n",
      "iteration 1600 / 2000: loss 0.009250\n",
      "iteration 1700 / 2000: loss 0.039233\n",
      "iteration 1800 / 2000: loss 0.006046\n",
      "iteration 1900 / 2000: loss 0.002828\n",
      "iteration 0 / 2000: loss 0.971392\n",
      "iteration 100 / 2000: loss 0.010359\n",
      "iteration 200 / 2000: loss 0.003917\n",
      "iteration 300 / 2000: loss 0.004283\n",
      "iteration 400 / 2000: loss 0.001450\n",
      "iteration 500 / 2000: loss 0.001850\n",
      "iteration 600 / 2000: loss 0.004135\n",
      "iteration 700 / 2000: loss 0.000090\n",
      "iteration 800 / 2000: loss 0.000058\n",
      "iteration 900 / 2000: loss 0.005703\n",
      "iteration 1000 / 2000: loss 0.003010\n",
      "iteration 1100 / 2000: loss 0.000780\n",
      "iteration 1200 / 2000: loss 0.002803\n",
      "iteration 1300 / 2000: loss 0.002433\n",
      "iteration 1400 / 2000: loss 0.000906\n",
      "iteration 1500 / 2000: loss 0.000126\n",
      "iteration 1600 / 2000: loss 0.000063\n",
      "iteration 1700 / 2000: loss 0.000376\n",
      "iteration 1800 / 2000: loss 0.001233\n",
      "iteration 1900 / 2000: loss 0.001306\n",
      "iteration 0 / 2000: loss 0.995180\n",
      "iteration 100 / 2000: loss 0.002257\n",
      "iteration 200 / 2000: loss 0.001786\n",
      "iteration 300 / 2000: loss 0.007230\n",
      "iteration 400 / 2000: loss 0.000281\n",
      "iteration 500 / 2000: loss 0.001582\n",
      "iteration 600 / 2000: loss 0.002101\n",
      "iteration 700 / 2000: loss 0.000469\n",
      "iteration 800 / 2000: loss 0.000391\n",
      "iteration 900 / 2000: loss 0.002262\n",
      "iteration 1000 / 2000: loss 0.000850\n",
      "iteration 1100 / 2000: loss 0.008995\n",
      "iteration 1200 / 2000: loss 0.000730\n",
      "iteration 1300 / 2000: loss 0.004140\n",
      "iteration 1400 / 2000: loss 0.003511\n",
      "iteration 1500 / 2000: loss 0.010312\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.007109\n",
      "iteration 1800 / 2000: loss 0.000088\n",
      "iteration 1900 / 2000: loss 0.000284\n",
      "iteration 0 / 2000: loss 0.984134\n",
      "iteration 100 / 2000: loss 1.239076\n",
      "iteration 200 / 2000: loss 0.478086\n",
      "iteration 300 / 2000: loss 0.186141\n",
      "iteration 400 / 2000: loss 0.071458\n",
      "iteration 500 / 2000: loss 0.028854\n",
      "iteration 600 / 2000: loss 0.011620\n",
      "iteration 700 / 2000: loss 0.007664\n",
      "iteration 800 / 2000: loss 0.001733\n",
      "iteration 900 / 2000: loss 0.029180\n",
      "iteration 1000 / 2000: loss 0.000761\n",
      "iteration 1100 / 2000: loss 0.001790\n",
      "iteration 1200 / 2000: loss 0.004231\n",
      "iteration 1300 / 2000: loss 0.013416\n",
      "iteration 1400 / 2000: loss 0.000272\n",
      "iteration 1500 / 2000: loss 0.003843\n",
      "iteration 1600 / 2000: loss 0.001487\n",
      "iteration 1700 / 2000: loss 0.000357\n",
      "iteration 1800 / 2000: loss 0.001884\n",
      "iteration 1900 / 2000: loss 0.001003\n",
      "iteration 0 / 2000: loss 0.925792\n",
      "iteration 100 / 2000: loss 0.005322\n",
      "iteration 200 / 2000: loss 0.001284\n",
      "iteration 300 / 2000: loss 0.000974\n",
      "iteration 400 / 2000: loss 0.000258\n",
      "iteration 500 / 2000: loss 0.000400\n",
      "iteration 600 / 2000: loss 0.004197\n",
      "iteration 700 / 2000: loss 0.000285\n",
      "iteration 800 / 2000: loss 0.002008\n",
      "iteration 900 / 2000: loss 0.002510\n",
      "iteration 1000 / 2000: loss 0.088594\n",
      "iteration 1100 / 2000: loss 0.031875\n",
      "iteration 1200 / 2000: loss 0.003317\n",
      "iteration 1300 / 2000: loss 0.003267\n",
      "iteration 1400 / 2000: loss 0.004006\n",
      "iteration 1500 / 2000: loss 0.007555\n",
      "iteration 1600 / 2000: loss 0.012375\n",
      "iteration 1700 / 2000: loss 0.002367\n",
      "iteration 1800 / 2000: loss 0.005162\n",
      "iteration 1900 / 2000: loss 0.008338\n",
      "iteration 0 / 2000: loss 0.980860\n",
      "iteration 100 / 2000: loss 0.003985\n",
      "iteration 200 / 2000: loss 0.001435\n",
      "iteration 300 / 2000: loss 0.010203\n",
      "iteration 400 / 2000: loss 0.003488\n",
      "iteration 500 / 2000: loss 0.057504\n",
      "iteration 600 / 2000: loss 0.005052\n",
      "iteration 700 / 2000: loss 0.008940\n",
      "iteration 800 / 2000: loss 0.010498\n",
      "iteration 900 / 2000: loss 0.005570\n",
      "iteration 1000 / 2000: loss 0.001794\n",
      "iteration 1100 / 2000: loss 0.032285\n",
      "iteration 1200 / 2000: loss 0.015956\n",
      "iteration 1300 / 2000: loss 0.021292\n",
      "iteration 1400 / 2000: loss 0.001854\n",
      "iteration 1500 / 2000: loss 0.000642\n",
      "iteration 1600 / 2000: loss 0.009215\n",
      "iteration 1700 / 2000: loss 0.039204\n",
      "iteration 1800 / 2000: loss 0.006022\n",
      "iteration 1900 / 2000: loss 0.002808\n",
      "iteration 0 / 2000: loss 0.971407\n",
      "iteration 100 / 2000: loss 451.918185\n",
      "iteration 200 / 2000: loss 373.683724\n",
      "iteration 300 / 2000: loss 308.994189\n",
      "iteration 400 / 2000: loss 255.502285\n",
      "iteration 500 / 2000: loss 211.270227\n",
      "iteration 600 / 2000: loss 174.703504\n",
      "iteration 700 / 2000: loss 144.455706\n",
      "iteration 800 / 2000: loss 119.446853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 2000: loss 98.776790\n",
      "iteration 1000 / 2000: loss 81.670516\n",
      "iteration 1100 / 2000: loss 67.532916\n",
      "iteration 1200 / 2000: loss 55.841347\n",
      "iteration 1300 / 2000: loss 46.174196\n",
      "iteration 1400 / 2000: loss 38.183942\n",
      "iteration 1500 / 2000: loss 31.571315\n",
      "iteration 1600 / 2000: loss 26.105929\n",
      "iteration 1700 / 2000: loss 21.591912\n",
      "iteration 1800 / 2000: loss 17.851534\n",
      "iteration 1900 / 2000: loss 14.760545\n",
      "iteration 0 / 2000: loss 0.995165\n",
      "iteration 100 / 2000: loss 0.204524\n",
      "iteration 200 / 2000: loss 0.163286\n",
      "iteration 300 / 2000: loss 0.148456\n",
      "iteration 400 / 2000: loss 0.111984\n",
      "iteration 500 / 2000: loss 0.092624\n",
      "iteration 600 / 2000: loss 0.076317\n",
      "iteration 700 / 2000: loss 0.065936\n",
      "iteration 800 / 2000: loss 0.052133\n",
      "iteration 900 / 2000: loss 0.045326\n",
      "iteration 1000 / 2000: loss 0.037699\n",
      "iteration 1100 / 2000: loss 0.034437\n",
      "iteration 1200 / 2000: loss 0.024924\n",
      "iteration 1300 / 2000: loss 0.022874\n",
      "iteration 1400 / 2000: loss 0.019225\n",
      "iteration 1500 / 2000: loss 0.019333\n",
      "iteration 1600 / 2000: loss 0.020702\n",
      "iteration 1700 / 2000: loss 0.019435\n",
      "iteration 1800 / 2000: loss 0.021962\n",
      "iteration 1900 / 2000: loss 0.007371\n",
      "iteration 0 / 2000: loss 0.984124\n",
      "iteration 100 / 2000: loss 0.621161\n",
      "iteration 200 / 2000: loss 0.515956\n",
      "iteration 300 / 2000: loss 0.425198\n",
      "iteration 400 / 2000: loss 0.351080\n",
      "iteration 500 / 2000: loss 0.293345\n",
      "iteration 600 / 2000: loss 0.241980\n",
      "iteration 700 / 2000: loss 0.212752\n",
      "iteration 800 / 2000: loss 0.188474\n",
      "iteration 900 / 2000: loss 0.163097\n",
      "iteration 1000 / 2000: loss 0.123563\n",
      "iteration 1100 / 2000: loss 0.102324\n",
      "iteration 1200 / 2000: loss 0.089938\n",
      "iteration 1300 / 2000: loss 0.094099\n",
      "iteration 1400 / 2000: loss 0.059975\n",
      "iteration 1500 / 2000: loss 0.048151\n",
      "iteration 1600 / 2000: loss 0.046957\n",
      "iteration 1700 / 2000: loss 0.031740\n",
      "iteration 1800 / 2000: loss 0.028024\n",
      "iteration 1900 / 2000: loss 0.022737\n",
      "iteration 0 / 2000: loss 0.925786\n",
      "iteration 100 / 2000: loss 1.102655\n",
      "iteration 200 / 2000: loss 0.907836\n",
      "iteration 300 / 2000: loss 0.760223\n",
      "iteration 400 / 2000: loss 0.636834\n",
      "iteration 500 / 2000: loss 0.555273\n",
      "iteration 600 / 2000: loss 0.446639\n",
      "iteration 700 / 2000: loss 0.366726\n",
      "iteration 800 / 2000: loss 0.300028\n",
      "iteration 900 / 2000: loss 0.244657\n",
      "iteration 1000 / 2000: loss 0.297884\n",
      "iteration 1100 / 2000: loss 0.169145\n",
      "iteration 1200 / 2000: loss 0.154448\n",
      "iteration 1300 / 2000: loss 0.117429\n",
      "iteration 1400 / 2000: loss 0.097322\n",
      "iteration 1500 / 2000: loss 0.080424\n",
      "iteration 1600 / 2000: loss 0.075138\n",
      "iteration 1700 / 2000: loss 0.055229\n",
      "iteration 1800 / 2000: loss 0.049305\n",
      "iteration 1900 / 2000: loss 0.105178\n",
      "iteration 0 / 2000: loss 0.980881\n",
      "iteration 100 / 2000: loss 0.588789\n",
      "iteration 200 / 2000: loss 0.498668\n",
      "iteration 300 / 2000: loss 0.433882\n",
      "iteration 400 / 2000: loss 0.344639\n",
      "iteration 500 / 2000: loss 0.354438\n",
      "iteration 600 / 2000: loss 0.244286\n",
      "iteration 700 / 2000: loss 0.193371\n",
      "iteration 800 / 2000: loss 0.167039\n",
      "iteration 900 / 2000: loss 0.136123\n",
      "iteration 1000 / 2000: loss 0.108284\n",
      "iteration 1100 / 2000: loss 0.135097\n",
      "iteration 1200 / 2000: loss 0.085219\n",
      "iteration 1300 / 2000: loss 0.101414\n",
      "iteration 1400 / 2000: loss 0.111039\n",
      "iteration 1500 / 2000: loss 0.057570\n",
      "iteration 1600 / 2000: loss 0.087167\n",
      "iteration 1700 / 2000: loss 0.114042\n",
      "iteration 1800 / 2000: loss 0.035690\n",
      "iteration 1900 / 2000: loss 0.023437\n",
      "iteration 0 / 2000: loss 0.971382\n",
      "iteration 100 / 2000: loss 2.665893\n",
      "iteration 200 / 2000: loss 2.616773\n",
      "iteration 300 / 2000: loss 2.567347\n",
      "iteration 400 / 2000: loss 2.519001\n",
      "iteration 500 / 2000: loss 2.470683\n",
      "iteration 600 / 2000: loss 2.431626\n",
      "iteration 700 / 2000: loss 2.380789\n",
      "iteration 800 / 2000: loss 2.334321\n",
      "iteration 900 / 2000: loss 2.298331\n",
      "iteration 1000 / 2000: loss 2.247287\n",
      "iteration 1100 / 2000: loss 2.205741\n",
      "iteration 1200 / 2000: loss 2.163456\n",
      "iteration 1300 / 2000: loss 2.122515\n",
      "iteration 1400 / 2000: loss 2.085743\n",
      "iteration 1500 / 2000: loss 2.043563\n",
      "iteration 1600 / 2000: loss 2.005131\n",
      "iteration 1700 / 2000: loss 1.972644\n",
      "iteration 1800 / 2000: loss 1.932152\n",
      "iteration 1900 / 2000: loss 1.894830\n",
      "iteration 0 / 2000: loss 0.995160\n",
      "iteration 100 / 2000: loss 0.049433\n",
      "iteration 200 / 2000: loss 0.045885\n",
      "iteration 300 / 2000: loss 0.058217\n",
      "iteration 400 / 2000: loss 0.044087\n",
      "iteration 500 / 2000: loss 0.043059\n",
      "iteration 600 / 2000: loss 0.041814\n",
      "iteration 700 / 2000: loss 0.043631\n",
      "iteration 800 / 2000: loss 0.040015\n",
      "iteration 900 / 2000: loss 0.041462\n",
      "iteration 1000 / 2000: loss 0.040476\n",
      "iteration 1100 / 2000: loss 0.042619\n",
      "iteration 1200 / 2000: loss 0.037465\n",
      "iteration 1300 / 2000: loss 0.039235\n",
      "iteration 1400 / 2000: loss 0.036215\n",
      "iteration 1500 / 2000: loss 0.038816\n",
      "iteration 1600 / 2000: loss 0.038414\n",
      "iteration 1700 / 2000: loss 0.042502\n",
      "iteration 1800 / 2000: loss 0.045602\n",
      "iteration 1900 / 2000: loss 0.032438\n",
      "iteration 0 / 2000: loss 0.984130\n",
      "iteration 100 / 2000: loss 0.046092\n",
      "iteration 200 / 2000: loss 0.044958\n",
      "iteration 300 / 2000: loss 0.042530\n",
      "iteration 400 / 2000: loss 0.040891\n",
      "iteration 500 / 2000: loss 0.044701\n",
      "iteration 600 / 2000: loss 0.039808\n",
      "iteration 700 / 2000: loss 0.040734\n",
      "iteration 800 / 2000: loss 0.051766\n",
      "iteration 900 / 2000: loss 0.058261\n",
      "iteration 1000 / 2000: loss 0.046027\n",
      "iteration 1100 / 2000: loss 0.041216\n",
      "iteration 1200 / 2000: loss 0.043756\n",
      "iteration 1300 / 2000: loss 0.062437\n",
      "iteration 1400 / 2000: loss 0.037432\n",
      "iteration 1500 / 2000: loss 0.036029\n",
      "iteration 1600 / 2000: loss 0.042116\n",
      "iteration 1700 / 2000: loss 0.032820\n",
      "iteration 1800 / 2000: loss 0.034156\n",
      "iteration 1900 / 2000: loss 0.032491\n",
      "iteration 0 / 2000: loss 0.925763\n",
      "iteration 100 / 2000: loss 0.113170\n",
      "iteration 200 / 2000: loss 0.103812\n",
      "iteration 300 / 2000: loss 0.110971\n",
      "iteration 400 / 2000: loss 0.119170\n",
      "iteration 500 / 2000: loss 0.133012\n",
      "iteration 600 / 2000: loss 0.118740\n",
      "iteration 700 / 2000: loss 0.094520\n",
      "iteration 800 / 2000: loss 0.102428\n",
      "iteration 900 / 2000: loss 0.095312\n",
      "iteration 1000 / 2000: loss 0.194505\n",
      "iteration 1100 / 2000: loss 0.091420\n",
      "iteration 1200 / 2000: loss 0.103442\n",
      "iteration 1300 / 2000: loss 0.087665\n",
      "iteration 1400 / 2000: loss 0.086309\n",
      "iteration 1500 / 2000: loss 0.084857\n",
      "iteration 1600 / 2000: loss 0.090175\n",
      "iteration 1700 / 2000: loss 0.091349\n",
      "iteration 1800 / 2000: loss 0.083713\n",
      "iteration 1900 / 2000: loss 0.142163\n",
      "iteration 0 / 2000: loss 0.980854\n",
      "iteration 100 / 2000: loss 0.042270\n",
      "iteration 200 / 2000: loss 0.051665\n",
      "iteration 300 / 2000: loss 0.071727\n",
      "iteration 400 / 2000: loss 0.051308\n",
      "iteration 500 / 2000: loss 0.117011\n",
      "iteration 600 / 2000: loss 0.060876\n",
      "iteration 700 / 2000: loss 0.042393\n",
      "iteration 800 / 2000: loss 0.049554\n",
      "iteration 900 / 2000: loss 0.042402\n",
      "iteration 1000 / 2000: loss 0.036970\n",
      "iteration 1100 / 2000: loss 0.082126\n",
      "iteration 1200 / 2000: loss 0.046364\n",
      "iteration 1300 / 2000: loss 0.074496\n",
      "iteration 1400 / 2000: loss 0.093909\n",
      "iteration 1500 / 2000: loss 0.048437\n",
      "iteration 1600 / 2000: loss 0.084613\n",
      "iteration 1700 / 2000: loss 0.116894\n",
      "iteration 1800 / 2000: loss 0.040012\n",
      "iteration 1900 / 2000: loss 0.033744\n",
      "iteration 0 / 2000: loss 0.971420\n",
      "iteration 100 / 2000: loss 211802633.047834\n",
      "iteration 200 / 2000: loss 192603230.993176\n",
      "iteration 300 / 2000: loss 175144212.369630\n",
      "iteration 400 / 2000: loss 159267811.698947\n",
      "iteration 500 / 2000: loss 144830568.478242\n",
      "iteration 600 / 2000: loss 131702026.563371\n",
      "iteration 700 / 2000: loss 119763555.315102\n",
      "iteration 800 / 2000: loss 108907277.711160\n",
      "iteration 900 / 2000: loss 99035095.512723\n",
      "iteration 1000 / 2000: loss 90057802.818783\n",
      "iteration 1100 / 2000: loss 81894280.075990\n",
      "iteration 1200 / 2000: loss 74470760.990409\n",
      "iteration 1300 / 2000: loss 67720166.016232\n",
      "iteration 1400 / 2000: loss 61581496.207716\n",
      "iteration 1500 / 2000: loss 55999282.014369\n",
      "iteration 1600 / 2000: loss 50923082.084610\n",
      "iteration 1700 / 2000: loss 46307027.447662\n",
      "iteration 1800 / 2000: loss 42109407.028436\n",
      "iteration 1900 / 2000: loss 38292290.784118\n",
      "iteration 0 / 2000: loss 0.995143\n",
      "iteration 100 / 2000: loss 0.149915\n",
      "iteration 200 / 2000: loss 0.128894\n",
      "iteration 300 / 2000: loss 0.130339\n",
      "iteration 400 / 2000: loss 0.106640\n",
      "iteration 500 / 2000: loss 0.096793\n",
      "iteration 600 / 2000: loss 0.087753\n",
      "iteration 700 / 2000: loss 0.082236\n",
      "iteration 800 / 2000: loss 0.072417\n",
      "iteration 900 / 2000: loss 0.068150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 2000: loss 0.061878\n",
      "iteration 1100 / 2000: loss 0.059610\n",
      "iteration 1200 / 2000: loss 0.049978\n",
      "iteration 1300 / 2000: loss 0.048087\n",
      "iteration 1400 / 2000: loss 0.041612\n",
      "iteration 1500 / 2000: loss 0.041450\n",
      "iteration 1600 / 2000: loss 0.034091\n",
      "iteration 1700 / 2000: loss 0.039688\n",
      "iteration 1800 / 2000: loss 0.041243\n",
      "iteration 1900 / 2000: loss 0.025614\n",
      "iteration 0 / 2000: loss 0.984146\n",
      "iteration 100 / 2000: loss 0.137702\n",
      "iteration 200 / 2000: loss 0.126252\n",
      "iteration 300 / 2000: loss 0.112817\n",
      "iteration 400 / 2000: loss 0.101923\n",
      "iteration 500 / 2000: loss 0.096180\n",
      "iteration 600 / 2000: loss 0.086190\n",
      "iteration 700 / 2000: loss 0.083801\n",
      "iteration 800 / 2000: loss 0.088785\n",
      "iteration 900 / 2000: loss 0.088325\n",
      "iteration 1000 / 2000: loss 0.071851\n",
      "iteration 1100 / 2000: loss 0.061449\n",
      "iteration 1200 / 2000: loss 0.061636\n",
      "iteration 1300 / 2000: loss 0.074225\n",
      "iteration 1400 / 2000: loss 0.045769\n",
      "iteration 1500 / 2000: loss 0.041152\n",
      "iteration 1600 / 2000: loss 0.044362\n",
      "iteration 1700 / 2000: loss 0.032496\n",
      "iteration 1800 / 2000: loss 0.031016\n",
      "iteration 1900 / 2000: loss 0.027835\n",
      "iteration 0 / 2000: loss 0.925777\n",
      "iteration 100 / 2000: loss 0.230728\n",
      "iteration 200 / 2000: loss 0.203185\n",
      "iteration 300 / 2000: loss 0.192726\n",
      "iteration 400 / 2000: loss 0.187394\n",
      "iteration 500 / 2000: loss 0.162678\n",
      "iteration 600 / 2000: loss 0.151604\n",
      "iteration 700 / 2000: loss 0.127290\n",
      "iteration 800 / 2000: loss 0.124903\n",
      "iteration 900 / 2000: loss 0.109304\n",
      "iteration 1000 / 2000: loss 0.197156\n",
      "iteration 1100 / 2000: loss 0.091122\n",
      "iteration 1200 / 2000: loss 0.096995\n",
      "iteration 1300 / 2000: loss 0.076444\n",
      "iteration 1400 / 2000: loss 0.069362\n",
      "iteration 1500 / 2000: loss 0.064450\n",
      "iteration 1600 / 2000: loss 0.064988\n",
      "iteration 1700 / 2000: loss 0.062751\n",
      "iteration 1800 / 2000: loss 0.052116\n",
      "iteration 1900 / 2000: loss 0.056051\n",
      "iteration 0 / 2000: loss 0.980869\n",
      "iteration 100 / 2000: loss 0.147406\n",
      "iteration 200 / 2000: loss 0.142873\n",
      "iteration 300 / 2000: loss 0.154506\n",
      "iteration 400 / 2000: loss 0.122442\n",
      "iteration 500 / 2000: loss 0.178788\n",
      "iteration 600 / 2000: loss 0.111663\n",
      "iteration 700 / 2000: loss 0.088403\n",
      "iteration 800 / 2000: loss 0.088460\n",
      "iteration 900 / 2000: loss 0.075140\n",
      "iteration 1000 / 2000: loss 0.064433\n",
      "iteration 1100 / 2000: loss 0.104005\n",
      "iteration 1200 / 2000: loss 0.064201\n",
      "iteration 1300 / 2000: loss 0.088303\n",
      "iteration 1400 / 2000: loss 0.104076\n",
      "iteration 1500 / 2000: loss 0.055340\n",
      "iteration 1600 / 2000: loss 0.088592\n",
      "iteration 1700 / 2000: loss 0.118256\n",
      "iteration 1800 / 2000: loss 0.035032\n",
      "iteration 1900 / 2000: loss 0.030677\n",
      "iteration 0 / 2000: loss 0.971418\n",
      "iteration 100 / 2000: loss 71885816806699275457255183435660735894850587590656.000000\n",
      "iteration 200 / 2000: loss 31355958896463727503236580722649086607836749758464.000000\n",
      "iteration 300 / 2000: loss 12099219000626971200454588230778333676894163042304.000000\n",
      "iteration 400 / 2000: loss 4668685174276155531797150283287318710826473357312.000000\n",
      "iteration 500 / 2000: loss 1801489935455875807324788802242252443683724984320.000000\n",
      "iteration 600 / 2000: loss 695134896957789116724783217265497543818281484288.000000\n",
      "iteration 700 / 2000: loss 268229378059909586837129781556383597475429089280.000000\n",
      "iteration 800 / 2000: loss 103500773115084691986764681507195629307032502272.000000\n",
      "iteration 900 / 2000: loss 39937497200726474891085629410920780658966003712.000000\n",
      "iteration 1000 / 2000: loss 15410548488217725603012379238401763810192916480.000000\n",
      "iteration 1100 / 2000: loss 5946416810099700257131152106487883396406050816.000000\n",
      "iteration 1200 / 2000: loss 2294523968856203002077540814828813222923993088.000000\n",
      "iteration 1300 / 2000: loss 885380290650588938093404892057263342394277888.000000\n",
      "iteration 1400 / 2000: loss 341638731916706684254450562637861652334116864.000000\n",
      "iteration 1500 / 2000: loss 131826994996568211963559724325333377220608000.000000\n",
      "iteration 1600 / 2000: loss 50867641711251139443092867838701051976876032.000000\n",
      "iteration 1700 / 2000: loss 19628126798548187271243380055384570969718784.000000\n",
      "iteration 1800 / 2000: loss 7573839648529860520140725946958136953798656.000000\n",
      "iteration 1900 / 2000: loss 2922492177189615834974316119634927859793920.000000\n",
      "iteration 0 / 2000: loss 0.995186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ceng783/neural_net_for_regression.py:99: RuntimeWarning: overflow encountered in multiply\n",
      "  loss += reg * 0.5 * np.sum(W1 * W1)\n",
      "ceng783/neural_net_for_regression.py:100: RuntimeWarning: overflow encountered in multiply\n",
      "  loss += reg * 0.5 * np.sum(W2 * W2)\n",
      "ceng783/neural_net_for_regression.py:98: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.sum(np.square(scores - y)) / (2 * number_of_training)\n",
      "ceng783/neural_net_for_regression.py:200: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.params['W1'] -= learning_rate * grads['W1']\n",
      "ceng783/neural_net_for_regression.py:78: RuntimeWarning: invalid value encountered in maximum\n",
      "  relu_1_o = np.maximum(X.dot(W1) + b1,0)\n",
      "ceng783/neural_net_for_regression.py:116: RuntimeWarning: invalid value encountered in greater\n",
      "  tempw1 = np.where((relu_1_o > 0), los_score_derivative.dot(W2.T), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ceng783/neural_net_for_regression.py:252: RuntimeWarning: invalid value encountered in maximum\n",
      "  relu_layer_o = np.maximum(X.dot(W1) + b1, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 0.984135\n",
      "iteration 100 / 2000: loss 419439.582233\n",
      "iteration 200 / 2000: loss 161847.748870\n",
      "iteration 300 / 2000: loss 62451.650446\n",
      "iteration 400 / 2000: loss 24098.009458\n",
      "iteration 500 / 2000: loss 9298.619545\n",
      "iteration 600 / 2000: loss 3588.027430\n",
      "iteration 700 / 2000: loss 1384.501819\n",
      "iteration 800 / 2000: loss 534.232550\n",
      "iteration 900 / 2000: loss 206.162185\n",
      "iteration 1000 / 2000: loss 79.543966\n",
      "iteration 1100 / 2000: loss 30.693884\n",
      "iteration 1200 / 2000: loss 11.847419\n",
      "iteration 1300 / 2000: loss 4.592309\n",
      "iteration 1400 / 2000: loss 1.763737\n",
      "iteration 1500 / 2000: loss 0.683299\n",
      "iteration 1600 / 2000: loss 0.263656\n",
      "iteration 1700 / 2000: loss 0.101607\n",
      "iteration 1800 / 2000: loss 0.040866\n",
      "iteration 1900 / 2000: loss 0.015780\n",
      "iteration 0 / 2000: loss 0.925777\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.980875\n",
      "iteration 100 / 2000: loss 136466286970785124550691582185046016.000000\n",
      "iteration 200 / 2000: loss 52657789790895124282676714938564608.000000\n",
      "iteration 300 / 2000: loss 20318885251532572207660453102354432.000000\n",
      "iteration 400 / 2000: loss 7840380302789202121881699208921088.000000\n",
      "iteration 500 / 2000: loss 3025341328099107050877488436609024.000000\n",
      "iteration 600 / 2000: loss 1167378341105266679201163164778496.000000\n",
      "iteration 700 / 2000: loss 450452376604376843311083172134912.000000\n",
      "iteration 800 / 2000: loss 173814552184015971084164602527744.000000\n",
      "iteration 900 / 2000: loss 67069239990855140073617060003840.000000\n",
      "iteration 1000 / 2000: loss 25879783346267937942110285594624.000000\n",
      "iteration 1100 / 2000: loss 9986145454176748685135976595456.000000\n",
      "iteration 1200 / 2000: loss 3853320551323542183170661154816.000000\n",
      "iteration 1300 / 2000: loss 1486867915091513561634163392512.000000\n",
      "iteration 1400 / 2000: loss 573732750100227581678765211648.000000\n",
      "iteration 1500 / 2000: loss 221384337637893621996545114112.000000\n",
      "iteration 1600 / 2000: loss 85424834023867252376380375040.000000\n",
      "iteration 1700 / 2000: loss 32962595032089550904090427392.000000\n",
      "iteration 1800 / 2000: loss 12719166313462953528067620864.000000\n",
      "iteration 1900 / 2000: loss 4907902170688878838613540864.000000\n",
      "iteration 0 / 2000: loss 0.971399\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995144\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984137\n",
      "iteration 100 / 2000: loss 279850698396074868736.000000\n",
      "iteration 200 / 2000: loss 254482907159138598912.000000\n",
      "iteration 300 / 2000: loss 231414645049444171776.000000\n",
      "iteration 400 / 2000: loss 210437465294561148928.000000\n",
      "iteration 500 / 2000: loss 191361816319524208640.000000\n",
      "iteration 600 / 2000: loss 174015328942729822208.000000\n",
      "iteration 700 / 2000: loss 158241258833395089408.000000\n",
      "iteration 800 / 2000: loss 143897070156494979072.000000\n",
      "iteration 900 / 2000: loss 130853147606870409216.000000\n",
      "iteration 1000 / 2000: loss 118991625194340810752.000000\n",
      "iteration 1100 / 2000: loss 108205321196622626816.000000\n",
      "iteration 1200 / 2000: loss 98396769656198594560.000000\n",
      "iteration 1300 / 2000: loss 89477339669661351936.000000\n",
      "iteration 1400 / 2000: loss 81366434511354789888.000000\n",
      "iteration 1500 / 2000: loss 73990763354527547392.000000\n",
      "iteration 1600 / 2000: loss 67283679009207427072.000000\n",
      "iteration 1700 / 2000: loss 61184575692542099456.000000\n",
      "iteration 1800 / 2000: loss 55638341389806125056.000000\n",
      "iteration 1900 / 2000: loss 50594859857562812416.000000\n",
      "iteration 0 / 2000: loss 0.925763\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.980879\n",
      "iteration 100 / 2000: loss 832.506709\n",
      "iteration 200 / 2000: loss 757.052354\n",
      "iteration 300 / 2000: loss 688.431152\n",
      "iteration 400 / 2000: loss 626.021657\n",
      "iteration 500 / 2000: loss 569.353763\n",
      "iteration 600 / 2000: loss 517.676259\n",
      "iteration 700 / 2000: loss 470.749999\n",
      "iteration 800 / 2000: loss 428.078845\n",
      "iteration 900 / 2000: loss 389.270825\n",
      "iteration 1000 / 2000: loss 353.980652\n",
      "iteration 1100 / 2000: loss 321.920596\n",
      "iteration 1200 / 2000: loss 292.727256\n",
      "iteration 1300 / 2000: loss 266.200427\n",
      "iteration 1400 / 2000: loss 242.059292\n",
      "iteration 1500 / 2000: loss 220.111111\n",
      "iteration 1600 / 2000: loss 200.188768\n",
      "iteration 1700 / 2000: loss 182.062322\n",
      "iteration 1800 / 2000: loss 165.520346\n",
      "iteration 1900 / 2000: loss 150.516163\n",
      "iteration 0 / 2000: loss 0.971407\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995156\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984148\n",
      "iteration 100 / 2000: loss 68.363006\n",
      "iteration 200 / 2000: loss 42.489740\n",
      "iteration 300 / 2000: loss 26.409610\n",
      "iteration 400 / 2000: loss 16.414206\n",
      "iteration 500 / 2000: loss 10.203139\n",
      "iteration 600 / 2000: loss 6.341484\n",
      "iteration 700 / 2000: loss 3.946731\n",
      "iteration 800 / 2000: loss 2.449688\n",
      "iteration 900 / 2000: loss 1.542171\n",
      "iteration 1000 / 2000: loss 0.946718\n",
      "iteration 1100 / 2000: loss 0.588824\n",
      "iteration 1200 / 2000: loss 0.369481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 2000: loss 0.249501\n",
      "iteration 1400 / 2000: loss 0.141536\n",
      "iteration 1500 / 2000: loss 0.090624\n",
      "iteration 1600 / 2000: loss 0.055647\n",
      "iteration 1700 / 2000: loss 0.034198\n",
      "iteration 1800 / 2000: loss 0.022846\n",
      "iteration 1900 / 2000: loss 0.013793\n",
      "iteration 0 / 2000: loss 0.925773\n",
      "iteration 100 / 2000: loss 738.288907\n",
      "iteration 200 / 2000: loss 458.869806\n",
      "iteration 300 / 2000: loss 285.203482\n",
      "iteration 400 / 2000: loss 177.265974\n",
      "iteration 500 / 2000: loss 110.175424\n",
      "iteration 600 / 2000: loss 68.481672\n",
      "iteration 700 / 2000: loss 42.561371\n",
      "iteration 800 / 2000: loss 26.456803\n",
      "iteration 900 / 2000: loss 16.444264\n",
      "iteration 1000 / 2000: loss 10.309962\n",
      "iteration 1100 / 2000: loss 6.371850\n",
      "iteration 1200 / 2000: loss 3.954761\n",
      "iteration 1300 / 2000: loss 2.456861\n",
      "iteration 1400 / 2000: loss 1.531247\n",
      "iteration 1500 / 2000: loss 0.951953\n",
      "iteration 1600 / 2000: loss 0.600919\n",
      "iteration 1700 / 2000: loss 0.369411\n",
      "iteration 1800 / 2000: loss 0.232943\n",
      "iteration 1900 / 2000: loss 0.148792\n",
      "iteration 0 / 2000: loss 0.980866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kursat/anaconda3/envs/ceng783/lib/python2.7/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.971415\n",
      "iteration 100 / 2000: loss 0.056795\n",
      "iteration 200 / 2000: loss 0.033096\n",
      "iteration 300 / 2000: loss 0.006051\n",
      "iteration 400 / 2000: loss 0.020944\n",
      "iteration 500 / 2000: loss 0.010995\n",
      "iteration 600 / 2000: loss 0.019226\n",
      "iteration 700 / 2000: loss 0.003379\n",
      "iteration 800 / 2000: loss 0.004359\n",
      "iteration 900 / 2000: loss 0.005570\n",
      "iteration 1000 / 2000: loss 0.002313\n",
      "iteration 1100 / 2000: loss 0.002679\n",
      "iteration 1200 / 2000: loss 0.001811\n",
      "iteration 1300 / 2000: loss 0.004010\n",
      "iteration 1400 / 2000: loss 0.002148\n",
      "iteration 1500 / 2000: loss 0.001050\n",
      "iteration 1600 / 2000: loss 0.001086\n",
      "iteration 1700 / 2000: loss 0.004017\n",
      "iteration 1800 / 2000: loss 0.002487\n",
      "iteration 1900 / 2000: loss 0.001141\n",
      "iteration 0 / 2000: loss 0.995176\n",
      "iteration 100 / 2000: loss 0.071690\n",
      "iteration 200 / 2000: loss 0.004455\n",
      "iteration 300 / 2000: loss 0.009096\n",
      "iteration 400 / 2000: loss 0.001791\n",
      "iteration 500 / 2000: loss 0.001359\n",
      "iteration 600 / 2000: loss 0.001612\n",
      "iteration 700 / 2000: loss 0.002935\n",
      "iteration 800 / 2000: loss 0.001222\n",
      "iteration 900 / 2000: loss 0.002601\n",
      "iteration 1000 / 2000: loss 0.002103\n",
      "iteration 1100 / 2000: loss 0.009888\n",
      "iteration 1200 / 2000: loss 0.001966\n",
      "iteration 1300 / 2000: loss 0.011177\n",
      "iteration 1400 / 2000: loss 0.002242\n",
      "iteration 1500 / 2000: loss 0.019441\n",
      "iteration 1600 / 2000: loss 0.000765\n",
      "iteration 1700 / 2000: loss 0.010532\n",
      "iteration 1800 / 2000: loss 0.001201\n",
      "iteration 1900 / 2000: loss 0.000619\n",
      "iteration 0 / 2000: loss 0.984133\n",
      "iteration 100 / 2000: loss 0.002289\n",
      "iteration 200 / 2000: loss 0.006889\n",
      "iteration 300 / 2000: loss 0.001694\n",
      "iteration 400 / 2000: loss 0.001205\n",
      "iteration 500 / 2000: loss 0.006843\n",
      "iteration 600 / 2000: loss 0.001944\n",
      "iteration 700 / 2000: loss 0.006192\n",
      "iteration 800 / 2000: loss 0.001261\n",
      "iteration 900 / 2000: loss 0.027384\n",
      "iteration 1000 / 2000: loss 0.001087\n",
      "iteration 1100 / 2000: loss 0.002395\n",
      "iteration 1200 / 2000: loss 0.008163\n",
      "iteration 1300 / 2000: loss 0.271598\n",
      "iteration 1400 / 2000: loss 0.000959\n",
      "iteration 1500 / 2000: loss 0.059920\n",
      "iteration 1600 / 2000: loss 0.002277\n",
      "iteration 1700 / 2000: loss 0.000974\n",
      "iteration 1800 / 2000: loss 0.002491\n",
      "iteration 1900 / 2000: loss 0.001082\n",
      "iteration 0 / 2000: loss 0.925791\n",
      "iteration 100 / 2000: loss 0.013689\n",
      "iteration 200 / 2000: loss 0.001421\n",
      "iteration 300 / 2000: loss 0.002134\n",
      "iteration 400 / 2000: loss 0.000681\n",
      "iteration 500 / 2000: loss 0.001191\n",
      "iteration 600 / 2000: loss 0.008356\n",
      "iteration 700 / 2000: loss 0.000493\n",
      "iteration 800 / 2000: loss 0.001438\n",
      "iteration 900 / 2000: loss 0.003426\n",
      "iteration 1000 / 2000: loss 0.076991\n",
      "iteration 1100 / 2000: loss 0.029757\n",
      "iteration 1200 / 2000: loss 0.008972\n",
      "iteration 1300 / 2000: loss 0.003595\n",
      "iteration 1400 / 2000: loss 0.005362\n",
      "iteration 1500 / 2000: loss 0.006926\n",
      "iteration 1600 / 2000: loss 0.012736\n",
      "iteration 1700 / 2000: loss 0.003074\n",
      "iteration 1800 / 2000: loss 0.003880\n",
      "iteration 1900 / 2000: loss 0.015827\n",
      "iteration 0 / 2000: loss 0.980881\n",
      "iteration 100 / 2000: loss 0.001544\n",
      "iteration 200 / 2000: loss 0.008694\n",
      "iteration 300 / 2000: loss 0.019272\n",
      "iteration 400 / 2000: loss 0.007268\n",
      "iteration 500 / 2000: loss 0.038285\n",
      "iteration 600 / 2000: loss 0.005379\n",
      "iteration 700 / 2000: loss 0.011953\n",
      "iteration 800 / 2000: loss 0.011500\n",
      "iteration 900 / 2000: loss 0.004868\n",
      "iteration 1000 / 2000: loss 0.002216\n",
      "iteration 1100 / 2000: loss 0.043521\n",
      "iteration 1200 / 2000: loss 0.017143\n",
      "iteration 1300 / 2000: loss 0.022511\n",
      "iteration 1400 / 2000: loss 0.003995\n",
      "iteration 1500 / 2000: loss 0.003202\n",
      "iteration 1600 / 2000: loss 0.028948\n",
      "iteration 1700 / 2000: loss 0.042852\n",
      "iteration 1800 / 2000: loss 0.005309\n",
      "iteration 1900 / 2000: loss 0.006991\n",
      "iteration 0 / 2000: loss 0.971408\n",
      "iteration 100 / 2000: loss 0.054817\n",
      "iteration 200 / 2000: loss 0.032026\n",
      "iteration 300 / 2000: loss 0.005667\n",
      "iteration 400 / 2000: loss 0.020570\n",
      "iteration 500 / 2000: loss 0.010421\n",
      "iteration 600 / 2000: loss 0.019111\n",
      "iteration 700 / 2000: loss 0.003122\n",
      "iteration 800 / 2000: loss 0.003604\n",
      "iteration 900 / 2000: loss 0.004674\n",
      "iteration 1000 / 2000: loss 0.001478\n",
      "iteration 1100 / 2000: loss 0.001936\n",
      "iteration 1200 / 2000: loss 0.001082\n",
      "iteration 1300 / 2000: loss 0.003216\n",
      "iteration 1400 / 2000: loss 0.001381\n",
      "iteration 1500 / 2000: loss 0.000358\n",
      "iteration 1600 / 2000: loss 0.000371\n",
      "iteration 1700 / 2000: loss 0.003337\n",
      "iteration 1800 / 2000: loss 0.001923\n",
      "iteration 1900 / 2000: loss 0.000519\n",
      "iteration 0 / 2000: loss 0.995144\n",
      "iteration 100 / 2000: loss 0.074344\n",
      "iteration 200 / 2000: loss 0.004147\n",
      "iteration 300 / 2000: loss 0.009194\n",
      "iteration 400 / 2000: loss 0.001837\n",
      "iteration 500 / 2000: loss 0.000428\n",
      "iteration 600 / 2000: loss 0.000805\n",
      "iteration 700 / 2000: loss 0.002068\n",
      "iteration 800 / 2000: loss 0.000415\n",
      "iteration 900 / 2000: loss 0.001949\n",
      "iteration 1000 / 2000: loss 0.000958\n",
      "iteration 1100 / 2000: loss 0.008830\n",
      "iteration 1200 / 2000: loss 0.000934\n",
      "iteration 1300 / 2000: loss 0.011269\n",
      "iteration 1400 / 2000: loss 0.001611\n",
      "iteration 1500 / 2000: loss 0.019577\n",
      "iteration 1600 / 2000: loss 0.000192\n",
      "iteration 1700 / 2000: loss 0.010318\n",
      "iteration 1800 / 2000: loss 0.000613\n",
      "iteration 1900 / 2000: loss 0.000092\n",
      "iteration 0 / 2000: loss 0.984114\n",
      "iteration 100 / 2000: loss 0.002018\n",
      "iteration 200 / 2000: loss 0.006704\n",
      "iteration 300 / 2000: loss 0.000845\n",
      "iteration 400 / 2000: loss 0.000369\n",
      "iteration 500 / 2000: loss 0.001197\n",
      "iteration 600 / 2000: loss 0.000853\n",
      "iteration 700 / 2000: loss 0.005524\n",
      "iteration 800 / 2000: loss 0.000540\n",
      "iteration 900 / 2000: loss 0.025650\n",
      "iteration 1000 / 2000: loss 0.000384\n",
      "iteration 1100 / 2000: loss 0.001688\n",
      "iteration 1200 / 2000: loss 0.008558\n",
      "iteration 1300 / 2000: loss 0.294652\n",
      "iteration 1400 / 2000: loss 0.000444\n",
      "iteration 1500 / 2000: loss 0.072772\n",
      "iteration 1600 / 2000: loss 0.001604\n",
      "iteration 1700 / 2000: loss 0.000487\n",
      "iteration 1800 / 2000: loss 0.001999\n",
      "iteration 1900 / 2000: loss 0.000568\n",
      "iteration 0 / 2000: loss 0.925777\n",
      "iteration 100 / 2000: loss 0.013511\n",
      "iteration 200 / 2000: loss 0.000583\n",
      "iteration 300 / 2000: loss 0.001423\n",
      "iteration 400 / 2000: loss 0.000471\n",
      "iteration 500 / 2000: loss 0.000958\n",
      "iteration 600 / 2000: loss 0.008098\n",
      "iteration 700 / 2000: loss 0.000354\n",
      "iteration 800 / 2000: loss 0.001210\n",
      "iteration 900 / 2000: loss 0.003252\n",
      "iteration 1000 / 2000: loss 0.077173\n",
      "iteration 1100 / 2000: loss 0.029671\n",
      "iteration 1200 / 2000: loss 0.008755\n",
      "iteration 1300 / 2000: loss 0.003413\n",
      "iteration 1400 / 2000: loss 0.005169\n",
      "iteration 1500 / 2000: loss 0.006667\n",
      "iteration 1600 / 2000: loss 0.012516\n",
      "iteration 1700 / 2000: loss 0.002874\n",
      "iteration 1800 / 2000: loss 0.003684\n",
      "iteration 1900 / 2000: loss 0.015642\n",
      "iteration 0 / 2000: loss 0.980851\n",
      "iteration 100 / 2000: loss 0.000712\n",
      "iteration 200 / 2000: loss 0.008304\n",
      "iteration 300 / 2000: loss 0.019189\n",
      "iteration 400 / 2000: loss 0.007187\n",
      "iteration 500 / 2000: loss 0.038620\n",
      "iteration 600 / 2000: loss 0.005219\n",
      "iteration 700 / 2000: loss 0.011856\n",
      "iteration 800 / 2000: loss 0.011264\n",
      "iteration 900 / 2000: loss 0.004704\n",
      "iteration 1000 / 2000: loss 0.002034\n",
      "iteration 1100 / 2000: loss 0.042717\n",
      "iteration 1200 / 2000: loss 0.016852\n",
      "iteration 1300 / 2000: loss 0.022338\n",
      "iteration 1400 / 2000: loss 0.003973\n",
      "iteration 1500 / 2000: loss 0.003175\n",
      "iteration 1600 / 2000: loss 0.029211\n",
      "iteration 1700 / 2000: loss 0.042867\n",
      "iteration 1800 / 2000: loss 0.005074\n",
      "iteration 1900 / 2000: loss 0.006886\n",
      "iteration 0 / 2000: loss 0.971411\n",
      "iteration 100 / 2000: loss 0.056637\n",
      "iteration 200 / 2000: loss 0.032934\n",
      "iteration 300 / 2000: loss 0.005891\n",
      "iteration 400 / 2000: loss 0.020746\n",
      "iteration 500 / 2000: loss 0.010635\n",
      "iteration 600 / 2000: loss 0.019188\n",
      "iteration 700 / 2000: loss 0.003211\n",
      "iteration 800 / 2000: loss 0.003999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 2000: loss 0.005063\n",
      "iteration 1000 / 2000: loss 0.001864\n",
      "iteration 1100 / 2000: loss 0.002268\n",
      "iteration 1200 / 2000: loss 0.001408\n",
      "iteration 1300 / 2000: loss 0.003573\n",
      "iteration 1400 / 2000: loss 0.001723\n",
      "iteration 1500 / 2000: loss 0.000669\n",
      "iteration 1600 / 2000: loss 0.000693\n",
      "iteration 1700 / 2000: loss 0.003640\n",
      "iteration 1800 / 2000: loss 0.002178\n",
      "iteration 1900 / 2000: loss 0.000802\n",
      "iteration 0 / 2000: loss 0.995154\n",
      "iteration 100 / 2000: loss 0.075362\n",
      "iteration 200 / 2000: loss 0.004289\n",
      "iteration 300 / 2000: loss 0.009283\n",
      "iteration 400 / 2000: loss 0.001946\n",
      "iteration 500 / 2000: loss 0.000845\n",
      "iteration 600 / 2000: loss 0.001164\n",
      "iteration 700 / 2000: loss 0.002451\n",
      "iteration 800 / 2000: loss 0.000777\n",
      "iteration 900 / 2000: loss 0.002247\n",
      "iteration 1000 / 2000: loss 0.001473\n",
      "iteration 1100 / 2000: loss 0.009258\n",
      "iteration 1200 / 2000: loss 0.001416\n",
      "iteration 1300 / 2000: loss 0.011222\n",
      "iteration 1400 / 2000: loss 0.001843\n",
      "iteration 1500 / 2000: loss 0.019136\n",
      "iteration 1600 / 2000: loss 0.000472\n",
      "iteration 1700 / 2000: loss 0.010432\n",
      "iteration 1800 / 2000: loss 0.000896\n",
      "iteration 1900 / 2000: loss 0.000362\n",
      "iteration 0 / 2000: loss 0.984137\n",
      "iteration 100 / 2000: loss 0.002102\n",
      "iteration 200 / 2000: loss 0.006835\n",
      "iteration 300 / 2000: loss 0.001221\n",
      "iteration 400 / 2000: loss 0.000741\n",
      "iteration 500 / 2000: loss 0.002580\n",
      "iteration 600 / 2000: loss 0.001352\n",
      "iteration 700 / 2000: loss 0.005814\n",
      "iteration 800 / 2000: loss 0.000877\n",
      "iteration 900 / 2000: loss 0.026403\n",
      "iteration 1000 / 2000: loss 0.000717\n",
      "iteration 1100 / 2000: loss 0.002033\n",
      "iteration 1200 / 2000: loss 0.008350\n",
      "iteration 1300 / 2000: loss 0.284159\n",
      "iteration 1400 / 2000: loss 0.000704\n",
      "iteration 1500 / 2000: loss 0.067134\n",
      "iteration 1600 / 2000: loss 0.002122\n",
      "iteration 1700 / 2000: loss 0.000738\n",
      "iteration 1800 / 2000: loss 0.002257\n",
      "iteration 1900 / 2000: loss 0.000837\n",
      "iteration 0 / 2000: loss 0.925792\n",
      "iteration 100 / 2000: loss 0.013620\n",
      "iteration 200 / 2000: loss 0.000956\n",
      "iteration 300 / 2000: loss 0.001740\n",
      "iteration 400 / 2000: loss 0.000565\n",
      "iteration 500 / 2000: loss 0.001068\n",
      "iteration 600 / 2000: loss 0.008203\n",
      "iteration 700 / 2000: loss 0.000415\n",
      "iteration 800 / 2000: loss 0.001312\n",
      "iteration 900 / 2000: loss 0.003333\n",
      "iteration 1000 / 2000: loss 0.077097\n",
      "iteration 1100 / 2000: loss 0.029714\n",
      "iteration 1200 / 2000: loss 0.008848\n",
      "iteration 1300 / 2000: loss 0.003492\n",
      "iteration 1400 / 2000: loss 0.005257\n",
      "iteration 1500 / 2000: loss 0.006788\n",
      "iteration 1600 / 2000: loss 0.012613\n",
      "iteration 1700 / 2000: loss 0.002967\n",
      "iteration 1800 / 2000: loss 0.003769\n",
      "iteration 1900 / 2000: loss 0.015727\n",
      "iteration 0 / 2000: loss 0.980865\n",
      "iteration 100 / 2000: loss 0.001082\n",
      "iteration 200 / 2000: loss 0.008463\n",
      "iteration 300 / 2000: loss 0.019204\n",
      "iteration 400 / 2000: loss 0.007295\n",
      "iteration 500 / 2000: loss 0.038886\n",
      "iteration 600 / 2000: loss 0.005336\n",
      "iteration 700 / 2000: loss 0.011943\n",
      "iteration 800 / 2000: loss 0.011343\n",
      "iteration 900 / 2000: loss 0.004798\n",
      "iteration 1000 / 2000: loss 0.002108\n",
      "iteration 1100 / 2000: loss 0.042465\n",
      "iteration 1200 / 2000: loss 0.016949\n",
      "iteration 1300 / 2000: loss 0.022414\n",
      "iteration 1400 / 2000: loss 0.004058\n",
      "iteration 1500 / 2000: loss 0.003246\n",
      "iteration 1600 / 2000: loss 0.029447\n",
      "iteration 1700 / 2000: loss 0.043013\n",
      "iteration 1800 / 2000: loss 0.005133\n",
      "iteration 1900 / 2000: loss 0.006973\n",
      "iteration 0 / 2000: loss 0.971421\n",
      "iteration 100 / 2000: loss 0.025691\n",
      "iteration 200 / 2000: loss 0.003096\n",
      "iteration 300 / 2000: loss 0.002835\n",
      "iteration 400 / 2000: loss 0.002648\n",
      "iteration 500 / 2000: loss 0.001076\n",
      "iteration 600 / 2000: loss 0.002984\n",
      "iteration 700 / 2000: loss 0.000658\n",
      "iteration 800 / 2000: loss 0.000672\n",
      "iteration 900 / 2000: loss 0.012048\n",
      "iteration 1000 / 2000: loss 0.001514\n",
      "iteration 1100 / 2000: loss 0.001958\n",
      "iteration 1200 / 2000: loss 0.000916\n",
      "iteration 1300 / 2000: loss 0.000854\n",
      "iteration 1400 / 2000: loss 0.003915\n",
      "iteration 1500 / 2000: loss 0.001236\n",
      "iteration 1600 / 2000: loss 0.000672\n",
      "iteration 1700 / 2000: loss 0.008035\n",
      "iteration 1800 / 2000: loss 0.002188\n",
      "iteration 1900 / 2000: loss 0.008302\n",
      "iteration 0 / 2000: loss 0.995160\n",
      "iteration 100 / 2000: loss 0.027593\n",
      "iteration 200 / 2000: loss 0.002767\n",
      "iteration 300 / 2000: loss 0.005835\n",
      "iteration 400 / 2000: loss 0.000438\n",
      "iteration 500 / 2000: loss 0.001168\n",
      "iteration 600 / 2000: loss 0.000902\n",
      "iteration 700 / 2000: loss 0.003677\n",
      "iteration 800 / 2000: loss 0.000628\n",
      "iteration 900 / 2000: loss 0.002489\n",
      "iteration 1000 / 2000: loss 0.007446\n",
      "iteration 1100 / 2000: loss 0.005873\n",
      "iteration 1200 / 2000: loss 0.001065\n",
      "iteration 1300 / 2000: loss 0.017680\n",
      "iteration 1400 / 2000: loss 0.004231\n",
      "iteration 1500 / 2000: loss 0.005012\n",
      "iteration 1600 / 2000: loss 0.012467\n",
      "iteration 1700 / 2000: loss 0.007443\n",
      "iteration 1800 / 2000: loss 0.017045\n",
      "iteration 1900 / 2000: loss 0.000428\n",
      "iteration 0 / 2000: loss 0.984152\n",
      "iteration 100 / 2000: loss 0.025312\n",
      "iteration 200 / 2000: loss 0.002917\n",
      "iteration 300 / 2000: loss 0.003308\n",
      "iteration 400 / 2000: loss 0.002541\n",
      "iteration 500 / 2000: loss 0.017431\n",
      "iteration 600 / 2000: loss 0.004634\n",
      "iteration 700 / 2000: loss 0.014230\n",
      "iteration 800 / 2000: loss 0.012262\n",
      "iteration 900 / 2000: loss 0.022876\n",
      "iteration 1000 / 2000: loss 0.001990\n",
      "iteration 1100 / 2000: loss 0.007626\n",
      "iteration 1200 / 2000: loss 0.004945\n",
      "iteration 1300 / 2000: loss 0.032158\n",
      "iteration 1400 / 2000: loss 0.021121\n",
      "iteration 1500 / 2000: loss 0.002296\n",
      "iteration 1600 / 2000: loss 0.009005\n",
      "iteration 1700 / 2000: loss 0.001776\n",
      "iteration 1800 / 2000: loss 0.003252\n",
      "iteration 1900 / 2000: loss 0.002916\n",
      "iteration 0 / 2000: loss 0.925781\n",
      "iteration 100 / 2000: loss 0.005553\n",
      "iteration 200 / 2000: loss 0.003164\n",
      "iteration 300 / 2000: loss 0.004217\n",
      "iteration 400 / 2000: loss 0.003044\n",
      "iteration 500 / 2000: loss 0.003162\n",
      "iteration 600 / 2000: loss 0.044484\n",
      "iteration 700 / 2000: loss 0.056435\n",
      "iteration 800 / 2000: loss 0.013602\n",
      "iteration 900 / 2000: loss 0.005063\n",
      "iteration 1000 / 2000: loss 0.107586\n",
      "iteration 1100 / 2000: loss 0.022354\n",
      "iteration 1200 / 2000: loss 0.029832\n",
      "iteration 1300 / 2000: loss 0.005515\n",
      "iteration 1400 / 2000: loss 0.012836\n",
      "iteration 1500 / 2000: loss 0.005152\n",
      "iteration 1600 / 2000: loss 0.012394\n",
      "iteration 1700 / 2000: loss 0.035637\n",
      "iteration 1800 / 2000: loss 0.008664\n",
      "iteration 1900 / 2000: loss 0.009736\n",
      "iteration 0 / 2000: loss 0.980882\n",
      "iteration 100 / 2000: loss 0.000979\n",
      "iteration 200 / 2000: loss 0.000604\n",
      "iteration 300 / 2000: loss 0.030615\n",
      "iteration 400 / 2000: loss 0.017199\n",
      "iteration 500 / 2000: loss 0.099872\n",
      "iteration 600 / 2000: loss 0.042418\n",
      "iteration 700 / 2000: loss 0.005821\n",
      "iteration 800 / 2000: loss 0.015838\n",
      "iteration 900 / 2000: loss 0.006926\n",
      "iteration 1000 / 2000: loss 0.002088\n",
      "iteration 1100 / 2000: loss 0.026361\n",
      "iteration 1200 / 2000: loss 0.013251\n",
      "iteration 1300 / 2000: loss 0.031426\n",
      "iteration 1400 / 2000: loss 0.059959\n",
      "iteration 1500 / 2000: loss 0.014231\n",
      "iteration 1600 / 2000: loss 0.041126\n",
      "iteration 1700 / 2000: loss 0.088724\n",
      "iteration 1800 / 2000: loss 0.017122\n",
      "iteration 1900 / 2000: loss 0.002963\n",
      "iteration 0 / 2000: loss 0.971403\n",
      "iteration 100 / 2000: loss 0.023578\n",
      "iteration 200 / 2000: loss 0.001058\n",
      "iteration 300 / 2000: loss 0.002418\n",
      "iteration 400 / 2000: loss 0.001846\n",
      "iteration 500 / 2000: loss 0.000715\n",
      "iteration 600 / 2000: loss 0.003133\n",
      "iteration 700 / 2000: loss 0.000366\n",
      "iteration 800 / 2000: loss 0.000480\n",
      "iteration 900 / 2000: loss 0.011498\n",
      "iteration 1000 / 2000: loss 0.001377\n",
      "iteration 1100 / 2000: loss 0.001855\n",
      "iteration 1200 / 2000: loss 0.000805\n",
      "iteration 1300 / 2000: loss 0.000774\n",
      "iteration 1400 / 2000: loss 0.003901\n",
      "iteration 1500 / 2000: loss 0.001182\n",
      "iteration 1600 / 2000: loss 0.000696\n",
      "iteration 1700 / 2000: loss 0.007787\n",
      "iteration 1800 / 2000: loss 0.002244\n",
      "iteration 1900 / 2000: loss 0.006049\n",
      "iteration 0 / 2000: loss 0.995120\n",
      "iteration 100 / 2000: loss 0.059101\n",
      "iteration 200 / 2000: loss 0.002542\n",
      "iteration 300 / 2000: loss 0.005530\n",
      "iteration 400 / 2000: loss 0.000274\n",
      "iteration 500 / 2000: loss 0.000805\n",
      "iteration 600 / 2000: loss 0.000704\n",
      "iteration 700 / 2000: loss 0.003412\n",
      "iteration 800 / 2000: loss 0.000432\n",
      "iteration 900 / 2000: loss 0.002431\n",
      "iteration 1000 / 2000: loss 0.003294\n",
      "iteration 1100 / 2000: loss 0.006235\n",
      "iteration 1200 / 2000: loss 0.000861\n",
      "iteration 1300 / 2000: loss 0.020121\n",
      "iteration 1400 / 2000: loss 0.003632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1500 / 2000: loss 0.003106\n",
      "iteration 1600 / 2000: loss 0.002806\n",
      "iteration 1700 / 2000: loss 0.007515\n",
      "iteration 1800 / 2000: loss 0.018957\n",
      "iteration 1900 / 2000: loss 0.000383\n",
      "iteration 0 / 2000: loss 0.984129\n",
      "iteration 100 / 2000: loss 0.009941\n",
      "iteration 200 / 2000: loss 0.000472\n",
      "iteration 300 / 2000: loss 0.000986\n",
      "iteration 400 / 2000: loss 0.000396\n",
      "iteration 500 / 2000: loss 0.015920\n",
      "iteration 600 / 2000: loss 0.003225\n",
      "iteration 700 / 2000: loss 0.012285\n",
      "iteration 800 / 2000: loss 0.011405\n",
      "iteration 900 / 2000: loss 0.025007\n",
      "iteration 1000 / 2000: loss 0.000559\n",
      "iteration 1100 / 2000: loss 0.006859\n",
      "iteration 1200 / 2000: loss 0.007916\n",
      "iteration 1300 / 2000: loss 0.030867\n",
      "iteration 1400 / 2000: loss 0.000299\n",
      "iteration 1500 / 2000: loss 0.001679\n",
      "iteration 1600 / 2000: loss 0.008280\n",
      "iteration 1700 / 2000: loss 0.001037\n",
      "iteration 1800 / 2000: loss 0.002876\n",
      "iteration 1900 / 2000: loss 0.005504\n",
      "iteration 0 / 2000: loss 0.925765\n",
      "iteration 100 / 2000: loss 0.002934\n",
      "iteration 200 / 2000: loss 0.000572\n",
      "iteration 300 / 2000: loss 0.001786\n",
      "iteration 400 / 2000: loss 0.005395\n",
      "iteration 500 / 2000: loss 0.000783\n",
      "iteration 600 / 2000: loss 0.033829\n",
      "iteration 700 / 2000: loss 0.000912\n",
      "iteration 800 / 2000: loss 0.009747\n",
      "iteration 900 / 2000: loss 0.003917\n",
      "iteration 1000 / 2000: loss 0.105074\n",
      "iteration 1100 / 2000: loss 0.003634\n",
      "iteration 1200 / 2000: loss 0.006652\n",
      "iteration 1300 / 2000: loss 0.003967\n",
      "iteration 1400 / 2000: loss 0.005513\n",
      "iteration 1500 / 2000: loss 0.006906\n",
      "iteration 1600 / 2000: loss 0.012474\n",
      "iteration 1700 / 2000: loss 0.003350\n",
      "iteration 1800 / 2000: loss 0.006525\n",
      "iteration 1900 / 2000: loss 0.008726\n",
      "iteration 0 / 2000: loss 0.980850\n",
      "iteration 100 / 2000: loss 0.000840\n",
      "iteration 200 / 2000: loss 0.001374\n",
      "iteration 300 / 2000: loss 0.041160\n",
      "iteration 400 / 2000: loss 0.016145\n",
      "iteration 500 / 2000: loss 0.101724\n",
      "iteration 600 / 2000: loss 0.009810\n",
      "iteration 700 / 2000: loss 0.005105\n",
      "iteration 800 / 2000: loss 0.020760\n",
      "iteration 900 / 2000: loss 0.006503\n",
      "iteration 1000 / 2000: loss 0.002073\n",
      "iteration 1100 / 2000: loss 0.027715\n",
      "iteration 1200 / 2000: loss 0.013085\n",
      "iteration 1300 / 2000: loss 0.029045\n",
      "iteration 1400 / 2000: loss 0.071198\n",
      "iteration 1500 / 2000: loss 0.014490\n",
      "iteration 1600 / 2000: loss 0.041390\n",
      "iteration 1700 / 2000: loss 0.088216\n",
      "iteration 1800 / 2000: loss 0.016581\n",
      "iteration 1900 / 2000: loss 0.003982\n",
      "iteration 0 / 2000: loss 0.971401\n",
      "iteration 100 / 2000: loss 0.028948\n",
      "iteration 200 / 2000: loss 0.004588\n",
      "iteration 300 / 2000: loss 0.002505\n",
      "iteration 400 / 2000: loss 0.001482\n",
      "iteration 500 / 2000: loss 0.000989\n",
      "iteration 600 / 2000: loss 0.002227\n",
      "iteration 700 / 2000: loss 0.000170\n",
      "iteration 800 / 2000: loss 0.000692\n",
      "iteration 900 / 2000: loss 0.010792\n",
      "iteration 1000 / 2000: loss 0.001259\n",
      "iteration 1100 / 2000: loss 0.001676\n",
      "iteration 1200 / 2000: loss 0.000606\n",
      "iteration 1300 / 2000: loss 0.000538\n",
      "iteration 1400 / 2000: loss 0.003675\n",
      "iteration 1500 / 2000: loss 0.000991\n",
      "iteration 1600 / 2000: loss 0.000466\n",
      "iteration 1700 / 2000: loss 0.007695\n",
      "iteration 1800 / 2000: loss 0.002003\n",
      "iteration 1900 / 2000: loss 0.005918\n",
      "iteration 0 / 2000: loss 0.995140\n",
      "iteration 100 / 2000: loss 0.017414\n",
      "iteration 200 / 2000: loss 0.001996\n",
      "iteration 300 / 2000: loss 0.006133\n",
      "iteration 400 / 2000: loss 0.000221\n",
      "iteration 500 / 2000: loss 0.000953\n",
      "iteration 600 / 2000: loss 0.000703\n",
      "iteration 700 / 2000: loss 0.003350\n",
      "iteration 800 / 2000: loss 0.000487\n",
      "iteration 900 / 2000: loss 0.002344\n",
      "iteration 1000 / 2000: loss 0.010352\n",
      "iteration 1100 / 2000: loss 0.005401\n",
      "iteration 1200 / 2000: loss 0.001090\n",
      "iteration 1300 / 2000: loss 0.019024\n",
      "iteration 1400 / 2000: loss 0.003633\n",
      "iteration 1500 / 2000: loss 0.004965\n",
      "iteration 1600 / 2000: loss 0.014774\n",
      "iteration 1700 / 2000: loss 0.017385\n",
      "iteration 1800 / 2000: loss 0.001662\n",
      "iteration 1900 / 2000: loss 0.000684\n",
      "iteration 0 / 2000: loss 0.984146\n",
      "iteration 100 / 2000: loss 0.005527\n",
      "iteration 200 / 2000: loss 0.000477\n",
      "iteration 300 / 2000: loss 0.000998\n",
      "iteration 400 / 2000: loss 0.000413\n",
      "iteration 500 / 2000: loss 0.016438\n",
      "iteration 600 / 2000: loss 0.007526\n",
      "iteration 700 / 2000: loss 0.011157\n",
      "iteration 800 / 2000: loss 0.014210\n",
      "iteration 900 / 2000: loss 0.022598\n",
      "iteration 1000 / 2000: loss 0.000514\n",
      "iteration 1100 / 2000: loss 0.008621\n",
      "iteration 1200 / 2000: loss 0.003675\n",
      "iteration 1300 / 2000: loss 0.031278\n",
      "iteration 1400 / 2000: loss 0.000312\n",
      "iteration 1500 / 2000: loss 0.001748\n",
      "iteration 1600 / 2000: loss 0.006177\n",
      "iteration 1700 / 2000: loss 0.000507\n",
      "iteration 1800 / 2000: loss 0.001920\n",
      "iteration 1900 / 2000: loss 0.000719\n",
      "iteration 0 / 2000: loss 0.925767\n",
      "iteration 100 / 2000: loss 0.004206\n",
      "iteration 200 / 2000: loss 0.001813\n",
      "iteration 300 / 2000: loss 0.003048\n",
      "iteration 400 / 2000: loss 0.006221\n",
      "iteration 500 / 2000: loss 0.053128\n",
      "iteration 600 / 2000: loss 0.031103\n",
      "iteration 700 / 2000: loss 0.001390\n",
      "iteration 800 / 2000: loss 0.009360\n",
      "iteration 900 / 2000: loss 0.004781\n",
      "iteration 1000 / 2000: loss 0.092901\n",
      "iteration 1100 / 2000: loss 0.015856\n",
      "iteration 1200 / 2000: loss 0.006635\n",
      "iteration 1300 / 2000: loss 0.008110\n",
      "iteration 1400 / 2000: loss 0.010793\n",
      "iteration 1500 / 2000: loss 0.007732\n",
      "iteration 1600 / 2000: loss 0.013090\n",
      "iteration 1700 / 2000: loss 0.003632\n",
      "iteration 1800 / 2000: loss 0.006756\n",
      "iteration 1900 / 2000: loss 0.009939\n",
      "iteration 0 / 2000: loss 0.980866\n",
      "iteration 100 / 2000: loss 0.002184\n",
      "iteration 200 / 2000: loss 0.001809\n",
      "iteration 300 / 2000: loss 0.031950\n",
      "iteration 400 / 2000: loss 0.018428\n",
      "iteration 500 / 2000: loss 0.102787\n",
      "iteration 600 / 2000: loss 0.011870\n",
      "iteration 700 / 2000: loss 0.007011\n",
      "iteration 800 / 2000: loss 0.020611\n",
      "iteration 900 / 2000: loss 0.007844\n",
      "iteration 1000 / 2000: loss 0.002721\n",
      "iteration 1100 / 2000: loss 0.032524\n",
      "iteration 1200 / 2000: loss 0.014090\n",
      "iteration 1300 / 2000: loss 0.031926\n",
      "iteration 1400 / 2000: loss 0.073119\n",
      "iteration 1500 / 2000: loss 0.015159\n",
      "iteration 1600 / 2000: loss 0.043447\n",
      "iteration 1700 / 2000: loss 0.090997\n",
      "iteration 1800 / 2000: loss 0.020233\n",
      "iteration 1900 / 2000: loss 0.005840\n",
      "iteration 0 / 2000: loss 0.971447\n",
      "iteration 100 / 2000: loss 0.147099\n",
      "iteration 200 / 2000: loss 0.023846\n",
      "iteration 300 / 2000: loss 0.006315\n",
      "iteration 400 / 2000: loss 0.001396\n",
      "iteration 500 / 2000: loss 0.001705\n",
      "iteration 600 / 2000: loss 0.004060\n",
      "iteration 700 / 2000: loss 0.000059\n",
      "iteration 800 / 2000: loss 0.000046\n",
      "iteration 900 / 2000: loss 0.005698\n",
      "iteration 1000 / 2000: loss 0.003008\n",
      "iteration 1100 / 2000: loss 0.000779\n",
      "iteration 1200 / 2000: loss 0.002803\n",
      "iteration 1300 / 2000: loss 0.002433\n",
      "iteration 1400 / 2000: loss 0.000906\n",
      "iteration 1500 / 2000: loss 0.000126\n",
      "iteration 1600 / 2000: loss 0.000063\n",
      "iteration 1700 / 2000: loss 0.000376\n",
      "iteration 1800 / 2000: loss 0.001233\n",
      "iteration 1900 / 2000: loss 0.001306\n",
      "iteration 0 / 2000: loss 0.995165\n",
      "iteration 100 / 2000: loss 0.381740\n",
      "iteration 200 / 2000: loss 0.057481\n",
      "iteration 300 / 2000: loss 0.015303\n",
      "iteration 400 / 2000: loss 0.001408\n",
      "iteration 500 / 2000: loss 0.001722\n",
      "iteration 600 / 2000: loss 0.002111\n",
      "iteration 700 / 2000: loss 0.000467\n",
      "iteration 800 / 2000: loss 0.000389\n",
      "iteration 900 / 2000: loss 0.002261\n",
      "iteration 1000 / 2000: loss 0.000850\n",
      "iteration 1100 / 2000: loss 0.008995\n",
      "iteration 1200 / 2000: loss 0.000730\n",
      "iteration 1300 / 2000: loss 0.004139\n",
      "iteration 1400 / 2000: loss 0.003511\n",
      "iteration 1500 / 2000: loss 0.010312\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.007109\n",
      "iteration 1800 / 2000: loss 0.000088\n",
      "iteration 1900 / 2000: loss 0.000284\n",
      "iteration 0 / 2000: loss 0.984171\n",
      "iteration 100 / 2000: loss 0.084239\n",
      "iteration 200 / 2000: loss 0.012571\n",
      "iteration 300 / 2000: loss 0.003565\n",
      "iteration 400 / 2000: loss 0.000570\n",
      "iteration 500 / 2000: loss 0.001436\n",
      "iteration 600 / 2000: loss 0.001031\n",
      "iteration 700 / 2000: loss 0.003581\n",
      "iteration 800 / 2000: loss 0.000156\n",
      "iteration 900 / 2000: loss 0.028571\n",
      "iteration 1000 / 2000: loss 0.000526\n",
      "iteration 1100 / 2000: loss 0.001699\n",
      "iteration 1200 / 2000: loss 0.004196\n",
      "iteration 1300 / 2000: loss 0.013402\n",
      "iteration 1400 / 2000: loss 0.000267\n",
      "iteration 1500 / 2000: loss 0.003841\n",
      "iteration 1600 / 2000: loss 0.001486\n",
      "iteration 1700 / 2000: loss 0.000356\n",
      "iteration 1800 / 2000: loss 0.001884\n",
      "iteration 1900 / 2000: loss 0.001003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 0.925833\n",
      "iteration 100 / 2000: loss 0.007971\n",
      "iteration 200 / 2000: loss 0.001111\n",
      "iteration 300 / 2000: loss 0.000730\n",
      "iteration 400 / 2000: loss 0.000137\n",
      "iteration 500 / 2000: loss 0.000347\n",
      "iteration 600 / 2000: loss 0.004176\n",
      "iteration 700 / 2000: loss 0.000277\n",
      "iteration 800 / 2000: loss 0.002005\n",
      "iteration 900 / 2000: loss 0.002509\n",
      "iteration 1000 / 2000: loss 0.088594\n",
      "iteration 1100 / 2000: loss 0.031875\n",
      "iteration 1200 / 2000: loss 0.003317\n",
      "iteration 1300 / 2000: loss 0.003267\n",
      "iteration 1400 / 2000: loss 0.004006\n",
      "iteration 1500 / 2000: loss 0.007555\n",
      "iteration 1600 / 2000: loss 0.012375\n",
      "iteration 1700 / 2000: loss 0.002367\n",
      "iteration 1800 / 2000: loss 0.005162\n",
      "iteration 1900 / 2000: loss 0.008338\n",
      "iteration 0 / 2000: loss 0.980893\n",
      "iteration 100 / 2000: loss 0.006315\n",
      "iteration 200 / 2000: loss 0.001084\n",
      "iteration 300 / 2000: loss 0.009872\n",
      "iteration 400 / 2000: loss 0.003331\n",
      "iteration 500 / 2000: loss 0.057439\n",
      "iteration 600 / 2000: loss 0.005026\n",
      "iteration 700 / 2000: loss 0.008930\n",
      "iteration 800 / 2000: loss 0.010494\n",
      "iteration 900 / 2000: loss 0.005569\n",
      "iteration 1000 / 2000: loss 0.001793\n",
      "iteration 1100 / 2000: loss 0.032285\n",
      "iteration 1200 / 2000: loss 0.015956\n",
      "iteration 1300 / 2000: loss 0.021292\n",
      "iteration 1400 / 2000: loss 0.001854\n",
      "iteration 1500 / 2000: loss 0.000642\n",
      "iteration 1600 / 2000: loss 0.009215\n",
      "iteration 1700 / 2000: loss 0.039204\n",
      "iteration 1800 / 2000: loss 0.006022\n",
      "iteration 1900 / 2000: loss 0.002808\n",
      "iteration 0 / 2000: loss 0.971402\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995158\n",
      "iteration 100 / 2000: loss 0.085846\n",
      "iteration 200 / 2000: loss 0.071385\n",
      "iteration 300 / 2000: loss 0.065116\n",
      "iteration 400 / 2000: loss 0.048276\n",
      "iteration 500 / 2000: loss 0.041319\n",
      "iteration 600 / 2000: loss 0.034978\n",
      "iteration 700 / 2000: loss 0.027662\n",
      "iteration 800 / 2000: loss 0.022879\n",
      "iteration 900 / 2000: loss 0.020858\n",
      "iteration 1000 / 2000: loss 0.016227\n",
      "iteration 1100 / 2000: loss 0.021711\n",
      "iteration 1200 / 2000: loss 0.011244\n",
      "iteration 1300 / 2000: loss 0.012834\n",
      "iteration 1400 / 2000: loss 0.011602\n",
      "iteration 1500 / 2000: loss 0.017003\n",
      "iteration 1600 / 2000: loss 0.005804\n",
      "iteration 1700 / 2000: loss 0.011684\n",
      "iteration 1800 / 2000: loss 0.003870\n",
      "iteration 1900 / 2000: loss 0.003412\n",
      "iteration 0 / 2000: loss 0.984158\n",
      "iteration 100 / 2000: loss 0.054160\n",
      "iteration 200 / 2000: loss 0.044627\n",
      "iteration 300 / 2000: loss 0.038495\n",
      "iteration 400 / 2000: loss 0.030702\n",
      "iteration 500 / 2000: loss 0.026537\n",
      "iteration 600 / 2000: loss 0.021814\n",
      "iteration 700 / 2000: loss 0.020766\n",
      "iteration 800 / 2000: loss 0.014369\n",
      "iteration 900 / 2000: loss 0.040325\n",
      "iteration 1000 / 2000: loss 0.010244\n",
      "iteration 1100 / 2000: loss 0.012962\n",
      "iteration 1200 / 2000: loss 0.010883\n",
      "iteration 1300 / 2000: loss 0.018931\n",
      "iteration 1400 / 2000: loss 0.004839\n",
      "iteration 1500 / 2000: loss 0.007621\n",
      "iteration 1600 / 2000: loss 0.004612\n",
      "iteration 1700 / 2000: loss 0.002941\n",
      "iteration 1800 / 2000: loss 0.004021\n",
      "iteration 1900 / 2000: loss 0.002770\n",
      "iteration 0 / 2000: loss 0.925791\n",
      "iteration 100 / 2000: loss 0.006839\n",
      "iteration 200 / 2000: loss 0.003589\n",
      "iteration 300 / 2000: loss 0.003286\n",
      "iteration 400 / 2000: loss 0.002325\n",
      "iteration 500 / 2000: loss 0.002168\n",
      "iteration 600 / 2000: loss 0.005716\n",
      "iteration 700 / 2000: loss 0.001550\n",
      "iteration 800 / 2000: loss 0.003058\n",
      "iteration 900 / 2000: loss 0.003380\n",
      "iteration 1000 / 2000: loss 0.089314\n",
      "iteration 1100 / 2000: loss 0.032470\n",
      "iteration 1200 / 2000: loss 0.003809\n",
      "iteration 1300 / 2000: loss 0.003674\n",
      "iteration 1400 / 2000: loss 0.004343\n",
      "iteration 1500 / 2000: loss 0.007833\n",
      "iteration 1600 / 2000: loss 0.012605\n",
      "iteration 1700 / 2000: loss 0.002557\n",
      "iteration 1800 / 2000: loss 0.005320\n",
      "iteration 1900 / 2000: loss 0.008468\n",
      "iteration 0 / 2000: loss 0.980865\n",
      "iteration 100 / 2000: loss 0.003367\n",
      "iteration 200 / 2000: loss 0.002271\n",
      "iteration 300 / 2000: loss 0.011416\n",
      "iteration 400 / 2000: loss 0.004691\n",
      "iteration 500 / 2000: loss 0.058577\n",
      "iteration 600 / 2000: loss 0.005969\n",
      "iteration 700 / 2000: loss 0.009710\n",
      "iteration 800 / 2000: loss 0.011139\n",
      "iteration 900 / 2000: loss 0.006102\n",
      "iteration 1000 / 2000: loss 0.002234\n",
      "iteration 1100 / 2000: loss 0.032650\n",
      "iteration 1200 / 2000: loss 0.016258\n",
      "iteration 1300 / 2000: loss 0.021541\n",
      "iteration 1400 / 2000: loss 0.002060\n",
      "iteration 1500 / 2000: loss 0.000812\n",
      "iteration 1600 / 2000: loss 0.009356\n",
      "iteration 1700 / 2000: loss 0.039321\n",
      "iteration 1800 / 2000: loss 0.006118\n",
      "iteration 1900 / 2000: loss 0.002888\n",
      "iteration 0 / 2000: loss 0.971430\n",
      "iteration 100 / 2000: loss 0.126364\n",
      "iteration 200 / 2000: loss 0.048691\n",
      "iteration 300 / 2000: loss 0.021560\n",
      "iteration 400 / 2000: loss 0.008117\n",
      "iteration 500 / 2000: loss 0.004423\n",
      "iteration 600 / 2000: loss 0.005127\n",
      "iteration 700 / 2000: loss 0.000474\n",
      "iteration 800 / 2000: loss 0.000206\n",
      "iteration 900 / 2000: loss 0.005760\n",
      "iteration 1000 / 2000: loss 0.003032\n",
      "iteration 1100 / 2000: loss 0.000788\n",
      "iteration 1200 / 2000: loss 0.002807\n",
      "iteration 1300 / 2000: loss 0.002434\n",
      "iteration 1400 / 2000: loss 0.000907\n",
      "iteration 1500 / 2000: loss 0.000126\n",
      "iteration 1600 / 2000: loss 0.000063\n",
      "iteration 1700 / 2000: loss 0.000376\n",
      "iteration 1800 / 2000: loss 0.001233\n",
      "iteration 1900 / 2000: loss 0.001306\n",
      "iteration 0 / 2000: loss 0.995154\n",
      "iteration 100 / 2000: loss 0.088664\n",
      "iteration 200 / 2000: loss 0.034756\n",
      "iteration 300 / 2000: loss 0.019952\n",
      "iteration 400 / 2000: loss 0.005190\n",
      "iteration 500 / 2000: loss 0.003476\n",
      "iteration 600 / 2000: loss 0.002832\n",
      "iteration 700 / 2000: loss 0.000751\n",
      "iteration 800 / 2000: loss 0.000500\n",
      "iteration 900 / 2000: loss 0.002304\n",
      "iteration 1000 / 2000: loss 0.000866\n",
      "iteration 1100 / 2000: loss 0.009001\n",
      "iteration 1200 / 2000: loss 0.000732\n",
      "iteration 1300 / 2000: loss 0.004140\n",
      "iteration 1400 / 2000: loss 0.003511\n",
      "iteration 1500 / 2000: loss 0.010312\n",
      "iteration 1600 / 2000: loss 0.000272\n",
      "iteration 1700 / 2000: loss 0.007109\n",
      "iteration 1800 / 2000: loss 0.000088\n",
      "iteration 1900 / 2000: loss 0.000284\n",
      "iteration 0 / 2000: loss 0.984168\n",
      "iteration 100 / 2000: loss 0.013437\n",
      "iteration 200 / 2000: loss 0.005153\n",
      "iteration 300 / 2000: loss 0.003652\n",
      "iteration 400 / 2000: loss 0.001041\n",
      "iteration 500 / 2000: loss 0.001683\n",
      "iteration 600 / 2000: loss 0.001136\n",
      "iteration 700 / 2000: loss 0.003619\n",
      "iteration 800 / 2000: loss 0.000171\n",
      "iteration 900 / 2000: loss 0.028578\n",
      "iteration 1000 / 2000: loss 0.000528\n",
      "iteration 1100 / 2000: loss 0.001734\n",
      "iteration 1200 / 2000: loss 0.004209\n",
      "iteration 1300 / 2000: loss 0.013407\n",
      "iteration 1400 / 2000: loss 0.000269\n",
      "iteration 1500 / 2000: loss 0.003841\n",
      "iteration 1600 / 2000: loss 0.001487\n",
      "iteration 1700 / 2000: loss 0.000357\n",
      "iteration 1800 / 2000: loss 0.001884\n",
      "iteration 1900 / 2000: loss 0.001003\n",
      "iteration 0 / 2000: loss 0.925788\n",
      "iteration 100 / 2000: loss 0.007733\n",
      "iteration 200 / 2000: loss 0.002215\n",
      "iteration 300 / 2000: loss 0.001333\n",
      "iteration 400 / 2000: loss 0.000396\n",
      "iteration 500 / 2000: loss 0.000451\n",
      "iteration 600 / 2000: loss 0.004217\n",
      "iteration 700 / 2000: loss 0.000293\n",
      "iteration 800 / 2000: loss 0.002011\n",
      "iteration 900 / 2000: loss 0.002511\n",
      "iteration 1000 / 2000: loss 0.088595\n",
      "iteration 1100 / 2000: loss 0.031875\n",
      "iteration 1200 / 2000: loss 0.003317\n",
      "iteration 1300 / 2000: loss 0.003267\n",
      "iteration 1400 / 2000: loss 0.004006\n",
      "iteration 1500 / 2000: loss 0.007555\n",
      "iteration 1600 / 2000: loss 0.012375\n",
      "iteration 1700 / 2000: loss 0.002367\n",
      "iteration 1800 / 2000: loss 0.005162\n",
      "iteration 1900 / 2000: loss 0.008338\n",
      "iteration 0 / 2000: loss 0.980867\n",
      "iteration 100 / 2000: loss 0.007285\n",
      "iteration 200 / 2000: loss 0.002708\n",
      "iteration 300 / 2000: loss 0.010699\n",
      "iteration 400 / 2000: loss 0.003679\n",
      "iteration 500 / 2000: loss 0.057578\n",
      "iteration 600 / 2000: loss 0.005080\n",
      "iteration 700 / 2000: loss 0.008951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 2000: loss 0.010502\n",
      "iteration 900 / 2000: loss 0.005572\n",
      "iteration 1000 / 2000: loss 0.001795\n",
      "iteration 1100 / 2000: loss 0.032286\n",
      "iteration 1200 / 2000: loss 0.015956\n",
      "iteration 1300 / 2000: loss 0.021292\n",
      "iteration 1400 / 2000: loss 0.001854\n",
      "iteration 1500 / 2000: loss 0.000642\n",
      "iteration 1600 / 2000: loss 0.009215\n",
      "iteration 1700 / 2000: loss 0.039204\n",
      "iteration 1800 / 2000: loss 0.006022\n",
      "iteration 1900 / 2000: loss 0.002808\n",
      "iteration 0 / 2000: loss 0.971445\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995231\n",
      "iteration 100 / 2000: loss 10.564256\n",
      "iteration 200 / 2000: loss 8.734602\n",
      "iteration 300 / 2000: loss 7.235551\n",
      "iteration 400 / 2000: loss 5.972435\n",
      "iteration 500 / 2000: loss 4.938474\n",
      "iteration 600 / 2000: loss 4.083369\n",
      "iteration 700 / 2000: loss 3.378772\n",
      "iteration 800 / 2000: loss 2.791893\n",
      "iteration 900 / 2000: loss 2.310921\n",
      "iteration 1000 / 2000: loss 1.910910\n",
      "iteration 1100 / 2000: loss 1.583417\n",
      "iteration 1200 / 2000: loss 1.305751\n",
      "iteration 1300 / 2000: loss 1.082129\n",
      "iteration 1400 / 2000: loss 0.954131\n",
      "iteration 1500 / 2000: loss 0.792643\n",
      "iteration 1600 / 2000: loss 0.653741\n",
      "iteration 1700 / 2000: loss 0.548021\n",
      "iteration 1800 / 2000: loss 0.459048\n",
      "iteration 1900 / 2000: loss 0.368792\n",
      "iteration 0 / 2000: loss 0.984181\n",
      "iteration 100 / 2000: loss 3.467690\n",
      "iteration 200 / 2000: loss 2.869712\n",
      "iteration 300 / 2000: loss 2.371485\n",
      "iteration 400 / 2000: loss 1.960456\n",
      "iteration 500 / 2000: loss 1.623846\n",
      "iteration 600 / 2000: loss 1.342366\n",
      "iteration 700 / 2000: loss 1.118296\n",
      "iteration 800 / 2000: loss 0.924635\n",
      "iteration 900 / 2000: loss 0.784282\n",
      "iteration 1000 / 2000: loss 0.640843\n",
      "iteration 1100 / 2000: loss 0.527826\n",
      "iteration 1200 / 2000: loss 0.442895\n",
      "iteration 1300 / 2000: loss 0.384561\n",
      "iteration 1400 / 2000: loss 0.298886\n",
      "iteration 1500 / 2000: loss 0.246203\n",
      "iteration 1600 / 2000: loss 0.211325\n",
      "iteration 1700 / 2000: loss 0.167645\n",
      "iteration 1800 / 2000: loss 0.140661\n",
      "iteration 1900 / 2000: loss 0.115663\n",
      "iteration 0 / 2000: loss 0.925824\n",
      "iteration 100 / 2000: loss 3.433300\n",
      "iteration 200 / 2000: loss 2.833524\n",
      "iteration 300 / 2000: loss 2.352378\n",
      "iteration 400 / 2000: loss 1.956806\n",
      "iteration 500 / 2000: loss 1.650242\n",
      "iteration 600 / 2000: loss 1.344442\n",
      "iteration 700 / 2000: loss 1.119736\n",
      "iteration 800 / 2000: loss 0.909311\n",
      "iteration 900 / 2000: loss 0.751508\n",
      "iteration 1000 / 2000: loss 0.737624\n",
      "iteration 1100 / 2000: loss 0.516426\n",
      "iteration 1200 / 2000: loss 0.441430\n",
      "iteration 1300 / 2000: loss 0.354729\n",
      "iteration 1400 / 2000: loss 0.293533\n",
      "iteration 1500 / 2000: loss 0.243644\n",
      "iteration 1600 / 2000: loss 0.208907\n",
      "iteration 1700 / 2000: loss 0.177296\n",
      "iteration 1800 / 2000: loss 0.142781\n",
      "iteration 1900 / 2000: loss 0.127331\n",
      "iteration 0 / 2000: loss 0.980895\n",
      "iteration 100 / 2000: loss 1.463200\n",
      "iteration 200 / 2000: loss 1.218872\n",
      "iteration 300 / 2000: loss 1.031721\n",
      "iteration 400 / 2000: loss 0.839001\n",
      "iteration 500 / 2000: loss 0.763346\n",
      "iteration 600 / 2000: loss 0.582814\n",
      "iteration 700 / 2000: loss 0.472798\n",
      "iteration 800 / 2000: loss 0.396971\n",
      "iteration 900 / 2000: loss 0.326433\n",
      "iteration 1000 / 2000: loss 0.266474\n",
      "iteration 1100 / 2000: loss 0.252825\n",
      "iteration 1200 / 2000: loss 0.193375\n",
      "iteration 1300 / 2000: loss 0.190846\n",
      "iteration 1400 / 2000: loss 0.185180\n",
      "iteration 1500 / 2000: loss 0.118978\n",
      "iteration 1600 / 2000: loss 0.138005\n",
      "iteration 1700 / 2000: loss 0.156197\n",
      "iteration 1800 / 2000: loss 0.070282\n",
      "iteration 1900 / 2000: loss 0.052025\n",
      "iteration 0 / 2000: loss 0.971410\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995130\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984148\n",
      "iteration 100 / 2000: loss 0.189258\n",
      "iteration 200 / 2000: loss 0.186367\n",
      "iteration 300 / 2000: loss 0.180746\n",
      "iteration 400 / 2000: loss 0.176449\n",
      "iteration 500 / 2000: loss 0.180361\n",
      "iteration 600 / 2000: loss 0.171720\n",
      "iteration 700 / 2000: loss 0.168747\n",
      "iteration 800 / 2000: loss 0.177014\n",
      "iteration 900 / 2000: loss 0.181514\n",
      "iteration 1000 / 2000: loss 0.167388\n",
      "iteration 1100 / 2000: loss 0.159941\n",
      "iteration 1200 / 2000: loss 0.162159\n",
      "iteration 1300 / 2000: loss 0.177041\n",
      "iteration 1400 / 2000: loss 0.147039\n",
      "iteration 1500 / 2000: loss 0.146792\n",
      "iteration 1600 / 2000: loss 0.149742\n",
      "iteration 1700 / 2000: loss 0.139121\n",
      "iteration 1800 / 2000: loss 0.138474\n",
      "iteration 1900 / 2000: loss 0.134843\n",
      "iteration 0 / 2000: loss 0.925762\n",
      "iteration 100 / 2000: loss 0.274158\n",
      "iteration 200 / 2000: loss 0.261640\n",
      "iteration 300 / 2000: loss 0.265827\n",
      "iteration 400 / 2000: loss 0.271128\n",
      "iteration 500 / 2000: loss 0.307763\n",
      "iteration 600 / 2000: loss 0.263575\n",
      "iteration 700 / 2000: loss 0.268658\n",
      "iteration 800 / 2000: loss 0.241019\n",
      "iteration 900 / 2000: loss 0.233519\n",
      "iteration 1000 / 2000: loss 0.316884\n",
      "iteration 1100 / 2000: loss 0.224537\n",
      "iteration 1200 / 2000: loss 0.231621\n",
      "iteration 1300 / 2000: loss 0.216603\n",
      "iteration 1400 / 2000: loss 0.212048\n",
      "iteration 1500 / 2000: loss 0.209552\n",
      "iteration 1600 / 2000: loss 0.211568\n",
      "iteration 1700 / 2000: loss 0.198704\n",
      "iteration 1800 / 2000: loss 0.198296\n",
      "iteration 1900 / 2000: loss 0.204661\n",
      "iteration 0 / 2000: loss 0.980861\n",
      "iteration 100 / 2000: loss 0.293885\n",
      "iteration 200 / 2000: loss 0.301515\n",
      "iteration 300 / 2000: loss 0.314124\n",
      "iteration 400 / 2000: loss 0.289053\n",
      "iteration 500 / 2000: loss 0.350091\n",
      "iteration 600 / 2000: loss 0.289655\n",
      "iteration 700 / 2000: loss 0.266958\n",
      "iteration 800 / 2000: loss 0.267027\n",
      "iteration 900 / 2000: loss 0.258037\n",
      "iteration 1000 / 2000: loss 0.249125\n",
      "iteration 1100 / 2000: loss 0.288229\n",
      "iteration 1200 / 2000: loss 0.250501\n",
      "iteration 1300 / 2000: loss 0.274809\n",
      "iteration 1400 / 2000: loss 0.290347\n",
      "iteration 1500 / 2000: loss 0.241311\n",
      "iteration 1600 / 2000: loss 0.273857\n",
      "iteration 1700 / 2000: loss 0.301878\n",
      "iteration 1800 / 2000: loss 0.223260\n",
      "iteration 1900 / 2000: loss 0.212498\n",
      "iteration 0 / 2000: loss 0.971439\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995158\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984132\n",
      "iteration 100 / 2000: loss 4.861461\n",
      "iteration 200 / 2000: loss 4.420886\n",
      "iteration 300 / 2000: loss 4.020473\n",
      "iteration 400 / 2000: loss 3.655541\n",
      "iteration 500 / 2000: loss 3.331004\n",
      "iteration 600 / 2000: loss 3.023848\n",
      "iteration 700 / 2000: loss 2.756519\n",
      "iteration 800 / 2000: loss 2.518901\n",
      "iteration 900 / 2000: loss 2.296630\n",
      "iteration 1000 / 2000: loss 2.072589\n",
      "iteration 1100 / 2000: loss 1.888442\n",
      "iteration 1200 / 2000: loss 1.723516\n",
      "iteration 1300 / 2000: loss 1.585062\n",
      "iteration 1400 / 2000: loss 1.421193\n",
      "iteration 1500 / 2000: loss 1.290451\n",
      "iteration 1600 / 2000: loss 1.179532\n",
      "iteration 1700 / 2000: loss 1.065490\n",
      "iteration 1800 / 2000: loss 0.970851\n",
      "iteration 1900 / 2000: loss 0.881947\n",
      "iteration 0 / 2000: loss 0.925810\n",
      "iteration 100 / 2000: loss 2.825509\n",
      "iteration 200 / 2000: loss 2.564357\n",
      "iteration 300 / 2000: loss 2.341045\n",
      "iteration 400 / 2000: loss 2.140318\n",
      "iteration 500 / 2000: loss 1.963869\n",
      "iteration 600 / 2000: loss 1.777048\n",
      "iteration 700 / 2000: loss 1.624260\n",
      "iteration 800 / 2000: loss 1.456615\n",
      "iteration 900 / 2000: loss 1.323284\n",
      "iteration 1000 / 2000: loss 1.299594\n",
      "iteration 1100 / 2000: loss 1.095244\n",
      "iteration 1200 / 2000: loss 1.005478\n",
      "iteration 1300 / 2000: loss 0.906717\n",
      "iteration 1400 / 2000: loss 0.824372\n",
      "iteration 1500 / 2000: loss 0.750883\n",
      "iteration 1600 / 2000: loss 0.689320\n",
      "iteration 1700 / 2000: loss 0.618971\n",
      "iteration 1800 / 2000: loss 0.566339\n",
      "iteration 1900 / 2000: loss 0.525528\n",
      "iteration 0 / 2000: loss 0.980887\n",
      "iteration 100 / 2000: loss 1.010254\n",
      "iteration 200 / 2000: loss 0.932899\n",
      "iteration 300 / 2000: loss 0.866802\n",
      "iteration 400 / 2000: loss 0.770704\n",
      "iteration 500 / 2000: loss 0.791285\n",
      "iteration 600 / 2000: loss 0.646606\n",
      "iteration 700 / 2000: loss 0.576256\n",
      "iteration 800 / 2000: loss 0.532306\n",
      "iteration 900 / 2000: loss 0.478392\n",
      "iteration 1000 / 2000: loss 0.431263\n",
      "iteration 1100 / 2000: loss 0.429588\n",
      "iteration 1200 / 2000: loss 0.367559\n",
      "iteration 1300 / 2000: loss 0.364095\n",
      "iteration 1400 / 2000: loss 0.354985\n",
      "iteration 1500 / 2000: loss 0.283619\n",
      "iteration 1600 / 2000: loss 0.295295\n",
      "iteration 1700 / 2000: loss 0.307027\n",
      "iteration 1800 / 2000: loss 0.211664\n",
      "iteration 1900 / 2000: loss 0.187653\n",
      "iteration 0 / 2000: loss 0.971429\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995176\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984180\n",
      "iteration 100 / 2000: loss 394323702.373542\n",
      "iteration 200 / 2000: loss 152156368.360709\n",
      "iteration 300 / 2000: loss 58712069.027810\n",
      "iteration 400 / 2000: loss 22655029.734264\n",
      "iteration 500 / 2000: loss 8741820.563167\n",
      "iteration 600 / 2000: loss 3373177.067030\n",
      "iteration 700 / 2000: loss 1301596.556149\n",
      "iteration 800 / 2000: loss 502242.709862\n",
      "iteration 900 / 2000: loss 193798.734542\n",
      "iteration 1000 / 2000: loss 74780.462460\n",
      "iteration 1100 / 2000: loss 28855.287493\n",
      "iteration 1200 / 2000: loss 11134.296961\n",
      "iteration 1300 / 2000: loss 4296.374719\n",
      "iteration 1400 / 2000: loss 1657.819463\n",
      "iteration 1500 / 2000: loss 639.699984\n",
      "iteration 1600 / 2000: loss 246.838888\n",
      "iteration 1700 / 2000: loss 95.246766\n",
      "iteration 1800 / 2000: loss 36.754211\n",
      "iteration 1900 / 2000: loss 14.182236\n",
      "iteration 0 / 2000: loss 0.925818\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.980881\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.971391\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995163\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984147\n",
      "iteration 100 / 2000: loss 2494609.704053\n",
      "iteration 200 / 2000: loss 2268479.347247\n",
      "iteration 300 / 2000: loss 2062847.163290\n",
      "iteration 400 / 2000: loss 1875855.039862\n",
      "iteration 500 / 2000: loss 1705813.303269\n",
      "iteration 600 / 2000: loss 1551185.438649\n",
      "iteration 700 / 2000: loss 1410574.219025\n",
      "iteration 800 / 2000: loss 1282709.064665\n",
      "iteration 900 / 2000: loss 1166434.599433\n",
      "iteration 1000 / 2000: loss 1060700.097174\n",
      "iteration 1100 / 2000: loss 964550.190428\n",
      "iteration 1200 / 2000: loss 877116.043993\n",
      "iteration 1300 / 2000: loss 797607.607592\n",
      "iteration 1400 / 2000: loss 725306.380624\n",
      "iteration 1500 / 2000: loss 659559.108113\n",
      "iteration 1600 / 2000: loss 599771.663311\n",
      "iteration 1700 / 2000: loss 545403.807062\n",
      "iteration 1800 / 2000: loss 495964.268139\n",
      "iteration 1900 / 2000: loss 451006.301824\n",
      "iteration 0 / 2000: loss 0.925784\n",
      "iteration 100 / 2000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.980865\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.971419\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.995163\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.984170\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.925781\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.980890\n",
      "iteration 100 / 2000: loss nan\n",
      "iteration 200 / 2000: loss nan\n",
      "iteration 300 / 2000: loss nan\n",
      "iteration 400 / 2000: loss nan\n",
      "iteration 500 / 2000: loss nan\n",
      "iteration 600 / 2000: loss nan\n",
      "iteration 700 / 2000: loss nan\n",
      "iteration 800 / 2000: loss nan\n",
      "iteration 900 / 2000: loss nan\n",
      "iteration 1000 / 2000: loss nan\n",
      "iteration 1100 / 2000: loss nan\n",
      "iteration 1200 / 2000: loss nan\n",
      "iteration 1300 / 2000: loss nan\n",
      "iteration 1400 / 2000: loss nan\n",
      "iteration 1500 / 2000: loss nan\n",
      "iteration 1600 / 2000: loss nan\n",
      "iteration 1700 / 2000: loss nan\n",
      "iteration 1800 / 2000: loss nan\n",
      "iteration 1900 / 2000: loss nan\n",
      "iteration 0 / 2000: loss 0.971451\n",
      "iteration 100 / 2000: loss 0.060173\n",
      "iteration 200 / 2000: loss 0.035616\n",
      "iteration 300 / 2000: loss 0.006525\n",
      "iteration 400 / 2000: loss 0.021461\n",
      "iteration 500 / 2000: loss 0.011638\n",
      "iteration 600 / 2000: loss 0.019692\n",
      "iteration 700 / 2000: loss 0.003263\n",
      "iteration 800 / 2000: loss 0.004837\n",
      "iteration 900 / 2000: loss 0.005513\n",
      "iteration 1000 / 2000: loss 0.002353\n",
      "iteration 1100 / 2000: loss 0.002690\n",
      "iteration 1200 / 2000: loss 0.001812\n",
      "iteration 1300 / 2000: loss 0.004034\n",
      "iteration 1400 / 2000: loss 0.002155\n",
      "iteration 1500 / 2000: loss 0.001060\n",
      "iteration 1600 / 2000: loss 0.001097\n",
      "iteration 1700 / 2000: loss 0.004016\n",
      "iteration 1800 / 2000: loss 0.002504\n",
      "iteration 1900 / 2000: loss 0.001147\n",
      "iteration 0 / 2000: loss 0.995168\n",
      "iteration 100 / 2000: loss 0.081578\n",
      "iteration 200 / 2000: loss 0.004438\n",
      "iteration 300 / 2000: loss 0.010044\n",
      "iteration 400 / 2000: loss 0.002150\n",
      "iteration 500 / 2000: loss 0.001391\n",
      "iteration 600 / 2000: loss 0.001630\n",
      "iteration 700 / 2000: loss 0.002938\n",
      "iteration 800 / 2000: loss 0.001244\n",
      "iteration 900 / 2000: loss 0.002634\n",
      "iteration 1000 / 2000: loss 0.002170\n",
      "iteration 1100 / 2000: loss 0.010128\n",
      "iteration 1200 / 2000: loss 0.002016\n",
      "iteration 1300 / 2000: loss 0.012632\n",
      "iteration 1400 / 2000: loss 0.002482\n",
      "iteration 1500 / 2000: loss 0.021122\n",
      "iteration 1600 / 2000: loss 0.000801\n",
      "iteration 1700 / 2000: loss 0.010895\n",
      "iteration 1800 / 2000: loss 0.001255\n",
      "iteration 1900 / 2000: loss 0.000650\n",
      "iteration 0 / 2000: loss 0.984197\n",
      "iteration 100 / 2000: loss 0.002378\n",
      "iteration 200 / 2000: loss 0.006840\n",
      "iteration 300 / 2000: loss 0.001712\n",
      "iteration 400 / 2000: loss 0.001227\n",
      "iteration 500 / 2000: loss 0.011348\n",
      "iteration 600 / 2000: loss 0.001976\n",
      "iteration 700 / 2000: loss 0.006201\n",
      "iteration 800 / 2000: loss 0.001303\n",
      "iteration 900 / 2000: loss 0.027573\n",
      "iteration 1000 / 2000: loss 0.001121\n",
      "iteration 1100 / 2000: loss 0.002432\n",
      "iteration 1200 / 2000: loss 0.008021\n",
      "iteration 1300 / 2000: loss 0.266979\n",
      "iteration 1400 / 2000: loss 0.000987\n",
      "iteration 1500 / 2000: loss 0.051561\n",
      "iteration 1600 / 2000: loss 0.002113\n",
      "iteration 1700 / 2000: loss 0.001004\n",
      "iteration 1800 / 2000: loss 0.002514\n",
      "iteration 1900 / 2000: loss 0.001116\n",
      "iteration 0 / 2000: loss 0.925791\n",
      "iteration 100 / 2000: loss 0.014788\n",
      "iteration 200 / 2000: loss 0.001453\n",
      "iteration 300 / 2000: loss 0.002144\n",
      "iteration 400 / 2000: loss 0.000711\n",
      "iteration 500 / 2000: loss 0.001286\n",
      "iteration 600 / 2000: loss 0.008328\n",
      "iteration 700 / 2000: loss 0.000513\n",
      "iteration 800 / 2000: loss 0.001423\n",
      "iteration 900 / 2000: loss 0.003426\n",
      "iteration 1000 / 2000: loss 0.078054\n",
      "iteration 1100 / 2000: loss 0.029919\n",
      "iteration 1200 / 2000: loss 0.009063\n",
      "iteration 1300 / 2000: loss 0.003601\n",
      "iteration 1400 / 2000: loss 0.005337\n",
      "iteration 1500 / 2000: loss 0.006918\n",
      "iteration 1600 / 2000: loss 0.012740\n",
      "iteration 1700 / 2000: loss 0.003126\n",
      "iteration 1800 / 2000: loss 0.003876\n",
      "iteration 1900 / 2000: loss 0.016020\n",
      "iteration 0 / 2000: loss 0.980907\n",
      "iteration 100 / 2000: loss 0.001572\n",
      "iteration 200 / 2000: loss 0.008060\n",
      "iteration 300 / 2000: loss 0.020014\n",
      "iteration 400 / 2000: loss 0.007209\n",
      "iteration 500 / 2000: loss 0.037924\n",
      "iteration 600 / 2000: loss 0.005347\n",
      "iteration 700 / 2000: loss 0.011924\n",
      "iteration 800 / 2000: loss 0.011540\n",
      "iteration 900 / 2000: loss 0.004867\n",
      "iteration 1000 / 2000: loss 0.002237\n",
      "iteration 1100 / 2000: loss 0.044166\n",
      "iteration 1200 / 2000: loss 0.017187\n",
      "iteration 1300 / 2000: loss 0.022524\n",
      "iteration 1400 / 2000: loss 0.003934\n",
      "iteration 1500 / 2000: loss 0.003157\n",
      "iteration 1600 / 2000: loss 0.028598\n",
      "iteration 1700 / 2000: loss 0.042715\n",
      "iteration 1800 / 2000: loss 0.005365\n",
      "iteration 1900 / 2000: loss 0.006958\n",
      "iteration 0 / 2000: loss 0.971403\n",
      "iteration 100 / 2000: loss 0.061538\n",
      "iteration 200 / 2000: loss 0.036246\n",
      "iteration 300 / 2000: loss 0.006362\n",
      "iteration 400 / 2000: loss 0.021753\n",
      "iteration 500 / 2000: loss 0.011966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 2000: loss 0.019933\n",
      "iteration 700 / 2000: loss 0.002892\n",
      "iteration 800 / 2000: loss 0.004214\n",
      "iteration 900 / 2000: loss 0.004589\n",
      "iteration 1000 / 2000: loss 0.001545\n",
      "iteration 1100 / 2000: loss 0.001938\n",
      "iteration 1200 / 2000: loss 0.001060\n",
      "iteration 1300 / 2000: loss 0.003237\n",
      "iteration 1400 / 2000: loss 0.001367\n",
      "iteration 1500 / 2000: loss 0.000356\n",
      "iteration 1600 / 2000: loss 0.000368\n",
      "iteration 1700 / 2000: loss 0.003314\n",
      "iteration 1800 / 2000: loss 0.001936\n",
      "iteration 1900 / 2000: loss 0.000524\n",
      "iteration 0 / 2000: loss 0.995128\n",
      "iteration 100 / 2000: loss 0.081865\n",
      "iteration 200 / 2000: loss 0.004171\n",
      "iteration 300 / 2000: loss 0.009906\n",
      "iteration 400 / 2000: loss 0.002017\n",
      "iteration 500 / 2000: loss 0.000438\n",
      "iteration 600 / 2000: loss 0.000802\n",
      "iteration 700 / 2000: loss 0.002059\n",
      "iteration 800 / 2000: loss 0.000418\n",
      "iteration 900 / 2000: loss 0.001964\n",
      "iteration 1000 / 2000: loss 0.000970\n",
      "iteration 1100 / 2000: loss 0.008964\n",
      "iteration 1200 / 2000: loss 0.000931\n",
      "iteration 1300 / 2000: loss 0.012036\n",
      "iteration 1400 / 2000: loss 0.001775\n",
      "iteration 1500 / 2000: loss 0.020802\n",
      "iteration 1600 / 2000: loss 0.000196\n",
      "iteration 1700 / 2000: loss 0.010514\n",
      "iteration 1800 / 2000: loss 0.000630\n",
      "iteration 1900 / 2000: loss 0.000094\n",
      "iteration 0 / 2000: loss 0.984142\n",
      "iteration 100 / 2000: loss 0.002112\n",
      "iteration 200 / 2000: loss 0.006601\n",
      "iteration 300 / 2000: loss 0.000845\n",
      "iteration 400 / 2000: loss 0.000373\n",
      "iteration 500 / 2000: loss 0.000861\n",
      "iteration 600 / 2000: loss 0.000853\n",
      "iteration 700 / 2000: loss 0.005596\n",
      "iteration 800 / 2000: loss 0.000519\n",
      "iteration 900 / 2000: loss 0.025211\n",
      "iteration 1000 / 2000: loss 0.000405\n",
      "iteration 1100 / 2000: loss 0.001637\n",
      "iteration 1200 / 2000: loss 0.009250\n",
      "iteration 1300 / 2000: loss 0.308394\n",
      "iteration 1400 / 2000: loss 0.000445\n",
      "iteration 1500 / 2000: loss 0.052511\n",
      "iteration 1600 / 2000: loss 0.001472\n",
      "iteration 1700 / 2000: loss 0.000538\n",
      "iteration 1800 / 2000: loss 0.002006\n",
      "iteration 1900 / 2000: loss 0.000619\n",
      "iteration 0 / 2000: loss 0.925750\n",
      "iteration 100 / 2000: loss 0.014796\n",
      "iteration 200 / 2000: loss 0.000587\n",
      "iteration 300 / 2000: loss 0.001413\n",
      "iteration 400 / 2000: loss 0.000492\n",
      "iteration 500 / 2000: loss 0.001052\n",
      "iteration 600 / 2000: loss 0.008021\n",
      "iteration 700 / 2000: loss 0.000356\n",
      "iteration 800 / 2000: loss 0.001176\n",
      "iteration 900 / 2000: loss 0.003245\n",
      "iteration 1000 / 2000: loss 0.078459\n",
      "iteration 1100 / 2000: loss 0.029840\n",
      "iteration 1200 / 2000: loss 0.008844\n",
      "iteration 1300 / 2000: loss 0.003407\n",
      "iteration 1400 / 2000: loss 0.005129\n",
      "iteration 1500 / 2000: loss 0.006627\n",
      "iteration 1600 / 2000: loss 0.012515\n",
      "iteration 1700 / 2000: loss 0.002923\n",
      "iteration 1800 / 2000: loss 0.003669\n",
      "iteration 1900 / 2000: loss 0.015837\n",
      "iteration 0 / 2000: loss 0.980870\n",
      "iteration 100 / 2000: loss 0.000716\n",
      "iteration 200 / 2000: loss 0.007721\n",
      "iteration 300 / 2000: loss 0.019899\n",
      "iteration 400 / 2000: loss 0.006902\n",
      "iteration 500 / 2000: loss 0.037156\n",
      "iteration 600 / 2000: loss 0.005057\n",
      "iteration 700 / 2000: loss 0.011660\n",
      "iteration 800 / 2000: loss 0.011363\n",
      "iteration 900 / 2000: loss 0.004629\n",
      "iteration 1000 / 2000: loss 0.002066\n",
      "iteration 1100 / 2000: loss 0.045001\n",
      "iteration 1200 / 2000: loss 0.016979\n",
      "iteration 1300 / 2000: loss 0.022341\n",
      "iteration 1400 / 2000: loss 0.003700\n",
      "iteration 1500 / 2000: loss 0.002958\n",
      "iteration 1600 / 2000: loss 0.027907\n",
      "iteration 1700 / 2000: loss 0.042329\n",
      "iteration 1800 / 2000: loss 0.005246\n",
      "iteration 1900 / 2000: loss 0.006722\n",
      "iteration 0 / 2000: loss 0.971403\n",
      "iteration 100 / 2000: loss 0.061968\n",
      "iteration 200 / 2000: loss 0.036322\n",
      "iteration 300 / 2000: loss 0.006421\n",
      "iteration 400 / 2000: loss 0.021980\n",
      "iteration 500 / 2000: loss 0.012279\n",
      "iteration 600 / 2000: loss 0.019952\n",
      "iteration 700 / 2000: loss 0.003027\n",
      "iteration 800 / 2000: loss 0.004473\n",
      "iteration 900 / 2000: loss 0.005010\n",
      "iteration 1000 / 2000: loss 0.001912\n",
      "iteration 1100 / 2000: loss 0.002280\n",
      "iteration 1200 / 2000: loss 0.001396\n",
      "iteration 1300 / 2000: loss 0.003596\n",
      "iteration 1400 / 2000: loss 0.001719\n",
      "iteration 1500 / 2000: loss 0.000675\n",
      "iteration 1600 / 2000: loss 0.000699\n",
      "iteration 1700 / 2000: loss 0.003623\n",
      "iteration 1800 / 2000: loss 0.002191\n",
      "iteration 1900 / 2000: loss 0.000815\n",
      "iteration 0 / 2000: loss 0.995174\n",
      "iteration 100 / 2000: loss 0.081249\n",
      "iteration 200 / 2000: loss 0.004303\n",
      "iteration 300 / 2000: loss 0.009851\n",
      "iteration 400 / 2000: loss 0.002098\n",
      "iteration 500 / 2000: loss 0.000863\n",
      "iteration 600 / 2000: loss 0.001174\n",
      "iteration 700 / 2000: loss 0.002454\n",
      "iteration 800 / 2000: loss 0.000789\n",
      "iteration 900 / 2000: loss 0.002266\n",
      "iteration 1000 / 2000: loss 0.001511\n",
      "iteration 1100 / 2000: loss 0.009442\n",
      "iteration 1200 / 2000: loss 0.001428\n",
      "iteration 1300 / 2000: loss 0.011943\n",
      "iteration 1400 / 2000: loss 0.002042\n",
      "iteration 1500 / 2000: loss 0.020421\n",
      "iteration 1600 / 2000: loss 0.000491\n",
      "iteration 1700 / 2000: loss 0.010600\n",
      "iteration 1800 / 2000: loss 0.000938\n",
      "iteration 1900 / 2000: loss 0.000375\n",
      "iteration 0 / 2000: loss 0.984145\n",
      "iteration 100 / 2000: loss 0.002237\n",
      "iteration 200 / 2000: loss 0.006710\n",
      "iteration 300 / 2000: loss 0.001232\n",
      "iteration 400 / 2000: loss 0.000755\n",
      "iteration 500 / 2000: loss 0.001469\n",
      "iteration 600 / 2000: loss 0.001380\n",
      "iteration 700 / 2000: loss 0.005898\n",
      "iteration 800 / 2000: loss 0.000862\n",
      "iteration 900 / 2000: loss 0.025707\n",
      "iteration 1000 / 2000: loss 0.000762\n",
      "iteration 1100 / 2000: loss 0.001980\n",
      "iteration 1200 / 2000: loss 0.009349\n",
      "iteration 1300 / 2000: loss 0.304807\n",
      "iteration 1400 / 2000: loss 0.000724\n",
      "iteration 1500 / 2000: loss 0.073062\n",
      "iteration 1600 / 2000: loss 0.001705\n",
      "iteration 1700 / 2000: loss 0.000803\n",
      "iteration 1800 / 2000: loss 0.002289\n",
      "iteration 1900 / 2000: loss 0.000895\n",
      "iteration 0 / 2000: loss 0.925751\n",
      "iteration 100 / 2000: loss 0.014639\n",
      "iteration 200 / 2000: loss 0.000972\n",
      "iteration 300 / 2000: loss 0.001738\n",
      "iteration 400 / 2000: loss 0.000587\n",
      "iteration 500 / 2000: loss 0.001150\n",
      "iteration 600 / 2000: loss 0.008185\n",
      "iteration 700 / 2000: loss 0.000427\n",
      "iteration 800 / 2000: loss 0.001293\n",
      "iteration 900 / 2000: loss 0.003325\n",
      "iteration 1000 / 2000: loss 0.078159\n",
      "iteration 1100 / 2000: loss 0.029877\n",
      "iteration 1200 / 2000: loss 0.008946\n",
      "iteration 1300 / 2000: loss 0.003495\n",
      "iteration 1400 / 2000: loss 0.005225\n",
      "iteration 1500 / 2000: loss 0.006761\n",
      "iteration 1600 / 2000: loss 0.012618\n",
      "iteration 1700 / 2000: loss 0.003010\n",
      "iteration 1800 / 2000: loss 0.003768\n",
      "iteration 1900 / 2000: loss 0.015905\n",
      "iteration 0 / 2000: loss 0.980854\n",
      "iteration 100 / 2000: loss 0.001099\n",
      "iteration 200 / 2000: loss 0.007717\n",
      "iteration 300 / 2000: loss 0.020193\n",
      "iteration 400 / 2000: loss 0.007149\n",
      "iteration 500 / 2000: loss 0.038051\n",
      "iteration 600 / 2000: loss 0.005241\n",
      "iteration 700 / 2000: loss 0.011856\n",
      "iteration 800 / 2000: loss 0.011408\n",
      "iteration 900 / 2000: loss 0.004763\n",
      "iteration 1000 / 2000: loss 0.002134\n",
      "iteration 1100 / 2000: loss 0.043792\n",
      "iteration 1200 / 2000: loss 0.017026\n",
      "iteration 1300 / 2000: loss 0.022428\n",
      "iteration 1400 / 2000: loss 0.003905\n",
      "iteration 1500 / 2000: loss 0.003128\n",
      "iteration 1600 / 2000: loss 0.028688\n",
      "iteration 1700 / 2000: loss 0.042694\n",
      "iteration 1800 / 2000: loss 0.005236\n",
      "iteration 1900 / 2000: loss 0.006897\n",
      "iteration 0 / 2000: loss 0.971447\n",
      "iteration 100 / 2000: loss 0.037194\n",
      "iteration 200 / 2000: loss 0.001403\n",
      "iteration 300 / 2000: loss 0.002207\n",
      "iteration 400 / 2000: loss 0.001942\n",
      "iteration 500 / 2000: loss 0.000955\n",
      "iteration 600 / 2000: loss 0.002782\n",
      "iteration 700 / 2000: loss 0.000487\n",
      "iteration 800 / 2000: loss 0.000512\n",
      "iteration 900 / 2000: loss 0.010929\n",
      "iteration 1000 / 2000: loss 0.001432\n",
      "iteration 1100 / 2000: loss 0.001856\n",
      "iteration 1200 / 2000: loss 0.000806\n",
      "iteration 1300 / 2000: loss 0.000755\n",
      "iteration 1400 / 2000: loss 0.003865\n",
      "iteration 1500 / 2000: loss 0.001129\n",
      "iteration 1600 / 2000: loss 0.000599\n",
      "iteration 1700 / 2000: loss 0.007863\n",
      "iteration 1800 / 2000: loss 0.002134\n",
      "iteration 1900 / 2000: loss 0.006063\n",
      "iteration 0 / 2000: loss 0.995156\n",
      "iteration 100 / 2000: loss 0.033956\n",
      "iteration 200 / 2000: loss 0.002841\n",
      "iteration 300 / 2000: loss 0.006199\n",
      "iteration 400 / 2000: loss 0.000366\n",
      "iteration 500 / 2000: loss 0.001141\n",
      "iteration 600 / 2000: loss 0.000890\n",
      "iteration 700 / 2000: loss 0.003632\n",
      "iteration 800 / 2000: loss 0.000605\n",
      "iteration 900 / 2000: loss 0.002504\n",
      "iteration 1000 / 2000: loss 0.003906\n",
      "iteration 1100 / 2000: loss 0.006296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 2000: loss 0.000935\n",
      "iteration 1300 / 2000: loss 0.021892\n",
      "iteration 1400 / 2000: loss 0.002815\n",
      "iteration 1500 / 2000: loss 0.003149\n",
      "iteration 1600 / 2000: loss 0.003068\n",
      "iteration 1700 / 2000: loss 0.007558\n",
      "iteration 1800 / 2000: loss 0.016829\n",
      "iteration 1900 / 2000: loss 0.000453\n",
      "iteration 0 / 2000: loss 0.984177\n",
      "iteration 100 / 2000: loss 0.027484\n",
      "iteration 200 / 2000: loss 0.000870\n",
      "iteration 300 / 2000: loss 0.001366\n",
      "iteration 400 / 2000: loss 0.000766\n",
      "iteration 500 / 2000: loss 0.016542\n",
      "iteration 600 / 2000: loss 0.002091\n",
      "iteration 700 / 2000: loss 0.002924\n",
      "iteration 800 / 2000: loss 0.002647\n",
      "iteration 900 / 2000: loss 0.019054\n",
      "iteration 1000 / 2000: loss 0.009218\n",
      "iteration 1100 / 2000: loss 0.000518\n",
      "iteration 1200 / 2000: loss 0.004261\n",
      "iteration 1300 / 2000: loss 0.023654\n",
      "iteration 1400 / 2000: loss 0.025496\n",
      "iteration 1500 / 2000: loss 0.001724\n",
      "iteration 1600 / 2000: loss 0.007165\n",
      "iteration 1700 / 2000: loss 0.000930\n",
      "iteration 1800 / 2000: loss 0.002085\n",
      "iteration 1900 / 2000: loss 0.000829\n",
      "iteration 0 / 2000: loss 0.925810\n",
      "iteration 100 / 2000: loss 0.003299\n",
      "iteration 200 / 2000: loss 0.000961\n",
      "iteration 300 / 2000: loss 0.002011\n",
      "iteration 400 / 2000: loss 0.005318\n",
      "iteration 500 / 2000: loss 0.001145\n",
      "iteration 600 / 2000: loss 0.034057\n",
      "iteration 700 / 2000: loss 0.000644\n",
      "iteration 800 / 2000: loss 0.009162\n",
      "iteration 900 / 2000: loss 0.003958\n",
      "iteration 1000 / 2000: loss 0.102875\n",
      "iteration 1100 / 2000: loss 0.003845\n",
      "iteration 1200 / 2000: loss 0.028707\n",
      "iteration 1300 / 2000: loss 0.004679\n",
      "iteration 1400 / 2000: loss 0.010739\n",
      "iteration 1500 / 2000: loss 0.005608\n",
      "iteration 1600 / 2000: loss 0.011810\n",
      "iteration 1700 / 2000: loss 0.003091\n",
      "iteration 1800 / 2000: loss 0.006227\n",
      "iteration 1900 / 2000: loss 0.009174\n",
      "iteration 0 / 2000: loss 0.980893\n",
      "iteration 100 / 2000: loss 0.001458\n",
      "iteration 200 / 2000: loss 0.003177\n",
      "iteration 300 / 2000: loss 0.062707\n",
      "iteration 400 / 2000: loss 0.013771\n",
      "iteration 500 / 2000: loss 0.099640\n",
      "iteration 600 / 2000: loss 0.011131\n",
      "iteration 700 / 2000: loss 0.005821\n",
      "iteration 800 / 2000: loss 0.020435\n",
      "iteration 900 / 2000: loss 0.006870\n",
      "iteration 1000 / 2000: loss 0.001962\n",
      "iteration 1100 / 2000: loss 0.025351\n",
      "iteration 1200 / 2000: loss 0.013182\n",
      "iteration 1300 / 2000: loss 0.028183\n",
      "iteration 1400 / 2000: loss 0.045379\n",
      "iteration 1500 / 2000: loss 0.014664\n",
      "iteration 1600 / 2000: loss 0.049220\n",
      "iteration 1700 / 2000: loss 0.103287\n",
      "iteration 1800 / 2000: loss 0.019099\n",
      "iteration 1900 / 2000: loss 0.004235\n",
      "iteration 0 / 2000: loss 0.971392\n",
      "iteration 100 / 2000: loss 0.034157\n",
      "iteration 200 / 2000: loss 0.000633\n",
      "iteration 300 / 2000: loss 0.002143\n",
      "iteration 400 / 2000: loss 0.001456\n",
      "iteration 500 / 2000: loss 0.000377\n",
      "iteration 600 / 2000: loss 0.002874\n",
      "iteration 700 / 2000: loss 0.000163\n",
      "iteration 800 / 2000: loss 0.000266\n",
      "iteration 900 / 2000: loss 0.010750\n",
      "iteration 1000 / 2000: loss 0.001194\n",
      "iteration 1100 / 2000: loss 0.001632\n",
      "iteration 1200 / 2000: loss 0.000605\n",
      "iteration 1300 / 2000: loss 0.000587\n",
      "iteration 1400 / 2000: loss 0.003730\n",
      "iteration 1500 / 2000: loss 0.000986\n",
      "iteration 1600 / 2000: loss 0.000510\n",
      "iteration 1700 / 2000: loss 0.007569\n",
      "iteration 1800 / 2000: loss 0.002072\n",
      "iteration 1900 / 2000: loss 0.005888\n",
      "iteration 0 / 2000: loss 0.995141\n",
      "iteration 100 / 2000: loss 0.094096\n",
      "iteration 200 / 2000: loss 0.002098\n",
      "iteration 300 / 2000: loss 0.005640\n",
      "iteration 400 / 2000: loss 0.000218\n",
      "iteration 500 / 2000: loss 0.000710\n",
      "iteration 600 / 2000: loss 0.000660\n",
      "iteration 700 / 2000: loss 0.003283\n",
      "iteration 800 / 2000: loss 0.000343\n",
      "iteration 900 / 2000: loss 0.002513\n",
      "iteration 1000 / 2000: loss 0.002703\n",
      "iteration 1100 / 2000: loss 0.006274\n",
      "iteration 1200 / 2000: loss 0.000749\n",
      "iteration 1300 / 2000: loss 0.018416\n",
      "iteration 1400 / 2000: loss 0.003780\n",
      "iteration 1500 / 2000: loss 0.003010\n",
      "iteration 1600 / 2000: loss 0.002331\n",
      "iteration 1700 / 2000: loss 0.007437\n",
      "iteration 1800 / 2000: loss 0.015959\n",
      "iteration 1900 / 2000: loss 0.000328\n",
      "iteration 0 / 2000: loss 0.984130\n",
      "iteration 100 / 2000: loss 0.001940\n",
      "iteration 200 / 2000: loss 0.000412\n",
      "iteration 300 / 2000: loss 0.000875\n",
      "iteration 400 / 2000: loss 0.000310\n",
      "iteration 500 / 2000: loss 0.017041\n",
      "iteration 600 / 2000: loss 0.002268\n",
      "iteration 700 / 2000: loss 0.002533\n",
      "iteration 800 / 2000: loss 0.005375\n",
      "iteration 900 / 2000: loss 0.021507\n",
      "iteration 1000 / 2000: loss 0.000465\n",
      "iteration 1100 / 2000: loss 0.003827\n",
      "iteration 1200 / 2000: loss 0.009424\n",
      "iteration 1300 / 2000: loss 0.028682\n",
      "iteration 1400 / 2000: loss 0.000360\n",
      "iteration 1500 / 2000: loss 0.002866\n",
      "iteration 1600 / 2000: loss 0.007548\n",
      "iteration 1700 / 2000: loss 0.000367\n",
      "iteration 1800 / 2000: loss 0.002478\n",
      "iteration 1900 / 2000: loss 0.005441\n",
      "iteration 0 / 2000: loss 0.925786\n",
      "iteration 100 / 2000: loss 0.002842\n",
      "iteration 200 / 2000: loss 0.000440\n",
      "iteration 300 / 2000: loss 0.001638\n",
      "iteration 400 / 2000: loss 0.004919\n",
      "iteration 500 / 2000: loss 0.000871\n",
      "iteration 600 / 2000: loss 0.050436\n",
      "iteration 700 / 2000: loss 0.051535\n",
      "iteration 800 / 2000: loss 0.010768\n",
      "iteration 900 / 2000: loss 0.003033\n",
      "iteration 1000 / 2000: loss 0.121804\n",
      "iteration 1100 / 2000: loss 0.003633\n",
      "iteration 1200 / 2000: loss 0.004586\n",
      "iteration 1300 / 2000: loss 0.003338\n",
      "iteration 1400 / 2000: loss 0.005193\n",
      "iteration 1500 / 2000: loss 0.007226\n",
      "iteration 1600 / 2000: loss 0.012873\n",
      "iteration 1700 / 2000: loss 0.004512\n",
      "iteration 1800 / 2000: loss 0.007407\n",
      "iteration 1900 / 2000: loss 0.009944\n",
      "iteration 0 / 2000: loss 0.980874\n",
      "iteration 100 / 2000: loss 0.000645\n",
      "iteration 200 / 2000: loss 0.002643\n",
      "iteration 300 / 2000: loss 0.062142\n",
      "iteration 400 / 2000: loss 0.013950\n",
      "iteration 500 / 2000: loss 0.100180\n",
      "iteration 600 / 2000: loss 0.009596\n",
      "iteration 700 / 2000: loss 0.004730\n",
      "iteration 800 / 2000: loss 0.016771\n",
      "iteration 900 / 2000: loss 0.005890\n",
      "iteration 1000 / 2000: loss 0.001629\n",
      "iteration 1100 / 2000: loss 0.038247\n",
      "iteration 1200 / 2000: loss 0.013377\n",
      "iteration 1300 / 2000: loss 0.053624\n",
      "iteration 1400 / 2000: loss 0.051957\n",
      "iteration 1500 / 2000: loss 0.022306\n",
      "iteration 1600 / 2000: loss 0.051628\n",
      "iteration 1700 / 2000: loss 0.106521\n",
      "iteration 1800 / 2000: loss 0.020685\n",
      "iteration 1900 / 2000: loss 0.002629\n",
      "iteration 0 / 2000: loss 0.971422\n",
      "iteration 100 / 2000: loss 0.017513\n",
      "iteration 200 / 2000: loss 0.001707\n",
      "iteration 300 / 2000: loss 0.002831\n",
      "iteration 400 / 2000: loss 0.002534\n",
      "iteration 500 / 2000: loss 0.001061\n",
      "iteration 600 / 2000: loss 0.003443\n",
      "iteration 700 / 2000: loss 0.000730\n",
      "iteration 800 / 2000: loss 0.000770\n",
      "iteration 900 / 2000: loss 0.012789\n",
      "iteration 1000 / 2000: loss 0.001585\n",
      "iteration 1100 / 2000: loss 0.002114\n",
      "iteration 1200 / 2000: loss 0.001053\n",
      "iteration 1300 / 2000: loss 0.001007\n",
      "iteration 1400 / 2000: loss 0.004063\n",
      "iteration 1500 / 2000: loss 0.001391\n",
      "iteration 1600 / 2000: loss 0.000852\n",
      "iteration 1700 / 2000: loss 0.008149\n",
      "iteration 1800 / 2000: loss 0.002375\n",
      "iteration 1900 / 2000: loss 0.008233\n",
      "iteration 0 / 2000: loss 0.995153\n",
      "iteration 100 / 2000: loss 0.005645\n",
      "iteration 200 / 2000: loss 0.002082\n",
      "iteration 300 / 2000: loss 0.005862\n",
      "iteration 400 / 2000: loss 0.000260\n",
      "iteration 500 / 2000: loss 0.000952\n",
      "iteration 600 / 2000: loss 0.000728\n",
      "iteration 700 / 2000: loss 0.003458\n",
      "iteration 800 / 2000: loss 0.000504\n",
      "iteration 900 / 2000: loss 0.002358\n",
      "iteration 1000 / 2000: loss 0.012992\n",
      "iteration 1100 / 2000: loss 0.003812\n",
      "iteration 1200 / 2000: loss 0.004620\n",
      "iteration 1300 / 2000: loss 0.004034\n",
      "iteration 1400 / 2000: loss 0.003582\n",
      "iteration 1500 / 2000: loss 0.004953\n",
      "iteration 1600 / 2000: loss 0.010089\n",
      "iteration 1700 / 2000: loss 0.007078\n",
      "iteration 1800 / 2000: loss 0.019172\n",
      "iteration 1900 / 2000: loss 0.000362\n",
      "iteration 0 / 2000: loss 0.984141\n",
      "iteration 100 / 2000: loss 0.011521\n",
      "iteration 200 / 2000: loss 0.000490\n",
      "iteration 300 / 2000: loss 0.000990\n",
      "iteration 400 / 2000: loss 0.000415\n",
      "iteration 500 / 2000: loss 0.016703\n",
      "iteration 600 / 2000: loss 0.002514\n",
      "iteration 700 / 2000: loss 0.012532\n",
      "iteration 800 / 2000: loss 0.005751\n",
      "iteration 900 / 2000: loss 0.018092\n",
      "iteration 1000 / 2000: loss 0.009524\n",
      "iteration 1100 / 2000: loss 0.000672\n",
      "iteration 1200 / 2000: loss 0.003492\n",
      "iteration 1300 / 2000: loss 0.027546\n",
      "iteration 1400 / 2000: loss 0.000528\n",
      "iteration 1500 / 2000: loss 0.002787\n",
      "iteration 1600 / 2000: loss 0.010879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700 / 2000: loss 0.000487\n",
      "iteration 1800 / 2000: loss 0.003016\n",
      "iteration 1900 / 2000: loss 0.005503\n",
      "iteration 0 / 2000: loss 0.925770\n",
      "iteration 100 / 2000: loss 0.003218\n",
      "iteration 200 / 2000: loss 0.000837\n",
      "iteration 300 / 2000: loss 0.001969\n",
      "iteration 400 / 2000: loss 0.000743\n",
      "iteration 500 / 2000: loss 0.001178\n",
      "iteration 600 / 2000: loss 0.043315\n",
      "iteration 700 / 2000: loss 0.015239\n",
      "iteration 800 / 2000: loss 0.010063\n",
      "iteration 900 / 2000: loss 0.003257\n",
      "iteration 1000 / 2000: loss 0.122480\n",
      "iteration 1100 / 2000: loss 0.003662\n",
      "iteration 1200 / 2000: loss 0.004308\n",
      "iteration 1300 / 2000: loss 0.003404\n",
      "iteration 1400 / 2000: loss 0.004687\n",
      "iteration 1500 / 2000: loss 0.007607\n",
      "iteration 1600 / 2000: loss 0.013304\n",
      "iteration 1700 / 2000: loss 0.004726\n",
      "iteration 1800 / 2000: loss 0.007100\n",
      "iteration 1900 / 2000: loss 0.011048\n",
      "iteration 0 / 2000: loss 0.980870\n",
      "iteration 100 / 2000: loss 0.001256\n",
      "iteration 200 / 2000: loss 0.003193\n",
      "iteration 300 / 2000: loss 0.053977\n",
      "iteration 400 / 2000: loss 0.015653\n",
      "iteration 500 / 2000: loss 0.100612\n",
      "iteration 600 / 2000: loss 0.010123\n",
      "iteration 700 / 2000: loss 0.005211\n",
      "iteration 800 / 2000: loss 0.012353\n",
      "iteration 900 / 2000: loss 0.005919\n",
      "iteration 1000 / 2000: loss 0.002137\n",
      "iteration 1100 / 2000: loss 0.023388\n",
      "iteration 1200 / 2000: loss 0.013552\n",
      "iteration 1300 / 2000: loss 0.024835\n",
      "iteration 1400 / 2000: loss 0.045182\n",
      "iteration 1500 / 2000: loss 0.015395\n",
      "iteration 1600 / 2000: loss 0.042627\n",
      "iteration 1700 / 2000: loss 0.089143\n",
      "iteration 1800 / 2000: loss 0.017954\n",
      "iteration 1900 / 2000: loss 0.004347\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "best_val = -1\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "learning_rates = [1,1e-1, 5e-1, 1.5e-2, 4e-2]\n",
    "regularization_strengths = [0.01, 0.001, 0.005]    \n",
    "hidden_size = [100, 500, 1000]\n",
    "num_classes = 2\n",
    "k=0\n",
    "while k<len(hidden_size):\n",
    "    i=0\n",
    "    while i< len(learning_rates):\n",
    "        j = 0\n",
    "        while j < len(regularization_strengths):\n",
    "            z = 0\n",
    "            while z < len(batch_sizes):\n",
    "                net = TwoLayerNet(input_size, hidden_size[k], num_classes)\n",
    "                stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=2000, batch_size=batch_sizes[z],\n",
    "                    learning_rate=learning_rates[i], learning_rate_decay=0.95,\n",
    "                    reg=regularization_strengths[j], verbose=True)\n",
    "                val_acc = np.sum(np.square(net.predict(X_val) - y_val), axis=1).mean()\n",
    "                train_acc = np.sum(np.square(net.predict(X_train) - y_train), axis=1).mean()\n",
    "                if best_val < val_acc:\n",
    "                    best_val = val_acc\n",
    "                    best_net = net\n",
    "                results[(hidden_size[k],learning_rates[i], regularization_strengths[j])] = (train_acc, val_acc)\n",
    "                z=z+1\n",
    "            j=j+1\n",
    "        i=i+1\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs: 1.000000e+02 lr: 1.500000e-02 reg: 1.000000e-03 train accuracy: 0.200660 val accuracy: 0.202504\n",
      "hs: 1.000000e+02 lr: 1.500000e-02 reg: 5.000000e-03 train accuracy: 0.208713 val accuracy: 0.209581\n",
      "hs: 1.000000e+02 lr: 1.500000e-02 reg: 1.000000e-02 train accuracy: 0.192362 val accuracy: 0.195222\n",
      "hs: 1.000000e+02 lr: 4.000000e-02 reg: 1.000000e-03 train accuracy: 0.090690 val accuracy: 0.090575\n",
      "hs: 1.000000e+02 lr: 4.000000e-02 reg: 5.000000e-03 train accuracy: 0.088817 val accuracy: 0.087667\n",
      "hs: 1.000000e+02 lr: 4.000000e-02 reg: 1.000000e-02 train accuracy: 0.107927 val accuracy: 0.111777\n",
      "hs: 1.000000e+02 lr: 1.000000e-01 reg: 1.000000e-03 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 1.000000e+02 lr: 1.000000e-01 reg: 5.000000e-03 train accuracy: 0.138682 val accuracy: 0.150456\n",
      "hs: 1.000000e+02 lr: 1.000000e-01 reg: 1.000000e-02 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 1.000000e+02 lr: 5.000000e-01 reg: 1.000000e-03 train accuracy: 0.151193 val accuracy: 0.161286\n",
      "hs: 1.000000e+02 lr: 5.000000e-01 reg: 5.000000e-03 train accuracy: 0.151193 val accuracy: 0.161286\n",
      "hs: 1.000000e+02 lr: 5.000000e-01 reg: 1.000000e-02 train accuracy: 0.151193 val accuracy: 0.161286\n",
      "hs: 1.000000e+02 lr: 1.000000e+00 reg: 1.000000e-03 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 1.000000e+02 lr: 1.000000e+00 reg: 5.000000e-03 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 1.000000e+02 lr: 1.000000e+00 reg: 1.000000e-02 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 5.000000e+02 lr: 1.500000e-02 reg: 1.000000e-03 train accuracy: 0.212925 val accuracy: 0.213285\n",
      "hs: 5.000000e+02 lr: 1.500000e-02 reg: 5.000000e-03 train accuracy: 0.211865 val accuracy: 0.212353\n",
      "hs: 5.000000e+02 lr: 1.500000e-02 reg: 1.000000e-02 train accuracy: 0.214358 val accuracy: 0.214546\n",
      "hs: 5.000000e+02 lr: 4.000000e-02 reg: 1.000000e-03 train accuracy: 0.088645 val accuracy: 0.087455\n",
      "hs: 5.000000e+02 lr: 4.000000e-02 reg: 5.000000e-03 train accuracy: 0.107719 val accuracy: 0.111381\n",
      "hs: 5.000000e+02 lr: 4.000000e-02 reg: 1.000000e-02 train accuracy: 0.096920 val accuracy: 0.098049\n",
      "hs: 5.000000e+02 lr: 1.000000e-01 reg: 1.000000e-03 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 5.000000e+02 lr: 1.000000e-01 reg: 5.000000e-03 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 5.000000e+02 lr: 1.000000e-01 reg: 1.000000e-02 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 5.000000e+02 lr: 5.000000e-01 reg: 1.000000e-03 train accuracy: 0.151193 val accuracy: 0.161286\n",
      "hs: 5.000000e+02 lr: 5.000000e-01 reg: 5.000000e-03 train accuracy: nan val accuracy: nan\n",
      "hs: 5.000000e+02 lr: 5.000000e-01 reg: 1.000000e-02 train accuracy: 0.151193 val accuracy: 0.161286\n",
      "hs: 5.000000e+02 lr: 1.000000e+00 reg: 1.000000e-03 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 5.000000e+02 lr: 1.000000e+00 reg: 5.000000e-03 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 5.000000e+02 lr: 1.000000e+00 reg: 1.000000e-02 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 1.000000e+03 lr: 1.500000e-02 reg: 1.000000e-03 train accuracy: 0.219288 val accuracy: 0.218885\n",
      "hs: 1.000000e+03 lr: 1.500000e-02 reg: 5.000000e-03 train accuracy: 0.215565 val accuracy: 0.215607\n",
      "hs: 1.000000e+03 lr: 1.500000e-02 reg: 1.000000e-02 train accuracy: 0.216116 val accuracy: 0.216092\n",
      "hs: 1.000000e+03 lr: 4.000000e-02 reg: 1.000000e-03 train accuracy: 0.098046 val accuracy: 0.099497\n",
      "hs: 1.000000e+03 lr: 4.000000e-02 reg: 5.000000e-03 train accuracy: 0.086312 val accuracy: 0.084267\n",
      "hs: 1.000000e+03 lr: 4.000000e-02 reg: 1.000000e-02 train accuracy: 0.084895 val accuracy: 0.082272\n",
      "hs: 1.000000e+03 lr: 1.000000e-01 reg: 1.000000e-03 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 1.000000e+03 lr: 1.000000e-01 reg: 5.000000e-03 train accuracy: 0.110206 val accuracy: 0.114703\n",
      "hs: 1.000000e+03 lr: 1.000000e-01 reg: 1.000000e-02 train accuracy: 0.110060 val accuracy: 0.114528\n",
      "hs: 1.000000e+03 lr: 5.000000e-01 reg: 1.000000e-03 train accuracy: nan val accuracy: nan\n",
      "hs: 1.000000e+03 lr: 5.000000e-01 reg: 5.000000e-03 train accuracy: nan val accuracy: nan\n",
      "hs: 1.000000e+03 lr: 5.000000e-01 reg: 1.000000e-02 train accuracy: nan val accuracy: nan\n",
      "hs: 1.000000e+03 lr: 1.000000e+00 reg: 1.000000e-03 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 1.000000e+03 lr: 1.000000e+00 reg: 5.000000e-03 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "hs: 1.000000e+03 lr: 1.000000e+00 reg: 1.000000e-02 train accuracy: 0.155596 val accuracy: 0.165902\n",
      "best validation accuracy achieved during cross-validation: 4.847301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for hs, lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(hs,lr, reg)]\n",
    "    print 'hs: %e lr: %e reg: %e train accuracy: %f val accuracy: %f' % (\n",
    "                hs,lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061122304464281595\n"
     ]
    }
   ],
   "source": [
    "test_err = np.sum(np.square(net.predict(X_test) - y_test), axis=1).mean()\n",
    "print test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
